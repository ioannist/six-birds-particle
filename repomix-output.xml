This file is a merged representation of the entire codebase, combined into a single document by Repomix.

<file_summary>
This section contains a summary of this file.

<purpose>
This file contains a packed representation of the entire repository's contents.
It is designed to be easily consumable by AI systems for analysis, code review,
or other automated processes.
</purpose>

<file_format>
The content is organized as follows:
1. This summary section
2. Repository information
3. Directory structure
4. Repository files (if enabled)
5. Multiple file entries, each consisting of:
  - File path as an attribute
  - Full contents of the file
</file_format>

<usage_guidelines>
- This file should be treated as read-only. Any changes should be made to the
  original repository files, not this packed version.
- When processing this file, use the file path to distinguish
  between different files in the repository.
- Be aware that this file may contain sensitive information. Handle it with
  the same level of security as you would the original repository.
</usage_guidelines>

<notes>
- Some files may have been excluded based on .gitignore rules and Repomix's configuration
- Binary files are not included in this packed representation. Please refer to the Repository Structure section for a complete list of file paths, including binary files
- Files matching patterns in .gitignore are excluded
- Files matching default ignore patterns are excluded
- Files are sorted by Git change count (files with more changes are at the bottom)
</notes>

</file_summary>

<directory_structure>
apps/
  web/
    src/
      sim/
        runCache.ts
        sim.worker.ts
        workerClient.ts
        workerMessages.ts
      wasm/
        .gitkeep
        README.md
      App.tsx
      main.tsx
      style.css
      vite-env.d.ts
    index.html
    package.json
    tsconfig.json
    tsconfig.tsbuildinfo
    vite.config.ts
crates/
  sim-core/
    src/
      lib.rs
    Cargo.toml
docs/
  materializations/
    adaptive_homeostatis.md
    clockwork_fabric.md
    coarse_graining_and_attention.md
    levin.md
    open_endedness.md
  00_glossary.md
  01_ratchet_theory_overview.md
  02_primitives_P1_P6.md
  03_atoms_MFQX.md
  04_weakness_x_economy.md
  05_constraint_x_constraint_ratchets.md
  06_alife_principles_and_null_regime.md
  07_central_device_reversible_channels.md
  08_deliverable_A_null_stationary_measure.md
  09_deliverable_B_transition_tables.md
  10_deliverable_C_cycle_affinity_wiring.md
  11_deliverable_D_diagnostics.md
  12_browser_impl_notes.md
  13_open_questions_and_extensions.md
  14_ratchet_motifs_and_asymmetries.md
  15_spacetime.md
  16_meta_meta.md
  17_obstruciton.md
  README.md
scripts/
  params/
    clock_code/
      clock_null.json
      clock_p6.json
      clock_tur_sweep_base.json
      code_deadline_gated_clock.json
      code_deadline_gated_random.json
      code_deadline_gated_static.json
      code_null.json
      code_p6_clock_gated_random.json
      code_p6_clock_gated_static.json
      code_p6_clock_gated.json
      code_p6_drive.json
      deadline_fidelity_drift.json
      deadline_fidelity_found.json
      deadline_fidelity_random.json
      deadline_fidelity_static.json
    meta/
      meta2_null_coupled.json
      meta2_null_decoupled.json
      meta2_p3_pump_coupled.json
      meta2_p3p6_combo_coupled.json
      meta2_p6_drive_coupled.json
    op_coupling/
      deadline_opk_best.json
      opS_null_energy.json
      opS_p6_drive_only.json
    op_motifs_selection/
      selection_base_tuned.json
    base_null_balanced.json
    base_p3_pump_minimal.json
    base_p3p6_combo_minimal.json
    base_p6_drive.json
  bootstrap.sh
  calibrate-gate-gaps.mjs
  deadline-event-utils.mjs
  opk-metrics.mjs
  opk-motif-basis.mjs
  opk-motif-utils.mjs
  ratchet-cli.mjs
  README.md
  run-clock-code-joint-sweep.mjs
  run-clock-tur-sweep.mjs
  run-code-maintenance.mjs
  run-deadline-event-stats.mjs
  run-deadline-opk-ci-iso.mjs
  run-deadline-opk-compare.mjs
  run-deadline-opk-decomposition.mjs
  run-deadline-opk-frontier.mjs
  run-deadline-opk-motif-compare.mjs
  run-deadline-opk-motif-events.mjs
  run-deadline-opk-motif-p2-ablation.mjs
  run-deadline-opk-repair-budget-curves.mjs
  run-deadline-opk-throughput-matched-v4.mjs
  run-deadline-opk-throughput-matched.mjs
  run-deadline-phase-diagram.mjs
  run-deadline-success-ci.mjs
  run-fidelity-separation-search.mjs
  run-meta-sweep.mjs
  run-moving-hazard-homeostasis.mjs
  run-moving-hazard-null-control.mjs
  run-moving-hazard-speed-sweep.mjs
  run-moving-hazard-stationary-vs-moving.mjs
  run-opk-composed-hierarchy.mjs
  run-opk-diagnostics.mjs
  run-opk-hierarchy-search.mjs
  run-opk-motif-language-compare.mjs
  run-opk-motif-language.mjs
  run-sweep.mjs
  test-clock-current.mjs
  test-clock-deadline-traversal.mjs
  test-clock-traversal-necessity.mjs
  test-ep-null-tight.mjs
  test-instrumentation-invariance.mjs
  test-opcoupling-effect.mjs
  test-opcoupling-invariants.mjs
  test-opcoupling-null-ep.mjs
  test-opk-drive-selection.mjs
  test-opk-null-ep-weights.mjs
  test-step2-meta-layers.mjs
  test-step3-eta-coupling.mjs
.gitignore
.repomixrc
AGENTS.md
Cargo.toml
EXPERIMENTS.md
Makefile
NOTE-TO-SELF.md
README.md
</directory_structure>

<files>
This section contains the contents of the repository's files.

<file path="docs/17_obstruciton.md">
You’re right — my previous answers implicitly assumed a particular *carrier implementation* (“particles + fields”). Strip that away.

A substrate‑independent way to ask your question is:

> Given **only** a state space and dynamics, is there a mathematically rigorous sense in which each primitive can (i) *exist as a real dynamical feature* and (ii) *become nontrivial / selected* without us “engineering” it?

Yes — but you need to phrase each primitive as a **property of a stochastic dynamical law**, not as a specific variable like `w` or `S`.

Below is a fully substrate‑agnostic formulation that makes “primitive comes alive” a theorem-checkable statement.

---

## 1) Minimal substrate assumptions

Assume only:

* A state space (X) (could be continuous, discrete, graphs, manifolds, strings, anything).
* A (possibly time-dependent) Markov dynamics on (X):

  * discrete time: kernels (P_t(x\to dy)), or
  * continuous time: generator (L_t).

No particles. No fields. Just “states and transitions”.

Now each primitive is defined as an **obstruction** to some symmetry/factorization property of the dynamics.

A primitive “comes alive” when the corresponding obstruction is **nonzero**.

That gives you **implementation‑independent proofs**.

---

## 2) What does “comes alive” mean mathematically?

There are two logically different notions:

### A. Structural existence

The primitive is present as a structural feature of the transition law.

This is **binary**: true/false, often equivalent to a clean algebraic condition.

### B. Dynamical selection / activation

Among many *possible* realizations of that primitive (many couplings, many modes, many constraints), the dynamics biases toward some nontrivial configuration.

This is not binary; it’s about **stationary measures** and **currents** on an enlarged state space.

Both can be made rigorous without referring to any specific substrate.

---

## 3) Substrate‑independent “alive certificates” for each primitive

I’ll phrase each primitive as:

* **Definition (substrate-free)**
* **Certificate (what you can prove / test)**
* **Genericity** (does it appear “for free” in a typical system?)

---

# P6 — Drive / Context (time‑reversal breaking in autonomous dynamics)

### Definition

P6 is present iff the dynamics violates **detailed balance** (reversibility) in steady state.

### Certificate (Kolmogorov / cycle affinity)

For a time‑homogeneous Markov chain with stationary distribution (\pi), define for any directed cycle
[
C: x_0\to x_1\to\cdots\to x_{n-1}\to x_0
]
the **cycle affinity**
[
\mathcal A(C) ;=; \log \prod_{i=0}^{n-1}\frac{P(x_i\to x_{i+1})}{P(x_{i+1}\to x_i)}.
]

Then:

* ( \mathcal A(C)=0) for all cycles (C) **iff** detailed balance holds.
* If there exists a cycle with (\mathcal A(C)\neq 0), the process is nonreversible and has **positive entropy production**.

So you can “prove P6 is alive” by exhibiting one such cycle (or proving EP (>0)).

### Genericity

Reversibility is a set of polynomial equalities among the transition probabilities; in the space of all kernels it is lower-dimensional. So “nonreversible” is generic once you allow any nonconservative bias.

---

# P3 — Protocol / Noncommutativity (time‑ordering as a source of pumping)

P3 is not “drive” by itself; it’s **time-structured application of otherwise reversible operators**.

### Definition

P3 is present iff the dynamics is **time‑inhomogeneous** in a way that cannot be reduced to a single reversible kernel.

The cleanest substrate-free formalization is: a periodic protocol with kernels (K_1,\dots,K_r) applied in sequence.

### Certificate (noncommutativity theorem)

Assume each phase kernel (K_i) is reversible with respect to the same (\pi) (self‑adjoint in (L^2(\pi))).

Let the one‑period kernel be:
[
K ;=; K_rK_{r-1}\cdots K_1.
]

Then:

* (K) is reversible **iff** all the (K_i) commute pairwise.

Proof sketch (fully general):

* (K_i^\dagger = K_i) (reversible).
* (K^\dagger = (K_r\cdots K_1)^\dagger = K_1\cdots K_r).
* So (K^\dagger=K) holds iff (K_r\cdots K_1 = K_1\cdots K_r), which requires commutation in general.

So P3 “comes alive” exactly when you can show **noncommuting reversible phases**.

### Genericity

Commutation is also a measure‑zero condition. So if you have multiple reversible operations, noncommutativity is generic unless engineered away.

---

# P1 — Coupling / Binding (non-factorization of dynamics)

This is the substrate‑free notion of “relations become state”.

### Definition

P1 is present iff the dynamics is not decomposable into independent subsystems.

Formally, if the substrate admits a decomposition (X=X_A\times X_B) (or more factors), then “no coupling” means:

* the kernel factorizes: (P = P_A\otimes P_B), or
* the generator splits: (L = L_A + L_B) with (L_A) acting only on (X_A), etc.

P1 is alive when **no such factorization exists** (at the decomposition you care about).

### Certificates

Any one of the following (depending on what structure you have) proves coupling:

1. **Transition dependence:**
   There exist (a\neq a') in (X_A) and (b\neq b') in (X_B) such that
   [
   P\big((a,b)\to (a',b)\big) \neq P\big((a,b')\to (a',b')\big).
   ]
   So the update of (A) depends on the state of (B).

2. **Stationary dependence (if (\pi) exists):**
   (\pi) is not a product measure: (I_\pi(A;B)>0) (mutual information).

### Genericity

Exact factorization again requires many equalities; it is nongeneric. So P1 is “almost always” present once there are interacting degrees of freedom.

---

# P4 — Discrete Modes / Symbolic Regimes (hidden state needed for Markovity)

This is the substrate‑free notion of “modes appear”.

### Definition

P4 is present when the observed dynamics requires a **finite discrete latent state** to become Markov (or approximately Markov).

Equivalently: your process on some chosen observables is not first‑order Markov, but becomes Markov after augmenting the state with a finite mode variable (m\in{1,\dots,M}).

### Certificates

1. **Non‑Markovianity implies hidden state:**
   If your observed process (Y_t=f(X_t)) is not Markov of order 1, then any exact Markov representation requires augmenting state with memory. When that memory can be compressed to a finite set of predictive equivalence classes, you get discrete modes.

2. **Metastability / spectral evidence (Markov chains):**
   If the transition operator has (k) eigenvalues close to 1 (or small eigenvalues in the generator), that indicates (k) long‑lived macro‑states. The induced coarse process is a finite‑state mode chain.

### Genericity

Not every system has clean metastable modes, but mode structure is common in systems with separation of timescales and barriers.

---

# P5 — Closure / Gating / Viability (state-dependent allowedness)

This is the substrate‑free notion of “constraints become state.”

### Definition

P5 is present when there exists a **nontrivial forward‑closed (or nearly closed) region** of state space — a “viability set”.

Two levels:

1. **Hard closure:** a subset (C\subsetneq X) such that
   [
   P(x\to X\setminus C)=0 \quad \forall x\in C.
   ]
   That’s an actual “gate”: transitions out are forbidden.

2. **Soft closure (metastable viability):**
   [
   \alpha(C) := \sup_{x\in C} P(x\to X\setminus C) \ll 1.
   ]
   Then mean lifetime in (C) is (\gtrsim 1/\alpha(C)).

### Certificates

* Hard: show the transition graph has a nontrivial closed communicating class.
* Soft: show there exists a set with small **conductance**
  [
  \Phi(C)=\frac{\sum_{x\in C,,y\notin C}\pi(x)P(x\to y)}{\pi(C)} \ll 1,
  ]
  which implies metastability and quasi-stationary behavior.

### Genericity

Hard closure is nongeneric in fully ergodic random chains, but soft closure/metastability is common in structured, local, bounded systems.

---

# P2 — Economy / Weakness (finite capacity / conserved budget / saturation)

This is the hardest to state “universally” because it’s not a single symmetry; it’s about **capacity constraints**.

### Definition

P2 is present when the dynamics is constrained by **bounded or conserved resources** that limit how much structure can accumulate.

The clean substrate‑free way to say that is:

There exists a “budget observable” (B:X\to\mathbb R) such that either

1. **Hard economy (conservation):**
   [
   B(X_{t+1}) = B(X_t) \quad \text{almost surely},
   ]
   equivalently (PB=B) (discrete) or (LB=0) (continuous).

or

2. **Weakness (bounded carriers):**
   there exists some coordinate (u) (possibly emergent) whose state is bounded and updates saturate at bounds, preventing unlimited amplification.

### Certificates

* Hard economy: exhibit a conserved quantity (B) (a first integral of the Markov dynamics).
* Boundedness: show state space is constrained to a compact set in some direction (reflecting boundaries, bounded integers, etc.) and updates cannot increase magnitude beyond a cap.

### Genericity

Some form of boundedness is almost unavoidable in finite physical systems. But “economy that actually constrains other processes” requires coupling between budget variables and transition rates.

---

## 4) So can we “prove the primitives materialize” irrespective of substrate?

Now we can answer your intent precisely:

### You cannot prove “every substrate will activate all six primitives.”

Counterexample is trivial: if (X) has one state, nothing happens; if dynamics is reversible and time-homogeneous and factorized, P6 and P3 won’t appear.

So there is no universal theorem “all six will always emerge” without assumptions.

### But you *can* prove something stronger and more relevant:

For each primitive there is a **substrate-independent, implementation-independent** condition that is:

* **Necessary and sufficient** (P6, P3 under the stated assumptions, hard P5),
* or at least **sufficient** (P1, soft P5, P4, P2),

and these conditions are stated purely in terms of the transition law (P_t) / (L_t).

That is already a rigorous answer: “comes alive” = “this invariant is nonzero”.

---

## 5) What does “selected” mean without referencing a particular substrate?

Selection is also definable substrate‑independently once you allow *any* primitive degrees of freedom (\Theta) to be part of the state.

Let the full state be (Z=(X,\Theta)) with Markov dynamics.

* In **null** (reversible) regimes, if the rules are symmetric in (\Theta), then (\Theta) is typically **neutral**: no currents, and its stationary distribution is determined only by equilibrium weights (often uniform under symmetry).
* In **driven/protocol** regimes, (\Theta) can become **biased** because it changes cycle affinities or protocol pumping.

A substrate‑free “selection criterion” is:

> (\Theta) is selected **iff** changing (\Theta) changes some time‑asymmetric functional of trajectories (cycle affinity / entropy production / pumped current).

In symbols:

* If for some cycle (C),
  [
  \frac{\partial}{\partial \theta}\mathcal A_\theta(C) \neq 0,
  ]
  then under drive/protocol the steady behavior of (\theta) can become biased (selection in operator space).

Conversely:

* If (\mathcal A_\theta(C)) is independent of (\theta) for all cycles (and protocol commutators don’t depend on (\theta)), then (\theta) is dynamically irrelevant and cannot be selected by ratcheting.

That is the clean, implementation‑free statement of when “a primitive degree of freedom can come alive and be selected.”

---

## 6) A useful reframing: the primitives are universal “obstructions”

If you want a single sentence per primitive that is truly substrate‑agnostic:

* **P1** = obstruction to **tensor factorization** (independence).
* **P2** = obstruction to **unbounded accumulation** (capacity/conservation).
* **P3** = obstruction to **commutativity** (order matters → pumping).
* **P4** = obstruction to **Markov order‑1** (needs modes/memory).
* **P5** = obstruction to **full mixing** (closed/almost-closed sets).
* **P6** = obstruction to **time reversal symmetry** (cycle affinities).

In that sense, they absolutely “materialize” across substrates because they are not particles/fields — they are *structural invariants* of dynamics.

---

## If you want, the next mathematical step

If your goal is to turn this into something you can put in your repo docs (and later in a report), the clean deliverable is:

* a formal definition of each primitive as an invariant/obstruction of a Markov semigroup,
* plus a “no-go map” (e.g., “without P6 or noncommuting P3, no sustained current/clock”; “without P5, no long-lived viability set”; “without P2, no scarcity, hence trivial combinatorial saturation,” etc.).

That would be fully independent of any particular substrate or implementation and would make “primitive comes alive” a theorem-like statement rather than an anecdotal observation.

===

Think of “obstruction” as **the thing that prevents you from simplifying the world**.

If a system were simple, you could describe it with an easy shortcut like:

* “all parts act independently,” or
* “the order of operations doesn’t matter,” or
* “if I reverse time it looks the same,” or
* “everything eventually mixes everywhere,” or
* “there’s no real ‘mode switch,’ it’s all one continuous behavior,” or
* “you can add unlimited structure for free.”

A **primitive “comes alive”** when it’s the **reason that shortcut fails**.

So each primitive is not “a widget” tied to particles or code. It’s a *name for a specific kind of shortcut that becomes impossible*.

---

## P1 is the obstruction to “independence”

**Shortcut it blocks:** “I can understand each part separately.”

When P1 is alive, parts become **coupled**: what happens to one part depends on others. You can’t factor the system into separate stories.

**Everyday intuition:** Two people pushing a couch. You can’t predict either person’s motion without knowing what the other does.

**How you’d tell (without math):** If changing one region/agent changes the behavior of another region/agent, P1 is present.

---

## P2 is the obstruction to “free, unlimited complexity”

**Shortcut it blocks:** “we can keep adding structure without paying for it.”

When P2 is alive, there is **scarcity**: limited bandwidth, limited tokens, limited energy, limited memory, limited “moves.” You can’t just grow complexity arbitrarily; you must allocate.

**Everyday intuition:** A phone with finite battery and storage. You can do a lot, but not everything at once.

**How you’d tell:** If the system hits tradeoffs—more of one thing means less of another—P2 is present.

---

## P3 is the obstruction to “order doesn’t matter”

**Shortcut it blocks:** “doing A then B is the same as B then A.”

When P3 is alive, the system has **noncommuting steps**: the order of operations creates effects that are impossible if you shuffle them randomly. This is how “pumping” and “protocol clocks” happen.

**Everyday intuition:** Brushing teeth then drinking orange juice ≠ drinking juice then brushing teeth. Same actions, different order, different outcome.

**How you’d tell:** If the same set of actions produces different results when you permute their sequence, P3 is present.

---

## P4 is the obstruction to “everything is one smooth behavior”

**Shortcut it blocks:** “there aren’t real modes, it’s all gradual.”

When P4 is alive, the system has **discrete modes** (gear states, phases, regimes). You can’t interpolate smoothly between them; you have to “click” from one to another.

**Everyday intuition:** A gearbox: 1st/2nd/3rd are real discrete regimes, not a continuous slider.

**How you’d tell:** If the system shows stable “states” that persist and then switch abruptly (even if noisy), P4 is present.

---

## P5 is the obstruction to “everything mixes everywhere”

**Shortcut it blocks:** “given time, any state can reach any other; nothing is protected.”

When P5 is alive, there are **gates/closures/viability regions**: some transitions are blocked or extremely unlikely. That creates “inside vs outside,” “allowed vs forbidden,” or “safe vs unsafe.”

**Everyday intuition:** A door with a lock, or a membrane: you can’t just wander anywhere.

**How you’d tell:** If there are regions/configurations the system stays in for a long time, or boundaries that limit flow, P5 is present.

---

## P6 is the obstruction to “time-reversal symmetry”

**Shortcut it blocks:** “if I run the movie backward, it looks statistically the same.”

When P6 is alive, you have a real **arrow of time**: cycles run preferentially one way; currents have a direction; entropy production is positive. This is the “ratchet” ingredient.

**Everyday intuition:** A wind-up toy or a refrigerator: it runs in a preferred direction because it’s consuming fuel / dissipating heat.

**How you’d tell:** If there’s a persistent directed cycle (a “clock hand” that rotates one way) that disappears when you remove drive, P6 is present.

---

# Why this framing answers your question “irrespective of substrate”

Because it doesn’t mention particles, grids, language, biology, etc.

It says:

* P1 exists whenever “independence” is broken.
* P2 exists whenever “free unlimited growth” is broken by scarcity.
* P3 exists whenever “order doesn’t matter” is broken.
* P4 exists whenever “smooth single regime” is broken by modes.
* P5 exists whenever “everything mixes” is broken by closures/gates.
* P6 exists whenever “time symmetry” is broken.

Those are **properties of dynamics**, not of the material.

So the primitives “materialize” whenever the world you’re studying has those obstructions. And in a given model, you can often *prove* a primitive is present by proving the corresponding shortcut is impossible (or by giving a simple counterexample inside the dynamics).

---

# A final intuitive punchline

If you imagine trying to “compress” a system into a simpler description:

* **Each primitive is the name of what prevents that compression.**
* “Comes alive” = “you cannot compress it that way anymore.”

That’s why this framing is substrate-independent and mathematically rigorous underneath, while still being explainable without equations.
</file>

<file path="scripts/run-moving-hazard-homeostasis.mjs">
#!/usr/bin/env node
import fs from "node:fs";
import path from "node:path";
import {
  loadWasm,
  readJson,
  mean,
  std,
  percentile,
  parseSeedList,
  meanAbsDiffRegion,
  errRegionBits,
} from "./deadline-event-utils.mjs";
import { parseOpOffsets } from "./opk-metrics.mjs";

const rootDir = path.resolve(path.dirname(new URL(import.meta.url).pathname), "..");

function ensureDir(dir) {
  fs.mkdirSync(dir, { recursive: true });
}

function quadrantIndex(idx, g) {
  const x = idx % g;
  const y = Math.floor(idx / g);
  const qx = x < g / 2 ? 0 : 1;
  const qy = y < g / 2 ? 0 : 1;
  return qy * 2 + qx;
}

function stripeIndex(idx, g, bins) {
  const x = idx % g;
  const fx = x / g;
  return Math.min(bins - 1, Math.floor(fx * bins));
}

function buildRegionMask(g, regionType, regionIndex, span, bins) {
  const cells = g * g;
  const mask = new Array(cells);
  if (regionType === "stripe") {
    const s = Math.max(1, span);
    for (let i = 0; i < cells; i += 1) {
      const stripe = stripeIndex(i, g, bins);
      let ok = false;
      for (let k = 0; k < s; k += 1) {
        if ((regionIndex + k) % bins === stripe) {
          ok = true;
          break;
        }
      }
      mask[i] = ok;
    }
  } else {
    for (let i = 0; i < cells; i += 1) {
      mask[i] = quadrantIndex(i, g) === regionIndex;
    }
  }
  return mask;
}

function shuffleInPlace(arr, seed) {
  let x = seed >>> 0;
  if (x === 0) x = 1;
  for (let i = arr.length - 1; i > 0; i -= 1) {
    x ^= x << 13;
    x ^= x >>> 17;
    x ^= x << 5;
    const j = x % (i + 1);
    [arr[i], arr[j]] = [arr[j], arr[i]];
  }
}

function buildMatchedOutsideMask(regionMask, seed) {
  const outside = [];
  const regionCount = regionMask.filter(Boolean).length;
  for (let i = 0; i < regionMask.length; i += 1) {
    if (!regionMask[i]) outside.push(i);
  }
  if (outside.length === 0) return regionMask.map(() => false);
  shuffleInPlace(outside, seed);
  const target = Math.min(regionCount, outside.length);
  const mask = new Array(regionMask.length).fill(false);
  for (let i = 0; i < target; i += 1) {
    mask[outside[i]] = true;
  }
  return mask;
}

function hazardIndicesStripe(gridSize, bins, hazardCount) {
  const indices = [];
  for (let i = 0; i < hazardCount; i += 1) {
    const pos = Math.floor((i + 0.5) * gridSize / hazardCount);
    const idx = Math.min(bins - 1, Math.floor((pos / gridSize) * bins));
    indices.push(idx);
  }
  return Array.from(new Set(indices));
}

function hazardIndicesQuadrant() {
  return [0, 1, 2, 3];
}

function entropyFromCounts(counts) {
  let total = 0;
  for (const val of counts.values()) total += val;
  if (total === 0) return 0;
  let h = 0;
  for (const val of counts.values()) {
    if (val <= 0) continue;
    const p = val / total;
    h += -p * Math.log(p);
  }
  return h;
}

function transitionEntropy(counts) {
  return entropyFromCounts(counts);
}

function symmetryGap(counts) {
  const pairMap = new Map();
  for (const [key, count] of counts.entries()) {
    const [fromStr, toStr] = key.split("->");
    const from = Number(fromStr);
    const to = Number(toStr);
    if (Number.isNaN(from) || Number.isNaN(to) || from === to) continue;
    const a = Math.min(from, to);
    const b = Math.max(from, to);
    const pairKey = `${a}|${b}`;
    const prev = pairMap.get(pairKey) ?? { ab: 0, ba: 0, a, b };
    if (from === a) prev.ab += count;
    else prev.ba += count;
    pairMap.set(pairKey, prev);
  }
  let num = 0;
  let denom = 0;
  for (const pair of pairMap.values()) {
    num += Math.abs(pair.ab - pair.ba);
    denom += pair.ab + pair.ba;
  }
  return denom > 0 ? num / denom : 0;
}

function coarseEPFromCounts(counts, alpha = 0.5) {
  const pairMap = new Map();
  for (const [key, count] of counts.entries()) {
    const [fromStr, toStr] = key.split("->");
    const from = Number(fromStr);
    const to = Number(toStr);
    if (Number.isNaN(from) || Number.isNaN(to) || from === to) continue;
    const a = Math.min(from, to);
    const b = Math.max(from, to);
    const pairKey = `${a}|${b}`;
    const prev = pairMap.get(pairKey) ?? { ab: 0, ba: 0 };
    if (from === a) prev.ab += count;
    else prev.ba += count;
    pairMap.set(pairKey, prev);
  }
  let total = 0;
  for (const pair of pairMap.values()) {
    const nij = pair.ab;
    const nji = pair.ba;
    const c1 = nij + alpha;
    const c2 = nji + alpha;
    total += (nij - nji) * Math.log(c1 / c2);
  }
  return total;
}

function computeMotifStats(counts, transitions, sampleCount, threshold = 20) {
  const statsOk = sampleCount >= threshold;
  return {
    motifSamples: sampleCount,
    motifCount: counts.size,
    motifEntropy: statsOk ? entropyFromCounts(counts) : null,
    transitionEntropy: statsOk ? transitionEntropy(transitions) : null,
    symmetryGap: statsOk ? symmetryGap(transitions) : null,
    coarseEP: statsOk ? coarseEPFromCounts(transitions, 0.5) : null,
    motifStatsOk: statsOk,
  };
}

function computeContextSensitivity(motifByHazard) {
  const jsDivergence = (pMap, qMap) => {
    const keys = new Set([...pMap.keys(), ...qMap.keys()]);
    let sumP = 0;
    let sumQ = 0;
    for (const k of keys) {
      sumP += pMap.get(k) ?? 0;
      sumQ += qMap.get(k) ?? 0;
    }
    if (sumP === 0 || sumQ === 0) return 0;
    let js = 0;
    for (const k of keys) {
      const p = (pMap.get(k) ?? 0) / sumP;
      const q = (qMap.get(k) ?? 0) / sumQ;
      const m = 0.5 * (p + q);
      if (p > 0) js += 0.5 * p * Math.log(p / m);
      if (q > 0) js += 0.5 * q * Math.log(q / m);
    }
    return js;
  };

  const hazardKeys = Array.from(motifByHazard.keys());
  let jsAcc = 0;
  let jsCount = 0;
  for (let i = 0; i < hazardKeys.length; i += 1) {
    for (let j = i + 1; j < hazardKeys.length; j += 1) {
      jsAcc += jsDivergence(motifByHazard.get(hazardKeys[i]), motifByHazard.get(hazardKeys[j]));
      jsCount += 1;
    }
  }
  const avgPairwiseJSD = jsCount > 0 ? jsAcc / jsCount : 0;

  const totalByHazard = new Map();
  const totalByMotif = new Map();
  let total = 0;
  for (const [hazard, counts] of motifByHazard.entries()) {
    for (const [motifId, count] of counts.entries()) {
      totalByHazard.set(hazard, (totalByHazard.get(hazard) ?? 0) + count);
      totalByMotif.set(motifId, (totalByMotif.get(motifId) ?? 0) + count);
      total += count;
    }
  }
  let mutualInfoHM = 0;
  if (total > 0) {
    for (const [hazard, counts] of motifByHazard.entries()) {
      for (const [motifId, count] of counts.entries()) {
        const pHM = count / total;
        const pH = (totalByHazard.get(hazard) ?? 0) / total;
        const pM = (totalByMotif.get(motifId) ?? 0) / total;
        if (pHM > 0 && pH > 0 && pM > 0) {
          mutualInfoHM += pHM * Math.log(pHM / (pH * pM));
        }
      }
    }
  }

  return {
    avgPairwiseJSD,
    mutualInfoHM,
    motifAlphabetSize: totalByMotif.size,
    sampleCount: total,
  };
}

function opkStatsForMask({
  tokens,
  offsets,
  rCount,
  opBudgetK,
  gridSize,
  metaLayers,
  mask,
}) {
  const cells = gridSize * gridSize;
  const budget = Math.max(1, opBudgetK ?? 1);
  const eps = 1e-9;
  let hSum = 0;
  let r2Sum = 0;
  let dMagSum = 0;
  let count = 0;

  for (let iface = 0; iface < metaLayers; iface += 1) {
    for (let q = 0; q < cells; q += 1) {
      if (!mask[q]) continue;
      const start = (iface * cells + q) * rCount;
      let h = 0;
      let r2 = 0;
      let dxAcc = 0;
      let dyAcc = 0;
      for (let r = 0; r < rCount; r += 1) {
        const k = tokens[start + r] / budget;
        if (k > 0) h += -k * Math.log(k + eps);
        const [dx, dy] = offsets[r];
        r2 += k * (dx * dx + dy * dy);
        dxAcc += k * dx;
        dyAcc += k * dy;
      }
      hSum += h;
      r2Sum += r2;
      dMagSum += Math.hypot(dxAcc, dyAcc);
      count += 1;
    }
  }

  if (count === 0) {
    return { Hk: 0, R2: 0, dMag: 0 };
  }
  return {
    Hk: hSum / count,
    R2: r2Sum / count,
    dMag: dMagSum / count,
  };
}

function motifIdFromMeta({ moveId, idxP5Base, idxP5Meta, layerByte, mismatchBin, kDir, effR }) {
  if (moveId === idxP5Base) {
    return (kDir % effR) * 3 + mismatchBin;
  }
  if (moveId === idxP5Meta) {
    return effR * 3 + layerByte * (effR * 3) + (kDir % effR) * 3 + mismatchBin;
  }
  return null;
}

function parseArgs(argv) {
  const out = {
    seeds: "1,2,3,4,5,6,7,8,9,10",
    steps: 2_000_000,
    eventEvery: null,
    deadline: null,
    reportEvery: null,
    hazardHoldEvents: 4,
    hazardCount: 8,
    region: "stripe",
    logMotifs: 1,
    variants: "A,B,C,D",
    errGood: 0.1,
    sdiffGood: 1.0,
    corruptFrac: 0.2,
    tailWindow: 200_000,
  };
  for (let i = 2; i < argv.length; i += 1) {
    const arg = argv[i];
    if (arg === "--seeds") out.seeds = argv[++i];
    else if (arg === "--steps") out.steps = Number(argv[++i]);
    else if (arg === "--eventEvery") out.eventEvery = Number(argv[++i]);
    else if (arg === "--deadline") out.deadline = Number(argv[++i]);
    else if (arg === "--reportEvery") out.reportEvery = Number(argv[++i]);
    else if (arg === "--hazardHoldEvents") out.hazardHoldEvents = Number(argv[++i]);
    else if (arg === "--hazardCount") out.hazardCount = Number(argv[++i]);
    else if (arg === "--region") out.region = argv[++i];
    else if (arg === "--logMotifs") out.logMotifs = Number(argv[++i]);
    else if (arg === "--variants") out.variants = argv[++i];
    else if (arg === "--errGood") out.errGood = Number(argv[++i]);
    else if (arg === "--sdiffGood") out.sdiffGood = Number(argv[++i]);
    else if (arg === "--corruptFrac") out.corruptFrac = Number(argv[++i]);
    else if (arg === "--tailWindow") out.tailWindow = Number(argv[++i]);
  }
  return out;
}

function pickPreset() {
  const preferred = path.resolve(rootDir, "scripts/params/clock_code/deadline_fidelity_found.json");
  const fallback = path.resolve(rootDir, "scripts/params/clock_code/code_deadline_gated_clock.json");
  if (fs.existsSync(preferred)) return preferred;
  return fallback;
}

export async function runMovingHazardHomeostasis({
  presetPath,
  seeds,
  steps,
  eventEvery,
  deadline,
  reportEvery,
  hazardHoldEvents,
  hazardCount,
  region,
  logMotifs,
  variants,
  writeOutputs = true,
  commonOverrides = {},
  errGood = 0.1,
  sdiffGood = 1.0,
  corruptFrac = 0.2,
  tailWindow = 200_000,
}) {
  const mod = await loadWasm();
  const presetRaw = readJson(presetPath);
  const baseParams = presetRaw.params ?? presetRaw;

  const stepsVal = steps ?? presetRaw.steps ?? 2_000_000;
  const eventEveryVal = eventEvery ?? presetRaw.eventEvery ?? 50_000;
  const deadlineVal = deadline ?? presetRaw.deadline ?? 15_000;
  const reportEveryVal = reportEvery ?? presetRaw.reportEvery ?? 1_000;
  const hazardHoldEventsVal = hazardHoldEvents ?? 4;
  const hazardCountVal = hazardCount ?? 8;
  const regionType = region ?? "stripe";

  const variantsList = variants ?? ["A", "B", "C", "D"];
  const seedList = Array.isArray(seeds) ? seeds : parseSeedList(seeds);

  const rawRows = [];
  const finalStates = [];

  const summaryAgg = new Map();
  const byHazardAgg = new Map();
  const motifByHazardAgg = new Map();
  const trackingAgg = new Map();

  for (const variant of variantsList) {
    for (const seed of seedList) {
      const params = { ...baseParams, ...commonOverrides, epDebug: 1 };
      if (variant === "A") {
        params.opCouplingOn = 0;
        params.sCouplingMode = 0;
        params.opDriveOnK = 0;
      } else if (variant === "B") {
        params.opCouplingOn = 1;
        params.sCouplingMode = 0;
        params.opDriveOnK = 0;
      } else if (variant === "C") {
        params.opCouplingOn = 1;
        params.sCouplingMode = 1;
        params.opDriveOnK = 0;
      } else if (variant === "D") {
        params.opCouplingOn = 1;
        params.sCouplingMode = 1;
        params.opDriveOnK = 1;
      }
      if (variant === "C" || variant === "D") {
        params.opStencil = 1;
        params.opBudgetK = 32;
        params.opKTargetWeight = 0.1;
      }

      const bins = params.clockK ?? 8;
      const gridSize = params.gridSize ?? 32;
      const hazardIndices =
        regionType === "quadrant"
          ? hazardIndicesQuadrant()
          : hazardIndicesStripe(gridSize, bins, hazardCountVal);
      const hazardMasks = new Map();
      const outsideMasks = new Map();
      for (const idx of hazardIndices) {
        const mask = buildRegionMask(gridSize, regionType, idx, params.repairGateSpan ?? 1, bins);
        hazardMasks.set(idx, mask);
        outsideMasks.set(idx, buildMatchedOutsideMask(mask, seed + idx * 997));
      }

      const maxEventStart = stepsVal - Math.max(deadlineVal, eventEveryVal);
      const eventCount = Math.floor(maxEventStart / eventEveryVal);
      const events = [];
      for (let e = 0; e < eventCount; e += 1) {
        const tEvent = (e + 1) * eventEveryVal;
        const epochIndex = Math.floor(e / hazardHoldEventsVal);
        const hazardIndex = hazardIndices[epochIndex % hazardIndices.length];
        const eventIndexWithinEpoch = e % hazardHoldEventsVal;
        events.push({
          idx: e,
          tEvent,
          epochIndex,
          hazardIndex,
          eventIndexWithinEpoch,
          recovered: false,
          miss: false,
          recoverySteps: null,
          samples: [],
          motifCountsHazard: new Map(),
          motifCountsOutside: new Map(),
          motifTransHazard: new Map(),
          motifTransOutside: new Map(),
          motifEpHazard: new Map(),
          motifEpOutside: new Map(),
          motifEpAbsHazard: new Map(),
          motifEpAbsOutside: new Map(),
          motifSamplesHazard: 0,
          motifSamplesOutside: 0,
          prevHazardMotif: null,
          prevOutsideMotif: null,
          startEp: null,
          endEp: null,
          startCounts: null,
          endCounts: null,
          tracking: null,
        });
      }

      const sim = new mod.Sim(50, seed);
      sim.set_params(params);
      if (logMotifs) {
        const moveLabels = sim.ep_move_labels ? Array.from(sim.ep_move_labels()) : [];
        const idxP5Base = moveLabels.indexOf("P5Base");
        const idxP5Meta = moveLabels.indexOf("P5Meta");
        if (idxP5Base >= 0 && idxP5Meta >= 0) {
          sim.set_params({
            acceptLogOn: 1,
            acceptLogMask: (1 << idxP5Base) | (1 << idxP5Meta),
            acceptLogCap: 200000,
          });
        }
      }
      if (sim.accept_log_clear) sim.accept_log_clear();

      const moveLabels = sim.ep_move_labels ? Array.from(sim.ep_move_labels()) : [];
      const idxP5Base = moveLabels.indexOf("P5Base");
      const idxP5Meta = moveLabels.indexOf("P5Meta");
      const idxOpK = moveLabels.indexOf("OpK");
      const idxClock = moveLabels.indexOf("Clock");

      const rCount = sim.op_r_count ? sim.op_r_count() : 0;
      const effR = Math.max(1, rCount);
      const metaLayers = params.metaLayers ?? 0;
      const opOffsets = sim.op_offsets ? sim.op_offsets() : new Int8Array();
      const offsets = parseOpOffsets(opOffsets);

      const readCounts = () => {
        const stats = sim.ep_q_stats();
        return Array.from(stats.count);
      };

      const baselineErr = [];
      let errFloor = null;
      let currentEventIdx = -1;
      let nextEventIdx = 0;

      const updateEventOutcome = (event, t, good) => {
        const elapsed = t - event.tEvent;
        if (!event.recovered && !event.miss && elapsed > deadlineVal) {
          event.miss = true;
          event.recoverySteps = null;
        } else if (!event.recovered && !event.miss && good) {
          event.recovered = true;
          event.recoverySteps = elapsed;
        }
        if ((event.recovered || event.miss) && !event.endEp) {
          const epTotal = sim.ep_exact_total();
          const epByMove = sim.ep_exact_by_move();
          const epRepair = (epByMove[idxP5Base] ?? 0) + (epByMove[idxP5Meta] ?? 0);
          const epOpK = epByMove[idxOpK] ?? 0;
          const epClock = epByMove[idxClock] ?? 0;
          event.endEp = { total: epTotal, repair: epRepair, opk: epOpK, clock: epClock };
          const countsNow = readCounts();
          event.endCounts = {
            p5Base: countsNow[idxP5Base] ?? 0,
            p5Meta: countsNow[idxP5Meta] ?? 0,
            opk: countsNow[idxOpK] ?? 0,
            clock: countsNow[idxClock] ?? 0,
          };
          if (params.opCouplingOn && metaLayers > 0 && rCount > 0) {
            const tokens = sim.op_k_tokens();
            const hazardMask = hazardMasks.get(event.hazardIndex);
            const outsideMask = outsideMasks.get(event.hazardIndex);
            const hazardStats = opkStatsForMask({
              tokens,
              offsets,
              rCount,
              opBudgetK: params.opBudgetK,
              gridSize,
              metaLayers,
              mask: hazardMask,
            });
            const outsideStats = opkStatsForMask({
              tokens,
              offsets,
              rCount,
              opBudgetK: params.opBudgetK,
              gridSize,
              metaLayers,
              mask: outsideMask,
            });
            event.tracking = {
              hazard: hazardStats,
              outside: outsideStats,
              delta: {
                Hk: hazardStats.Hk - outsideStats.Hk,
                R2: hazardStats.R2 - outsideStats.R2,
                dMag: hazardStats.dMag - outsideStats.dMag,
              },
            };
          } else {
            event.tracking = { available: false };
          }
        }
      };

      const processAcceptLog = () => {
        if (!logMotifs || !sim.accept_log_len) return;
        const len = sim.accept_log_len();
        if (len === 0) return;
        const u32 = sim.accept_log_u32();
        const ep = sim.accept_log_ep();
        const entries = Math.min(ep.length, Math.floor(u32.length / 3));
        for (let i = 0; i < entries; i += 1) {
          const tEntry = u32[i * 3];
          const q = u32[i * 3 + 1];
          const meta = u32[i * 3 + 2];
          const moveId = meta & 0xff;
          if (moveId !== idxP5Base && moveId !== idxP5Meta) continue;
          if (tEntry < eventEveryVal) continue;
          const t0 = Math.floor(tEntry / eventEveryVal) * eventEveryVal;
          if (t0 < eventEveryVal) continue;
          if (t0 > maxEventStart) continue;
          if (tEntry - t0 >= deadlineVal) continue;
          const eventIdx = Math.floor(tEntry / eventEveryVal) - 1;
          if (eventIdx < 0 || eventIdx >= events.length) continue;
          const event = events[eventIdx];
          const hazardMask = hazardMasks.get(event.hazardIndex);
          const outsideMask = outsideMasks.get(event.hazardIndex);
          const qIdx = Number(q);
          const inHazard = hazardMask ? hazardMask[qIdx] : false;
          const inOutside = outsideMask ? outsideMask[qIdx] : false;
          if (!inHazard && !inOutside) continue;

          const layerByte = (meta >>> 8) & 0xff;
          const mismatchBin = (meta >>> 16) & 0xff;
          const kDir = (meta >>> 24) & 0xff;
          const motifId = motifIdFromMeta({
            moveId,
            idxP5Base,
            idxP5Meta,
            layerByte,
            mismatchBin,
            kDir,
            effR,
          });
          if (motifId === null) continue;

          const epDelta = ep[i] ?? 0;
          if (inHazard) {
            event.motifSamplesHazard += 1;
            event.motifCountsHazard.set(motifId, (event.motifCountsHazard.get(motifId) ?? 0) + 1);
            event.motifEpHazard.set(motifId, (event.motifEpHazard.get(motifId) ?? 0) + epDelta);
            event.motifEpAbsHazard.set(
              motifId,
              (event.motifEpAbsHazard.get(motifId) ?? 0) + Math.abs(epDelta),
            );
            if (event.prevHazardMotif != null) {
              const key = `${event.prevHazardMotif}->${motifId}`;
              event.motifTransHazard.set(key, (event.motifTransHazard.get(key) ?? 0) + 1);
            }
            event.prevHazardMotif = motifId;
          } else if (inOutside) {
            event.motifSamplesOutside += 1;
            event.motifCountsOutside.set(
              motifId,
              (event.motifCountsOutside.get(motifId) ?? 0) + 1,
            );
            event.motifEpOutside.set(motifId, (event.motifEpOutside.get(motifId) ?? 0) + epDelta);
            event.motifEpAbsOutside.set(
              motifId,
              (event.motifEpAbsOutside.get(motifId) ?? 0) + Math.abs(epDelta),
            );
            if (event.prevOutsideMotif != null) {
              const key = `${event.prevOutsideMotif}->${motifId}`;
              event.motifTransOutside.set(key, (event.motifTransOutside.get(key) ?? 0) + 1);
            }
            event.prevOutsideMotif = motifId;
          }
        }
        sim.accept_log_clear();
      };

      for (let t = reportEveryVal; t <= stepsVal; t += reportEveryVal) {
        sim.step(reportEveryVal);
        processAcceptLog();

        while (nextEventIdx < events.length && t >= events[nextEventIdx].tEvent) {
          const event = events[nextEventIdx];
          const perturb = {
            target: "metaS",
            layer: 0,
            frac: corruptFrac,
            mode: "randomize",
          };
          if (regionType === "stripe") {
            perturb.region = "stripe";
            perturb.bins = bins;
            perturb.span = params.repairGateSpan ?? 1;
            perturb.bin = event.hazardIndex;
          } else {
            perturb.region = "quadrant";
            perturb.quadrant = event.hazardIndex;
          }
          perturb.seed = seed * 1000 + event.tEvent;
          sim.apply_perturbation(perturb);
          currentEventIdx = nextEventIdx;
          nextEventIdx += 1;

          const epTotal = sim.ep_exact_total();
          const epByMove = sim.ep_exact_by_move();
          const epRepair = (epByMove[idxP5Base] ?? 0) + (epByMove[idxP5Meta] ?? 0);
          const epOpK = epByMove[idxOpK] ?? 0;
          const epClock = epByMove[idxClock] ?? 0;
          event.startEp = { total: epTotal, repair: epRepair, opk: epOpK, clock: epClock };
          const countsNow = readCounts();
          event.startCounts = {
            p5Base: countsNow[idxP5Base] ?? 0,
            p5Meta: countsNow[idxP5Meta] ?? 0,
            opk: countsNow[idxOpK] ?? 0,
            clock: countsNow[idxClock] ?? 0,
          };
        }

        if (currentEventIdx >= 0) {
          const event = events[currentEventIdx];
          if (!event.recovered && !event.miss) {
            const hazardMask = hazardMasks.get(event.hazardIndex);
            const baseS = sim.base_s_field();
            const metaS = sim.meta_field();
            const cells = baseS.length;
            const meta0 = metaS.subarray(0, cells);
            const lS = params.lS ?? 1;
            const errSample = errRegionBits(baseS, meta0, gridSize, lS, hazardMask);
            if (t < eventEveryVal) {
              baselineErr.push(errSample);
            } else if (errFloor === null) {
              errFloor = baselineErr.length ? mean(baselineErr) : 0;
            }
            const errAdj = errFloor === null ? 0 : Math.max(0, errSample - errFloor);
            const sdiff = meanAbsDiffRegion(baseS, meta0, hazardMask);
            const good = sdiff <= sdiffGood && errAdj <= errGood;
            event.samples.push({ t, err: errAdj, sdiff, good });
            updateEventOutcome(event, t, good);
          }
        }
      }

      for (const event of events) {
        if (!event.recovered && !event.miss) {
          event.miss = true;
        }
      }

      const baseSFinal = sim.base_s_field();
      const metaFinal = sim.meta_field();
      const baseSum = baseSFinal.reduce((acc, v) => acc + v, 0);
      const metaSum = metaFinal.reduce((acc, v) => acc + v, 0);
      const epTotalFinal = sim.ep_exact_total();
      const epByMoveFinal = sim.ep_exact_by_move();
      const finalCounts = readCounts();
      finalStates.push({
        variant,
        seed,
        epExactTotal: epTotalFinal,
        epRepairTotal: (epByMoveFinal[idxP5Base] ?? 0) + (epByMoveFinal[idxP5Meta] ?? 0),
        epOpKTotal: epByMoveFinal[idxOpK] ?? 0,
        epClockTotal: epByMoveFinal[idxClock] ?? 0,
        countP5Base: finalCounts[idxP5Base] ?? 0,
        countP5Meta: finalCounts[idxP5Meta] ?? 0,
        countOpK: finalCounts[idxOpK] ?? 0,
        countClock: finalCounts[idxClock] ?? 0,
        baseSum,
        metaSum,
        baseLen: baseSFinal.length,
        metaLen: metaFinal.length,
      });

      for (const event of events) {
        const tailStart = event.tEvent + Math.max(0, deadlineVal - tailWindow);
        const tailEnd = event.tEvent + deadlineVal;
        const tailSamples = event.samples.filter((s) => s.t >= tailStart && s.t <= tailEnd);
        const uptimeTail = tailSamples.length
          ? tailSamples.filter((s) => s.good).length / tailSamples.length
          : 0;
        const errTail = tailSamples.length ? mean(tailSamples.map((s) => s.err)) : 0;
        const sdiffTail = tailSamples.length ? mean(tailSamples.map((s) => s.sdiff)) : 0;

        const motifStatsHaz = computeMotifStats(
          event.motifCountsHazard,
          event.motifTransHazard,
          event.motifSamplesHazard,
        );
        const motifStatsOut = computeMotifStats(
          event.motifCountsOutside,
          event.motifTransOutside,
          event.motifSamplesOutside,
        );

        const epTotal = event.endEp && event.startEp ? event.endEp.total - event.startEp.total : 0;
        const epRepair = event.endEp && event.startEp ? event.endEp.repair - event.startEp.repair : 0;
        const epOpK = event.endEp && event.startEp ? event.endEp.opk - event.startEp.opk : 0;
        const epClock = event.endEp && event.startEp ? event.endEp.clock - event.startEp.clock : 0;

        const repairCount = event.endCounts && event.startCounts
          ? (event.endCounts.p5Base - event.startCounts.p5Base) +
            (event.endCounts.p5Meta - event.startCounts.p5Meta)
          : 0;
        const opkCount = event.endCounts && event.startCounts
          ? event.endCounts.opk - event.startCounts.opk
          : 0;
        const epRepairPerAction = repairCount > 0 ? epRepair / repairCount : 0;

        rawRows.push({
          variant,
          seed,
          eventIndex: event.idx,
          epochIndex: event.epochIndex,
          hazardIndex: event.hazardIndex,
          eventIndexWithinEpoch: event.eventIndexWithinEpoch,
          success: event.recovered,
          miss: event.miss,
          recoverySteps: event.recoverySteps,
          uptimeTail,
          errTail,
          sdiffTail,
          epTotal,
          epRepair,
          epOpK,
          epClock,
          repairCount,
          opkCount,
          epRepairPerAction,
          motifSamplesHazard: motifStatsHaz.motifSamples,
          motifEntropyHazard: motifStatsHaz.motifEntropy,
          transitionEntropyHazard: motifStatsHaz.transitionEntropy,
          symmetryGapHazard: motifStatsHaz.symmetryGap,
          coarseEPHazard: motifStatsHaz.coarseEP,
          motifStatsOkHazard: motifStatsHaz.motifStatsOk,
          motifSamplesOutside: motifStatsOut.motifSamples,
          motifEntropyOutside: motifStatsOut.motifEntropy,
          symmetryGapOutside: motifStatsOut.symmetryGap,
          coarseEPOutside: motifStatsOut.coarseEP,
          trackingAvailable: event.tracking && event.tracking.available === false ? false : !!event.tracking,
          hazard_Hk_mean: event.tracking?.hazard?.Hk ?? null,
          outside_Hk_mean: event.tracking?.outside?.Hk ?? null,
          hazard_R2_mean: event.tracking?.hazard?.R2 ?? null,
          outside_R2_mean: event.tracking?.outside?.R2 ?? null,
          hazard_dmag_mean: event.tracking?.hazard?.dMag ?? null,
          outside_dmag_mean: event.tracking?.outside?.dMag ?? null,
          hazard_minus_outside_Hk: event.tracking?.delta?.Hk ?? null,
          hazard_minus_outside_R2: event.tracking?.delta?.R2 ?? null,
          hazard_minus_outside_dmag: event.tracking?.delta?.dMag ?? null,
        });

        if (logMotifs && event.motifSamplesHazard > 0) {
          const key = `${variant}|${event.hazardIndex}`;
          const entry = motifByHazardAgg.get(key) ?? {
            variant,
            hazardIndex: event.hazardIndex,
            counts: new Map(),
          };
          for (const [motifId, count] of event.motifCountsHazard.entries()) {
            entry.counts.set(motifId, (entry.counts.get(motifId) ?? 0) + count);
          }
          motifByHazardAgg.set(key, entry);
        }
      }
    }
  }

  const summaryRows = [];
  const byHazardRows = [];
  const motifByHazardRows = [];
  const trackingRows = [];
  const contextSensitivityRows = [];

  const groupBy = (rows, keys) => {
    const map = new Map();
    for (const row of rows) {
      const key = keys.map((k) => row[k]).join("|");
      const arr = map.get(key) ?? [];
      arr.push(row);
      map.set(key, arr);
    }
    return map;
  };

  const byVariant = groupBy(rawRows, ["variant"]);
  for (const [key, rows] of byVariant.entries()) {
    const misses = rows.filter((r) => r.miss).length;
    const missFrac = rows.length > 0 ? misses / rows.length : 0;
    const recoverySteps = rows
      .filter((r) => Number.isFinite(r.recoverySteps))
      .map((r) => r.recoverySteps);
    const motifRows = rows.filter((r) => r.motifStatsOkHazard);
    summaryRows.push({
      variant: key,
      missFrac,
      uptimeTailMean: rows.length ? mean(rows.map((r) => r.uptimeTail)) : 0,
      errTailMean: rows.length ? mean(rows.map((r) => r.errTail)) : 0,
      recoveryMedian: percentile(recoverySteps, 0.5),
      recoveryP95: percentile(recoverySteps, 0.95),
      epTotalMean: rows.length ? mean(rows.map((r) => r.epTotal)) : 0,
      epRepairMean: rows.length ? mean(rows.map((r) => r.epRepair)) : 0,
      epOpKMean: rows.length ? mean(rows.map((r) => r.epOpK)) : 0,
      epClockMean: rows.length ? mean(rows.map((r) => r.epClock)) : 0,
      epTotalRateMean: rows.length ? mean(rows.map((r) => r.epTotal / deadlineVal)) : 0,
      epRepairPerActionMean: rows.length ? mean(rows.map((r) => r.epRepairPerAction)) : 0,
      epRepairPerActionMedian: percentile(rows.map((r) => r.epRepairPerAction), 0.5),
      motifEntropyMean: motifRows.length ? mean(motifRows.map((r) => r.motifEntropyHazard)) : null,
      motifSymmetryGapMean: motifRows.length ? mean(motifRows.map((r) => r.symmetryGapHazard)) : null,
      motifCoarseEPMean: motifRows.length ? mean(motifRows.map((r) => r.coarseEPHazard)) : null,
      trackingDeltaHkMean: rows.length
        ? mean(rows.map((r) => r.hazard_minus_outside_Hk ?? 0))
        : null,
      trackingDeltaR2Mean: rows.length
        ? mean(rows.map((r) => r.hazard_minus_outside_R2 ?? 0))
        : null,
      trackingDeltaDmagMean: rows.length
        ? mean(rows.map((r) => r.hazard_minus_outside_dmag ?? 0))
        : null,
    });
  }

  const byVariantHazard = groupBy(rawRows, ["variant", "hazardIndex"]);
  for (const [key, rows] of byVariantHazard.entries()) {
    const [variant, hazardIndex] = key.split("|");
    const misses = rows.filter((r) => r.miss).length;
    const missFrac = rows.length > 0 ? misses / rows.length : 0;
    const recoverySteps = rows
      .filter((r) => Number.isFinite(r.recoverySteps))
      .map((r) => r.recoverySteps);
    const motifRows = rows.filter((r) => r.motifStatsOkHazard);
    byHazardRows.push({
      variant,
      hazardIndex: Number(hazardIndex),
      missFrac,
      recoveryMedian: percentile(recoverySteps, 0.5),
      uptimeTailMean: rows.length ? mean(rows.map((r) => r.uptimeTail)) : 0,
      epRepairPerActionMean: rows.length ? mean(rows.map((r) => r.epRepairPerAction)) : 0,
      motifEntropyMean: motifRows.length ? mean(motifRows.map((r) => r.motifEntropyHazard)) : null,
      motifSymmetryGapMean: motifRows.length ? mean(motifRows.map((r) => r.symmetryGapHazard)) : null,
      motifCoarseEPMean: motifRows.length ? mean(motifRows.map((r) => r.coarseEPHazard)) : null,
      trackingDeltaHkMean: rows.length
        ? mean(rows.map((r) => r.hazard_minus_outside_Hk ?? 0))
        : null,
      trackingDeltaR2Mean: rows.length
        ? mean(rows.map((r) => r.hazard_minus_outside_R2 ?? 0))
        : null,
      trackingDeltaDmagMean: rows.length
        ? mean(rows.map((r) => r.hazard_minus_outside_dmag ?? 0))
        : null,
    });
  }

  const motifGroups = new Map();
  for (const entry of motifByHazardAgg.values()) {
    motifGroups.set(`${entry.variant}|${entry.hazardIndex}`, entry);
  }

  const jsByVariant = new Map();
  for (const entry of motifGroups.values()) {
    const key = entry.variant;
    const list = jsByVariant.get(key) ?? [];
    list.push(entry);
    jsByVariant.set(key, list);
  }

  for (const entry of motifGroups.values()) {
    const counts = entry.counts;
    let total = 0;
    for (const val of counts.values()) total += val;
    if (total === 0) continue;
    const sorted = Array.from(counts.entries()).sort((a, b) => b[1] - a[1]);
    const top = sorted.slice(0, 10);
    let rank = 0;
    for (const [motifId, count] of top) {
      motifByHazardRows.push({
        variant: entry.variant,
        hazardIndex: entry.hazardIndex,
        motifId,
        prob: count / total,
        rank,
      });
      rank += 1;
    }
  }

  for (const [variant, entries] of jsByVariant.entries()) {
    const hazardMap = new Map();
    for (const entry of entries) {
      hazardMap.set(entry.hazardIndex, entry.counts);
    }
    const ctx = computeContextSensitivity(hazardMap);
    contextSensitivityRows.push({ variant, ...ctx });
    for (const row of summaryRows) {
      if (row.variant === variant) {
        row.avgPairwiseJSD = ctx.avgPairwiseJSD;
        row.mutualInfoHM = ctx.mutualInfoHM;
      }
    }
    for (const row of byHazardRows) {
      if (row.variant === variant) {
        row.avgPairwiseJSD = ctx.avgPairwiseJSD;
        row.mutualInfoHM = ctx.mutualInfoHM;
      }
    }
  }

  const byVariantEpoch = groupBy(rawRows, ["variant", "epochIndex"]);
  for (const [key, rows] of byVariantEpoch.entries()) {
    const [variant, epochIndexStr] = key.split("|");
    const epochIndex = Number(epochIndexStr);
    const first = rows.filter((r) => r.eventIndexWithinEpoch === 0);
    const last = rows.filter((r) => r.eventIndexWithinEpoch === Math.max(...rows.map((r) => r.eventIndexWithinEpoch)));
    const deltaMiss = first.length && last.length
      ? mean(last.map((r) => (r.miss ? 1 : 0))) - mean(first.map((r) => (r.miss ? 1 : 0)))
      : null;
    const deltaCoarseEP = first.length && last.length
      ? mean(last.map((r) => r.coarseEPHazard ?? 0)) - mean(first.map((r) => r.coarseEPHazard ?? 0))
      : null;
    const deltaSymmetry = first.length && last.length
      ? mean(last.map((r) => r.symmetryGapHazard ?? 0)) - mean(first.map((r) => r.symmetryGapHazard ?? 0))
      : null;
    trackingRows.push({
      variant,
      epochIndex,
      hazardIndex: rows[0]?.hazardIndex ?? null,
      deltaMiss,
      deltaCoarseEP,
      deltaSymmetryGap: deltaSymmetry,
      trackingDeltaHkMean: rows.length
        ? mean(rows.map((r) => r.hazard_minus_outside_Hk ?? 0))
        : null,
      trackingDeltaR2Mean: rows.length
        ? mean(rows.map((r) => r.hazard_minus_outside_R2 ?? 0))
        : null,
      trackingDeltaDmagMean: rows.length
        ? mean(rows.map((r) => r.hazard_minus_outside_dmag ?? 0))
        : null,
    });
  }

  if (writeOutputs) {
    const outDir = path.resolve(rootDir, ".tmp", "homeostasis");
    ensureDir(outDir);

    const rawPath = path.join(outDir, "moving_hazard_raw.jsonl");
    fs.writeFileSync(rawPath, rawRows.map((row) => JSON.stringify(row)).join("\n") + "\n");

    const summaryHeader = [
      "variant",
      "missFrac",
      "uptimeTailMean",
      "errTailMean",
      "recoveryMedian",
      "recoveryP95",
      "epTotalMean",
      "epRepairMean",
      "epOpKMean",
      "epClockMean",
      "epTotalRateMean",
      "epRepairPerActionMean",
      "epRepairPerActionMedian",
      "motifEntropyMean",
      "motifSymmetryGapMean",
      "motifCoarseEPMean",
      "avgPairwiseJSD",
      "mutualInfoHM",
      "trackingDeltaHkMean",
      "trackingDeltaR2Mean",
      "trackingDeltaDmagMean",
    ];
    const summaryLines = [summaryHeader.join(",")];
    for (const row of summaryRows) {
      summaryLines.push(summaryHeader.map((k) => row[k]).join(","));
    }
    fs.writeFileSync(path.join(outDir, "moving_hazard_summary.csv"), summaryLines.join("\n") + "\n");

    const byHazardHeader = [
      "variant",
      "hazardIndex",
      "missFrac",
      "recoveryMedian",
      "uptimeTailMean",
      "epRepairPerActionMean",
      "motifEntropyMean",
      "motifSymmetryGapMean",
      "motifCoarseEPMean",
      "avgPairwiseJSD",
      "mutualInfoHM",
      "trackingDeltaHkMean",
      "trackingDeltaR2Mean",
      "trackingDeltaDmagMean",
    ];
    const byHazardLines = [byHazardHeader.join(",")];
    for (const row of byHazardRows) {
      byHazardLines.push(byHazardHeader.map((k) => row[k]).join(","));
    }
    fs.writeFileSync(path.join(outDir, "moving_hazard_by_hazard.csv"), byHazardLines.join("\n") + "\n");

    const motifHeader = ["variant", "hazardIndex", "motifId", "prob", "rank"];
    const motifLines = [motifHeader.join(",")];
    for (const row of motifByHazardRows) {
      motifLines.push(motifHeader.map((k) => row[k]).join(","));
    }
    fs.writeFileSync(path.join(outDir, "moving_hazard_motif_by_hazard.csv"), motifLines.join("\n") + "\n");

    const ctxHeader = [
      "variant",
      "avgPairwiseJSD",
      "mutualInfoHM",
      "motifAlphabetSize",
      "sampleCount",
    ];
    const ctxLines = [ctxHeader.join(",")];
    for (const row of contextSensitivityRows) {
      ctxLines.push(ctxHeader.map((k) => row[k]).join(","));
    }
    fs.writeFileSync(
      path.join(outDir, "moving_hazard_context_sensitivity.csv"),
      ctxLines.join("\n") + "\n",
    );

    const trackingHeader = [
      "variant",
      "epochIndex",
      "hazardIndex",
      "deltaMiss",
      "deltaCoarseEP",
      "deltaSymmetryGap",
      "trackingDeltaHkMean",
      "trackingDeltaR2Mean",
      "trackingDeltaDmagMean",
    ];
    const trackingLines = [trackingHeader.join(",")];
    for (const row of trackingRows) {
      trackingLines.push(trackingHeader.map((k) => row[k]).join(","));
    }
    fs.writeFileSync(path.join(outDir, "moving_hazard_tracking.csv"), trackingLines.join("\n") + "\n");
  }

  return {
    rawRows,
    summaryRows,
    byHazardRows,
    motifByHazardRows,
    trackingRows,
    finalStates,
  };
}

if (import.meta.url === `file://${process.argv[1]}`) {
  const args = parseArgs(process.argv);
  const presetPath = pickPreset();
  const variants = args.variants.split(",").map((v) => v.trim()).filter(Boolean);
  const seeds = parseSeedList(args.seeds);
  const result = await runMovingHazardHomeostasis({
    presetPath,
    seeds,
    steps: args.steps,
    eventEvery: args.eventEvery,
    deadline: args.deadline,
    reportEvery: args.reportEvery,
    hazardHoldEvents: args.hazardHoldEvents,
    hazardCount: args.hazardCount,
    region: args.region,
    logMotifs: args.logMotifs === 1,
    variants,
    errGood: args.errGood,
    sdiffGood: args.sdiffGood,
    corruptFrac: args.corruptFrac,
    tailWindow: args.tailWindow,
  });

  const summaryLine = result.summaryRows
    .map((row) => `${row.variant}: miss=${row.missFrac.toFixed(3)}`)
    .join(" | ");
  const motifLine = result.summaryRows
    .map((row) => `${row.variant}: H=${row.motifEntropyMean ?? "na"}, gap=${row.motifSymmetryGapMean ?? "na"}`)
    .join(" | ");
  const trackingAvailable = result.summaryRows.some((row) => row.trackingDeltaHkMean !== null);

  console.log(`MOVING_HAZARD_SUMMARY: ${summaryLine}`);
  console.log(`MOVING_HAZARD_MOTIFS: ${motifLine}`);
  console.log(`TRACKING_AVAILABLE: ${trackingAvailable}`);
}
</file>

<file path="scripts/run-moving-hazard-null-control.mjs">
#!/usr/bin/env node
import fs from "node:fs";
import path from "node:path";
import { runMovingHazardHomeostasis } from "./run-moving-hazard-homeostasis.mjs";

function mean(values) {
  if (!values.length) return 0;
  return values.reduce((acc, v) => acc + v, 0) / values.length;
}

const rootDir = path.resolve(path.dirname(new URL(import.meta.url).pathname), "..");
const outDir = path.resolve(rootDir, ".tmp", "homeostasis");
fs.mkdirSync(outDir, { recursive: true });

const presetPathCandidates = [
  path.resolve(rootDir, "scripts/params/clock_code/deadline_fidelity_found.json"),
  path.resolve(rootDir, "scripts/params/clock_code/code_deadline_gated_clock.json"),
];
const presetPath = presetPathCandidates.find((p) => fs.existsSync(p)) ?? presetPathCandidates[1];

const baseConfig = {
  presetPath,
  seeds: [1, 2, 3, 4, 5],
  steps: 1_000_000,
  eventEvery: 50_000,
  deadline: 15_000,
  reportEvery: 1_000,
  hazardHoldEvents: 4,
  hazardCount: 8,
  region: "stripe",
  logMotifs: true,
  variants: ["C"],
  writeOutputs: false,
};

const driven = await runMovingHazardHomeostasis({
  ...baseConfig,
});

const nullish = await runMovingHazardHomeostasis({
  ...baseConfig,
  commonOverrides: { p6On: 0, etaDrive: 0, p3On: 0 },
});

const pickSummary = (res) => res.summaryRows.find((row) => row.variant === "C");
const pickRows = (res) => res.rawRows.filter((row) => row.variant === "C");
const drivenRow = pickSummary(driven);
const nullRow = pickSummary(nullish);
const drivenRows = pickRows(driven);
const nullRows = pickRows(nullish);

const header = [
  "condition",
  "missFrac",
  "motifEntropyMean",
  "motifSymmetryGapMean",
  "motifCoarseEPMean",
  "epTotalRate",
  "epRepairMean",
  "epOpKMean",
  "epClockMean",
].join(",");
const lines = [header];

const addLine = (label, row) => {
  const epTotalRate = row ? row.epTotalRateMean : null;
  lines.push(
    [
      label,
      row?.missFrac ?? null,
      row?.motifEntropyMean ?? null,
      row?.motifSymmetryGapMean ?? null,
      row?.motifCoarseEPMean ?? null,
      epTotalRate,
      row?.epRepairMean ?? null,
      row?.epOpKMean ?? null,
      row?.epClockMean ?? null,
    ].join(","),
  );
};

addLine("driven", drivenRow);
addLine("null", nullRow);

const outPath = path.join(outDir, "moving_hazard_null_control.csv");
fs.writeFileSync(outPath, lines.join("\n") + "\n");

const finishingHeader = [
  "condition",
  "epTotalRate",
  "epRepairMean",
  "epRepairPerActionMean",
  "repairActionCountMean",
].join(",");
const finishingLines = [finishingHeader];

const addFinishing = (label, summaryRow, rows) => {
  const repairCounts = rows.map((r) => r.repairCount ?? 0);
  finishingLines.push(
    [
      label,
      summaryRow?.epTotalRateMean ?? null,
      summaryRow?.epRepairMean ?? null,
      summaryRow?.epRepairPerActionMean ?? null,
      mean(repairCounts),
    ].join(","),
  );
};

addFinishing("driven", drivenRow, drivenRows);
addFinishing("null", nullRow, nullRows);

const finishingPath = path.join(outDir, "finishing_null_control_ep_per_action.csv");
fs.writeFileSync(finishingPath, finishingLines.join("\n") + "\n");

const deltaSym = (drivenRow?.motifSymmetryGapMean ?? 0) - (nullRow?.motifSymmetryGapMean ?? 0);
const deltaCoarse = (drivenRow?.motifCoarseEPMean ?? 0) - (nullRow?.motifCoarseEPMean ?? 0);
const deltaEp = (drivenRow?.epTotalRateMean ?? 0) - (nullRow?.epTotalRateMean ?? 0);

console.log(
  `Driven vs Null: ΔsymmetryGap=${deltaSym.toFixed(4)} ΔcoarseEP=${deltaCoarse.toFixed(4)} ΔepTotalRate=${deltaEp.toFixed(4)}`,
);
</file>

<file path="scripts/run-moving-hazard-speed-sweep.mjs">
#!/usr/bin/env node
import fs from "node:fs";
import path from "node:path";
import { runMovingHazardHomeostasis } from "./run-moving-hazard-homeostasis.mjs";

const rootDir = path.resolve(path.dirname(new URL(import.meta.url).pathname), "..");
const outDir = path.resolve(rootDir, ".tmp", "homeostasis");
fs.mkdirSync(outDir, { recursive: true });

const presetPathCandidates = [
  path.resolve(rootDir, "scripts/params/clock_code/deadline_fidelity_found.json"),
  path.resolve(rootDir, "scripts/params/clock_code/code_deadline_gated_clock.json"),
];
const presetPath = presetPathCandidates.find((p) => fs.existsSync(p)) ?? presetPathCandidates[1];

const seeds = [1, 2, 3, 4, 5];
const hazardHoldEventsList = [1, 2, 4, 8, 16];

const baseConfig = {
  presetPath,
  seeds,
  steps: 1_000_000,
  eventEvery: 50_000,
  deadline: 15_000,
  reportEvery: 1_000,
  hazardCount: 8,
  region: "stripe",
  logMotifs: true,
  variants: ["A", "C"],
  writeOutputs: false,
};

const rows = [];
for (const hold of hazardHoldEventsList) {
  const res = await runMovingHazardHomeostasis({
    ...baseConfig,
    hazardHoldEvents: hold,
  });
  for (const row of res.summaryRows) {
    rows.push({
      hazardHoldEvents: hold,
      variant: row.variant,
      missFrac: row.missFrac,
      uptimeTailMean: row.uptimeTailMean,
      recoveryP95: row.recoveryP95,
      epTotalRateMean: row.epTotalRateMean,
      epRepairPerActionMean: row.epRepairPerActionMean,
      avgPairwiseJSD: row.avgPairwiseJSD,
      mutualInfoHM: row.mutualInfoHM,
    });
  }
}

const header = [
  "variant",
  "hazardHoldEvents",
  "missFrac",
  "uptimeTailMean",
  "recoveryP95",
  "epTotalRateMean",
  "epRepairPerActionMean",
  "avgPairwiseJSD",
  "mutualInfoHM",
].join(",");
const lines = [header];
for (const row of rows) {
  lines.push(
    [
      row.variant,
      row.hazardHoldEvents,
      row.missFrac,
      row.uptimeTailMean,
      row.recoveryP95,
      row.epTotalRateMean,
      row.epRepairPerActionMean,
      row.avgPairwiseJSD,
      row.mutualInfoHM,
    ].join(","),
  );
}

const outPath = path.join(outDir, "finishing_speed_sweep.csv");
fs.writeFileSync(outPath, lines.join("\n") + "\n");

const pickBest = (variantRows) => {
  if (!variantRows.length) return null;
  let best = variantRows[0];
  for (const row of variantRows.slice(1)) {
    if (row.missFrac < best.missFrac) {
      best = row;
    } else if (row.missFrac === best.missFrac) {
      if (row.uptimeTailMean > best.uptimeTailMean) {
        best = row;
      } else if (row.uptimeTailMean === best.uptimeTailMean) {
        if (row.epTotalRateMean < best.epTotalRateMean) best = row;
      }
    }
  }
  return best;
};

const rowsA = rows.filter((r) => r.variant === "A");
const rowsC = rows.filter((r) => r.variant === "C");
const bestA = pickBest(rowsA);
const bestC = pickBest(rowsC);

const bestLine = [
  `BEST_SPEED_A=${bestA ? bestA.hazardHoldEvents : "na"}`,
  `BEST_SPEED_C=${bestC ? bestC.hazardHoldEvents : "na"}`,
].join(" ");
console.log(bestLine);
</file>

<file path="scripts/run-moving-hazard-stationary-vs-moving.mjs">
#!/usr/bin/env node
import fs from "node:fs";
import path from "node:path";
import { runMovingHazardHomeostasis } from "./run-moving-hazard-homeostasis.mjs";

const rootDir = path.resolve(path.dirname(new URL(import.meta.url).pathname), "..");
const outDir = path.resolve(rootDir, ".tmp", "homeostasis");
fs.mkdirSync(outDir, { recursive: true });

const presetPathCandidates = [
  path.resolve(rootDir, "scripts/params/clock_code/deadline_fidelity_found.json"),
  path.resolve(rootDir, "scripts/params/clock_code/code_deadline_gated_clock.json"),
];
const presetPath = presetPathCandidates.find((p) => fs.existsSync(p)) ?? presetPathCandidates[1];

const seeds = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10];
const baseConfig = {
  presetPath,
  seeds,
  steps: 2_000_000,
  eventEvery: 50_000,
  deadline: 15_000,
  reportEvery: 1_000,
  hazardHoldEvents: 4,
  region: "stripe",
  logMotifs: true,
  variants: ["A", "C"],
};

const stationary = await runMovingHazardHomeostasis({
  ...baseConfig,
  hazardCount: 1,
  writeOutputs: false,
});

const moving = await runMovingHazardHomeostasis({
  ...baseConfig,
  hazardCount: 8,
  writeOutputs: false,
});

const pickRows = (res) => res.summaryRows.map((row) => ({ ...row }));

const rows = [];
for (const row of pickRows(stationary)) {
  rows.push({ ...row, scenario: "stationary" });
}
for (const row of pickRows(moving)) {
  rows.push({ ...row, scenario: "moving" });
}

const header = [
  "variant",
  "scenario",
  "missFrac",
  "uptimeTailMean",
  "errTailMean",
  "recoveryP95",
  "epTotalRateMean",
  "epRepairPerActionMean",
  "motifEntropyMean",
  "motifSymmetryGapMean",
  "avgPairwiseJSD",
  "mutualInfoHM",
].join(",");

const lines = [header];
for (const row of rows) {
  lines.push(
    [
      row.variant,
      row.scenario,
      row.missFrac,
      row.uptimeTailMean,
      row.errTailMean,
      row.recoveryP95,
      row.epTotalRateMean,
      row.epRepairPerActionMean,
      row.motifEntropyMean,
      row.motifSymmetryGapMean,
      row.avgPairwiseJSD,
      row.mutualInfoHM,
    ].join(","),
  );
}

const outPath = path.join(outDir, "finishing_stationary_vs_moving.csv");
fs.writeFileSync(outPath, lines.join("\n") + "\n");

const byVariant = new Map();
for (const row of rows) {
  const list = byVariant.get(row.variant) ?? [];
  list.push(row);
  byVariant.set(row.variant, list);
}

for (const [variant, list] of byVariant.entries()) {
  const stationaryRow = list.find((r) => r.scenario === "stationary");
  const movingRow = list.find((r) => r.scenario === "moving");
  if (!stationaryRow || !movingRow) continue;
  const deltaMiss = movingRow.missFrac - stationaryRow.missFrac;
  const deltaJSD = (movingRow.avgPairwiseJSD ?? 0) - (stationaryRow.avgPairwiseJSD ?? 0);
  const deltaMI = (movingRow.mutualInfoHM ?? 0) - (stationaryRow.mutualInfoHM ?? 0);
  const deltaEP = (movingRow.epRepairPerActionMean ?? 0) - (stationaryRow.epRepairPerActionMean ?? 0);
  console.log(
    `${variant} Δ(moving-stationary): miss=${deltaMiss.toFixed(4)} JSD=${deltaJSD.toFixed(4)} MI=${deltaMI.toFixed(4)} epPerAction=${deltaEP.toFixed(6)}`,
  );
}
</file>

<file path="scripts/test-instrumentation-invariance.mjs">
#!/usr/bin/env node
import fs from "node:fs";
import path from "node:path";
import { runMovingHazardHomeostasis } from "./run-moving-hazard-homeostasis.mjs";

const rootDir = path.resolve(path.dirname(new URL(import.meta.url).pathname), "..");
const outDir = path.resolve(rootDir, ".tmp", "homeostasis");
fs.mkdirSync(outDir, { recursive: true });

const presetPathCandidates = [
  path.resolve(rootDir, "scripts/params/clock_code/deadline_fidelity_found.json"),
  path.resolve(rootDir, "scripts/params/clock_code/code_deadline_gated_clock.json"),
];
const presetPath = presetPathCandidates.find((p) => fs.existsSync(p)) ?? presetPathCandidates[1];

const baseConfig = {
  presetPath,
  seeds: [1],
  steps: 300_000,
  eventEvery: 50_000,
  deadline: 15_000,
  reportEvery: 1_000,
  hazardHoldEvents: 2,
  hazardCount: 8,
  region: "stripe",
  variants: ["C"],
  writeOutputs: false,
};

const runWith = async (logMotifs) => {
  const res = await runMovingHazardHomeostasis({
    ...baseConfig,
    logMotifs,
  });
  return res.finalStates[0];
};

const a = await runWith(true);
const b = await runWith(false);

const rows = [];
const addRow = (name, v1, v2, tol) => {
  const diff = Math.abs(v1 - v2);
  const pass = diff <= tol;
  rows.push({ metric: name, valueA: v1, valueB: v2, diff, pass });
};

addRow("epExactTotal", a.epExactTotal, b.epExactTotal, 1e-9);
addRow("epRepairTotal", a.epRepairTotal, b.epRepairTotal, 1e-9);
addRow("epOpKTotal", a.epOpKTotal, b.epOpKTotal, 1e-9);
addRow("epClockTotal", a.epClockTotal, b.epClockTotal, 1e-9);
addRow("countP5Base", a.countP5Base, b.countP5Base, 0);
addRow("countP5Meta", a.countP5Meta, b.countP5Meta, 0);
addRow("countOpK", a.countOpK, b.countOpK, 0);
addRow("countClock", a.countClock, b.countClock, 0);
addRow("baseSum", a.baseSum, b.baseSum, 0);
addRow("metaSum", a.metaSum, b.metaSum, 0);
addRow("baseLen", a.baseLen, b.baseLen, 0);
addRow("metaLen", a.metaLen, b.metaLen, 0);

const header = "metric,valueA,valueB,diff,pass";
const lines = [header];
for (const row of rows) {
  lines.push([row.metric, row.valueA, row.valueB, row.diff, row.pass].join(","));
}
const outPath = path.join(outDir, "instrumentation_invariance.csv");
fs.writeFileSync(outPath, lines.join("\n") + "\n");

const allPass = rows.every((row) => row.pass);
console.log(`INSTRUMENTATION_INVARIANCE: ${allPass ? "PASS" : "FAIL"}`);
</file>

<file path="apps/web/src/sim/workerClient.ts">
import type { SimMessage, SimRequest, SimSnapshot } from "./workerMessages";
// Use Vite's ?worker import for proper bundling (compiles TS and handles production correctly)
import SimWorker from "./sim.worker.ts?worker";

export class SimWorkerClient {
  private readonly worker: Worker;
  private snapshotHandlers: Array<(s: SimSnapshot) => void> = [];
  private errorHandlers: Array<(m: string) => void> = [];
  private debugHandlers: Array<(m: string) => void> = [];
  private readyHandlers: Array<() => void> = [];
  // Queue messages that arrive before handlers are registered
  private pendingDebug: string[] = [];
  private pendingReady = false;
  private terminated = false;

  constructor() {
    try {
      this.worker = new SimWorker();
    } catch (err) {
      const message = err instanceof Error ? err.message : String(err);
      throw new Error(`Failed to create worker: ${message}`);
    }

    this.worker.onmessage = (ev: MessageEvent<SimMessage>) => {
      const msg = ev.data;
      if (msg.type === "snapshot") this.snapshotHandlers.forEach((h) => h(msg.snapshot));
      if (msg.type === "ready") {
        if (this.readyHandlers.length) {
          this.readyHandlers.forEach((h) => h());
        } else {
          this.pendingReady = true;
        }
      }
      if (msg.type === "error") this.errorHandlers.forEach((h) => h(msg.message));
      if (msg.type === "debug") {
        if (this.debugHandlers.length) {
          this.debugHandlers.forEach((h) => h(msg.message));
        } else {
          this.pendingDebug.push(msg.message);
        }
      }
    };
    this.worker.onerror = (ev) => {
      const detail =
        ev instanceof ErrorEvent
          ? `${ev.message}${ev.filename ? ` (${ev.filename}:${ev.lineno}:${ev.colno})` : ""}`
          : String(ev);
      this.errorHandlers.forEach((h) => h(`Worker error: ${detail}`));
    };
    this.worker.onmessageerror = () => {
      this.errorHandlers.forEach((h) => h("Worker message error (structured clone failed)."));
    };
  }

  onSnapshot(handler: (s: SimSnapshot) => void) {
    this.snapshotHandlers.push(handler);
    return () => (this.snapshotHandlers = this.snapshotHandlers.filter((h) => h !== handler));
  }

  onReady(handler: () => void) {
    this.readyHandlers.push(handler);
    // Flush pending ready if it arrived before handler was registered
    if (this.pendingReady) {
      this.pendingReady = false;
      handler();
    }
    return () => (this.readyHandlers = this.readyHandlers.filter((h) => h !== handler));
  }

  onError(handler: (m: string) => void) {
    this.errorHandlers.push(handler);
    return () => (this.errorHandlers = this.errorHandlers.filter((h) => h !== handler));
  }

  onDebug(handler: (m: string) => void) {
    this.debugHandlers.push(handler);
    // Flush pending debug messages that arrived before handler was registered
    if (this.pendingDebug.length) {
      const pending = this.pendingDebug;
      this.pendingDebug = [];
      pending.forEach((m) => handler(m));
    }
    return () => (this.debugHandlers = this.debugHandlers.filter((h) => h !== handler));
  }

  send(req: SimRequest) {
    if (this.terminated) {
      return;
    }
    this.worker.postMessage(req);
  }

  terminate() {
    this.terminated = true;
    this.worker.terminate();
  }
}
</file>

<file path="apps/web/src/wasm/.gitkeep">

</file>

<file path="apps/web/src/wasm/README.md">
# Generated WASM bindings

This folder is intentionally mostly empty in git.

Build the Rust/WASM package into:

`apps/web/src/wasm/sim_core`

via:

```bash
cd crates/sim-core
wasm-pack build --target web --out-dir ../../apps/web/src/wasm/sim_core --out-name sim_core
```
</file>

<file path="apps/web/src/main.tsx">
import React from "react";
import ReactDOM from "react-dom/client";
import App from "./App";
import "./style.css";

ReactDOM.createRoot(document.getElementById("root")!).render(
  <React.StrictMode>
    <App />
  </React.StrictMode>,
);
</file>

<file path="apps/web/src/vite-env.d.ts">
/// <reference types="vite/client" />
</file>

<file path="apps/web/index.html">
<!doctype html>
<html lang="en">
  <head>
    <meta charset="UTF-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>Ratchet Playground</title>
  </head>
  <body>
    <div id="root"></div>
    <script type="module" src="/src/main.tsx"></script>
  </body>
</html>
</file>

<file path="apps/web/package.json">
{
  "name": "ratchet-playground-web",
  "private": true,
  "version": "0.1.0",
  "type": "module",
  "scripts": {
    "dev": "vite",
    "build": "tsc -b && vite build",
    "preview": "vite preview"
  },
  "dependencies": {
    "react": "^18.3.1",
    "react-dom": "^18.3.1"
  },
  "devDependencies": {
    "@types/react": "^18.3.18",
    "@types/react-dom": "^18.3.5",
    "@vitejs/plugin-react": "^4.3.4",
    "typescript": "^5.7.2",
    "vite": "^6.0.4"
  }
}
</file>

<file path="apps/web/tsconfig.json">
{
  "compilerOptions": {
    "target": "ES2022",
    "useDefineForClassFields": true,
    "lib": ["ES2022", "DOM", "DOM.Iterable"],
    "module": "ESNext",
    "skipLibCheck": true,
    "moduleResolution": "Bundler",
    "resolveJsonModule": true,
    "isolatedModules": true,
    "noEmit": true,
    "jsx": "react-jsx",
    "strict": true,
    "types": ["vite/client"]
  },
  "include": ["src"]
}
</file>

<file path="apps/web/vite.config.ts">
import react from "@vitejs/plugin-react";
import { defineConfig } from "vite";

export default defineConfig({
  plugins: [react()],
  worker: {
    format: "es",
  },
});
</file>

<file path="crates/sim-core/Cargo.toml">
[package]
name = "sim-core"
version = "0.1.0"
edition = "2021"

[lib]
crate-type = ["cdylib", "rlib"]

[dependencies]
js-sys = "0.3"
wasm-bindgen = "0.2"
</file>

<file path="docs/materializations/adaptive_homeostatis.md">
“Adaptive homeostasis via rewritable coupling” is the idea that the system doesn’t just *repair state* (S/A/N/…)—it **repairs and reconfigures the *rules of repair***.

In your sandbox language:

* **Symptoms** live in the ordinary substrates: mismatch, corruption, deadline misses, code error, etc.
* **Regulation** lives in the rewritable coupling substrates: (K) (and later ( \omega )).
* **Energy** lives in P6 (and the EP ledger): you can’t actively stabilize anything without paying dissipation.

Once coupling is writable, you’re basically giving the system the ability to form a **distributed thermostat network**—but not a single thermostat. More like: millions of tiny valves that can self-tune.

Here’s a playful but concrete mental model for what can happen.

---

## The core homeostasis loop in one picture

Think of each location (q) and interface (\ell-1\to\ell) having:

* an “error” (a symptom):
  [
  e^{(\ell)}(q) = \underbrace{\text{upper}(q)}*{\text{what is}} ;-; \underbrace{\sum*{r\in R} k^{(\ell)}(q,r),\text{lower}(q+r)}_{\text{what coupling predicts}}
  ]
* a coupling knob (the regulator):
  [
  k^{(\ell)}(q,\cdot) \in \Delta^{|R|}\quad\text{implemented as token budget }K^{(\ell)}(q,r)
  ]
* and a drive channel that can bias updates to reduce error:
  [
  W_{\text{align}} \propto - \eta_{\text{drive}},\Delta\left(\tfrac12 e^2\right)
  ]

Now inject noise/damage somewhere (your codeNoise, corruption events, deadline constraints). That pushes (e) up locally. If P6 is on, the system has a way to spend EP so that moves that reduce (e^2) are statistically favored. But the crucial upgrade is:

> **It can reduce error either by changing the state, or by changing how the coupling “interprets” / routes state.**

That second channel is where “adaptive homeostasis” lives.

---

## What counts as *adaptive* homeostasis (not just “stability”)?

A system is merely “stable” if it returns to baseline after small perturbations.

It’s *adaptive homeostasis* if:

1. the response depends on **where/how** the perturbation happened (localized, structured response),
2. the response is **resource-aware** (budgets reallocate: some places get stronger coupling, others get weaker),
3. the response **persists** or “learns” if the perturbation is chronic (allostasis / scarring),
4. and crucially: when you turn off P6, the maintenance collapses (it’s genuinely dissipation-priced).

Rewritable coupling is what gives you (1)–(3) without adding any detector logic.

---

## Six “organism-like” homeostasis archetypes you can plausibly see

### 1) Wound healing as rerouting

**Picture:** You corrupt a patch (or increase noise rate) in quadrant 2. The “damage” isn’t repaired by brute force everywhere; instead, (K) reallocates routing weights so that upper-layer cells start “listening” to *healthy neighbors* more than damaged ones.

What it looks like in fields:

* Near the wound, (K(q,\cdot)) becomes **anisotropic**: it points away from the damaged region.
* You get a “bypass” operator: upper layer reconstructs from a ring of intact context.

What it looks like in time:

* fast: the state stabilizes locally
* slower: the coupling fabric reorganizes into a new stable configuration

This is exactly how real tissues heal: not by restoring every microstructure, but by rerouting function around damage.

---

### 2) Immune patrol (scan-based repair) that *self-optimizes*

You already engineered a gating scanline for deadline repair, and saw why drift beats random walks when deadlines bite.

With rewritable coupling, that scanline can become *endogenous*:

* (K) evolves into a moving “active window” (a coupling wave)
* because a traveling repair wave is the easiest way to guarantee bounded revisit times under noise

Homeostatic adaptation shows up when:

* noise shifts location → wave speed increases or the wave path bends
* noise becomes chronic → the wave “parks” more frequently in that region (visit frequency rises)

That’s immune patrol: a roaming maintenance process whose route adapts to threats.

---

### 3) Vascularization (repair highways)

Budgets matter. If coupling capacity is scarce, you can’t make every region perfectly connected and robust.

A very generic response under scarcity is:

* create **high-capacity corridors** (token-dense routes)
* leave the rest more weakly connected

This produces “vascular” patterns:

* highways where information/repair flows quickly
* capillary-like weak coupling elsewhere

Homeostasis becomes *adaptive* when those highways **move** or **reconfigure** as the environment changes (noise/hazards migrate).

If you ever see sparse, persistent channels in (d(q)=\sum_r k(q,r)r), you’re basically watching a vasculature emerge.

---

### 4) Homeostatic oscillations (the thermostat rings)

Control systems often oscillate when gain is high or delays exist.

Here, delays are inherent:

* state changes affect mismatch,
* mismatch affects coupling updates,
* coupling updates affect future mismatch.

If (K) adapts “too aggressively,” you can get:

* overshoot → oscillation
* traveling oscillations → waves
* global oscillations → “stress cycles”

This is *interesting* homeostasis: it’s not “perfect stability,” it’s a regulated breathing pattern.

In your EP ledger, these regimes often show:

* periodic bursts of epRepair / epOpK
* and improved uptime despite oscillations

---

### 5) Scar tissue and memory

A very real possibility: after repeated damage in one region, (K) no longer returns to its old distribution even if damage stops.

That’s not a bug. It’s a minimal model of “history-dependent morphology.”

Two versions:

* **scar tissue (maladaptive memory):** coupling becomes rigid / overfocused and reduces flexibility elsewhere
* **immune memory (adaptive memory):** future recovery is faster because (K) is already biased into a useful routing configuration

This is where you start seeing *learning-like* behavior without any explicit learning rule: it’s just persistent configurations in the operator substrate.

---

### 6) Allostasis: shifting setpoints under chronic stress

Sometimes the best strategy isn’t to keep the original setpoint; it’s to change what “normal” is.

In your terms:

* maybe perfect cross-layer alignment is too expensive at high noise
* the system might adopt a new regime where it maintains **a coarser abstraction** (higher layer becomes a low-pass version), because that’s what can be kept stable under constraints

So the macrovariables shift:

* code error might stabilize at a nonzero value, but uptime stays high
* coupling entropy rises (more averaging), trading detail for stability

That’s allostasis: stability through change.

---

## Why rewritable coupling is a “pure” homeostasis mechanism

Because it gives you the two ingredients homeostasis always needs:

1. **sensing**
   (mismatch / local instability / deadlines / missed repairs)

2. **actuation**
   (change the transition structure itself by changing (K), i.e., change what gets updated/propagated)

And it does this without a detector: mismatch is already there as part of the dynamics, and K updates are just token exchanges.

So you’re not “engineering a controller.” You’re giving the system a **control substrate** and letting ratchets discover currents through it.

---

## What would be the signature that you *really* got adaptive homeostasis?

Here are signatures that don’t rely on you naming a “goal,” and they’re testable with your existing harness style.

### 1) Robustness curves under shifting environments

Introduce a moving “hazard patch” (noise rate field that moves across the grid). Measure:

* uptimeTail (fraction of time code is “good”)
* deadline miss fraction
* recovery time distribution (especially 95th percentile)

Then compare:

* fixed coupling vs rewritable coupling
* same EP budget

Adaptive homeostasis shows up as:

* higher uptimeTail at same EP, **and**
* smaller sensitivity to hazard drift speed (tracks changes)

### 2) Budget reallocation patterns

Track “where coupling capacity goes”:

* (H_K(q)) entropy of kernels
* (|d(q)|) focus / routing strength
* total token flux into/out of a region

When hazards move, adaptive homeostasis looks like:

* coupling focus follows hazards with a lag
* after hazards stop, either returns (no memory) or stays biased (memory)

### 3) “Anticipatory” improvement with repeated perturbations

Do a training/testing schedule:

* apply identical perturbation pattern multiple times
* measure recovery time each episode

If recovery gets faster without changing parameters, the only place that “memory” can live is in the coupling substrate. That’s adaptive regulation.

### 4) EP efficiency improves

You already compute EP buckets. Define:

* EP per avoided miss
* EP per unit uptimeTail gained
* EP per recovered-cell

Homeostatic adaptation is not just “more EP → better stability.”
It’s “with the same EP, the system does better because it routes and targets repair more intelligently.”

---

## Failure modes (also informative)

Adaptive homeostasis has “pathologies,” which is fun because they’re also biologically real:

* **autoimmune-like overrepair:** coupling overfocuses on noisy regions, destabilizes everything else
* **cancer-like runaway:** coupling becomes self-reinforcing around a bad attractor, hogs budget
* **anemia:** budgets too small → cannot form highways; system stays brittle
* **chronic inflammation:** constant high EP spent, but uptime doesn’t improve (inefficient regulation)
* **scar rigidity:** great at one threat pattern, terrible at new ones

If you can get these by varying budgets/gains, you’ve basically built a little “regulatory pathology phase diagram.”

---

## The most out-of-the-box picture

Imagine you run with:

* moving hazard patch + deadlines
* P6 on
* K rewritable
* budgets tight

Over time, the system might self-organize into something like:

* a **mobile repair organ**: a coherent K-wave that acts like a roving maintenance unit
* a **vascular network**: stable channels that distribute repair influence
* and a **memory layer**: slow-changing K patterns that encode “where the world tends to be dangerous”

That’s an organism-like regulatory architecture, but made out of nothing but:

* local token budgets,
* reversible moves in null,
* and dissipative bias under drive.

---

## If you want a single “killer experiment” for this idea

You already know how to do deadlines + event stats.

Do this with rewritable coupling ON:

1. Start with hazard patch in quadrant 1 for 2M steps
2. Move hazard patch to quadrant 3 for 2M steps
3. Move it back to quadrant 1 for 2M steps

Measure:

* does recovery in step 3 happen faster than in step 1? (memory)
* does the coupling focus relocate correctly in step 2? (adaptation)
* does EP efficiency improve over episodes? (learning-like homeostasis)

If you see (a) relocation + (b) faster reacquisition + (c) stable null when P6 off, you’ve got the clean “adaptive homeostasis via rewritable coupling” story.

---

If you want, I can also propose a set of **noninvasive on-screen diagnostics** for K that make this visually obvious in the web app (vector field (d(q)), kernel entropy heatmap, token flux arrows), without any interpretation baked into dynamics.
</file>

<file path="docs/materializations/clockwork_fabric.md">
Clockwork fabric is the idea that **the *wiring* becomes the clock signal**.

Right now (with fixed η coupling), your “clock” is something like a single biased counter or a global protocol. With operator‑lifted coupling, you get something qualitatively different:

* a **distributed lattice of tiny “gears”** (the local kernel tokens (K^{(\ell)}(q,r))),
* whose collective motion can form **waves, spirals, scanlines, and oscillatory lattices**,
* and those spatiotemporal patterns become an **internal timing field** that can *also* route and repair information.

Think of it like moving from “a metronome” to “a whole mechanical clock mechanism spread across a fabric.”

Below are a few concrete mental pictures and how they’d emerge in your primitives world.

---

## What is the “fabric” made of?

At each interface (\ell-1\to\ell) and each cell (q), you have a **token budget** (B_K) spread across offsets (r\in\mathcal R):

[
K^{(\ell)}(q,r)\in{0,\dots,B_K},\quad \sum_r K^{(\ell)}(q,r)=B_K.
]

Normalize:
[
k^{(\ell)}(q,r)=K^{(\ell)}(q,r)/B_K.
]

This induces a local operator (feature‑agnostic):
[
\mathrm{pred}^{(\ell)}(q)=\sum_{r\in\mathcal R} k^{(\ell)}(q,r);\mathrm{lowerS}(q+r).
]

Now define a very intuitive “direction preference” vector for each cell:

[
d^{(\ell)}(q)=\sum_{r\in\mathcal R} k^{(\ell)}(q,r);r.
]

* If (d(q)) points right, that cell is “listening to” the right neighbor in the layer below.
* If (d(q)\approx 0), it’s mixing evenly (blur/coarse-grain).
* If (d(q)) points around a loop over time, it’s cycling “attention” direction.

So the fabric is literally a **field of tiny local routing arrows** (d(q)) (plus their magnitudes).

---

## What makes it “clockwork” instead of “random jitter”?

In null (P3 off, P6 off), the **joint system is reversible**, so any motion in (K) is unbiased: (d(q,t)) jitters but doesn’t march.

Clockwork appears when something creates a **nonzero current** in the operator degrees of freedom:

* **P6** can bias K‑updates (drive-only alignment work): it gives K a preferred direction of change in operator-space.
* **P3** can pump K through noncommuting update orderings (even without P6), producing a Floquet-like cycle in operator space.

Once K is driven, you can get a macroscopic phenomenon:

> **K does not just settle — it *circulates*.**
> That circulation is your “clock hand,” but distributed across space.

In the strict clock language: define a current (Q_K) that counts “token transfers around a cycle of offsets.” If offsets are arranged cyclically (e.g., Right→Up→Left→Down→Right), then every accepted transfer “rotating” the token mass contributes ±1. If the drift (\langle Q_K\rangle/t\neq 0), the operator field itself is a clock.

---

## Four archetypes of clockwork fabric

### 1) Scanline clock

Imagine (\mathcal R) is the cross stencil ({(0,0),(\pm1,0),(0,\pm1)}).

Suppose K organizes so that:

* at time (t), most cells have (d(q)) pointing right (routing from left neighbor),
* then K slowly rotates so (d(q)) points up,
* then left,
* then down,
* repeat.

But here’s the fabric twist: **phase need not be uniform**. You can have:

[
\theta(q,t)\approx k\cdot q - \omega t,
]
where (\theta) is the angle of (d(q)).

That’s a traveling wave of operator phase. The wavefront is a **scanline**: it sweeps across the lattice. Anything “gated” by the phase (repairs, writes, copying) happens when the scanline passes.

This is exactly the mechanism that beats random traversal under deadlines:

* scanline revisit time is almost deterministic (bounded)
* random walk revisit time has heavy tails

So the “clock” is literally the “repair truck” making rounds.

### 2) Spiral clock (distributed phase without a global clock)

In 2D, the most natural self-organized oscillator geometry is a spiral wave (think cardiac tissue / reaction-diffusion).

In your terms:

* (d(q)) rotates locally,
* but the rotation phase (\theta(q)) forms a spiral with a core,
* so each spatial location has a phase offset.

This is a **clock field** in the same way a vortex is a “direction field”:

* You can read time locally by reading the local phase.
* No one cell is “the clock.”

If you ever see a stable rotating defect (a spiral core) in (d(q,t)), that’s a huge signature of “clockwork fabric.”

### 3) Gear trains / conveyor belts (information transport)

Another archetype: K becomes sharply sparse so each cell essentially performs a shift:

* “upper copies lower from left” (shift operator),
* but spatially, different regions choose different shifts.

Then the coupling fabric creates **conveyor belts**: directed channels that transport patterns across layers and across space.

This can become a “clock” if the transport is periodic—e.g., a ring conveyor in a torus.

You’d literally have something like a **thermodynamic shift register**: information gets moved one step per cycle, and that cycle is the clock.

### 4) Synchronized metronomes (oscillator lattice)

Instead of waves, you might get many local oscillators that phase-lock:

* each cell’s K cycles through a small loop in operator-space,
* coupling via shared mismatch and shared S-fields synchronizes them.

Then you get “clockwork fabric” as a **grid-wide synchronous oscillation**—like a clock tree, but emergent.

This is closer to “a global clock signal” but still made from local stochastic pieces.

---

## Why this is weirdly powerful

Because K is not just a clock—K is also **the control plane**.

Once you let K evolve, you can get a three-way loop:

1. **K shapes information flow** (what upper layers “see”).
2. Information flow shapes mismatch.
3. Mismatch (under η or ηDrive) biases K updates.

That loop is the simplest physically honest version of “top-down causation”:

* no semantics,
* no detector,
* just co-evolution of state and operator.

Clockwork fabric is the special case where this loop locks into a **limit cycle** or traveling cycle.

---

## What I’d expect to happen first in practice

If you implement K‑coupling v1 (S only), and turn on P6 with drive-only alignment:

* At small (B_K): K becomes **sparse** (most budget on one or two offsets). You’ll see crisp (d(q)) arrows. This is the “gear teeth” regime—good for clockwork.
* At large (B_K): K becomes diffuse (blur). This is coarse-graining but less clockwork; phase becomes weak.
* Under noise+deadlines: K will tend to form **moving repair infrastructure**, and you may see scanline-like patterns because they’re the easiest way to guarantee bounded revisit times.

So: I’d expect the earliest clockwork fabric to look like **a traveling stripe of high “operator activity”** sweeping the grid—basically the emergent version of the gated repair you manually added, but now implemented by the coupling substrate itself.

---

## How to “see” clockwork fabric with simple diagnostics

Even without fancy detection logic, you can add noninvasive diagnostics that don’t affect dynamics:

### 1) Plot (d(q)) as a vector field

* magnitude = how “focused” routing is
* angle = phase

You’ll see:

* random arrows → molten wiring
* coherent arrows → routing
* rotating arrows → oscillators
* waves/spirals → clockwork fabric

### 2) Kernel entropy heatmap

[
H_K(q)=-\sum_r k(q,r)\log k(q,r)
]

* low entropy = “attention / gear”
* high entropy = “blur / abstraction”

Clockwork fabric tends to live in **intermediate entropy**: enough focus to have phase, enough flexibility to move.

### 3) Spatiotemporal correlation to detect wave speed

Compute correlation:
[
C(\Delta x,\Delta t)=\langle d(q,t)\cdot d(q+\Delta x,t+\Delta t)\rangle
]
A ridge in (C) reveals a wave with velocity (v=\Delta x/\Delta t).

### 4) Operator current (Q_K)

Define an arbitrary cyclic order of offsets (purely representational, but symmetry-consistent) and count net rotations in token transfers. Then test:

* null: drift ≈ 0
* driven: drift ≠ 0
* deadline regimes: drift improves bounded revisit times

You can TUR-test (Q_K) too, just like your clock counter.

---

## Failure modes (also fun)

Clockwork fabric is not guaranteed. Here are fun “non-clock” regimes:

* **Frozen wiring:** K gets pinned; you have a static circuit (still useful, not a clock).
* **Turbulent wiring:** K thrashes; no coherent phase.
* **Glassy wiring:** K slowly ages; looks like memory but not clock.
* **Patchy oscillators:** local oscillations exist but don’t synchronize; lots of microclocks with no fabric.

All of these are informative because they’re different “materials” of operator space.

---

## The most “out of the box” possibility

A really wild (but plausible) emergent behavior is that the fabric becomes a **mechanochemical wave**:

* K-wave sweeps → causes S updates to align → changes mismatch → pulls K-wave forward.
* That is a self-propagating cycle: *the wave carries the conditions for its own motion*.

If you see that, you’ve basically got a minimal “metabolic clock”: the coupling wave is doing work to maintain the structures that allow it to continue doing work.

Not life, not cognition—just **a self-maintaining spatiotemporal engine**.

---

## What would convince me we truly got “clockwork fabric”

These three together:

1. A **stable wave/spiral** in (d(q,t)) or (H_K(q,t)).
2. A nonzero **operator current drift** ( \langle Q_K\rangle/t\neq 0 ) in driven regimes.
3. A deadline/noise experiment where:

   * coherent wave regimes meet deadlines reliably,
   * incoherent/random regimes miss them,
   * and the gap correlates with EP bucket spent on operator motion.

That would be the “clockwork fabric” story in one shot.

---

If you want, I can sketch (in repo-agent-prompt form) a minimal set of diagnostics + one “clockwork fabric” experiment that’s likely to produce a scanline/spiral (without injecting directionality) and that reuses your existing deadline/noise machinery.
</file>

<file path="docs/materializations/coarse_graining_and_attention.md">
In your operator‑lifted world, **coarse‑graining and attention are literally the same mathematical object**, just in different “phases” of the coupling tokens.

Once you have a local simplex of weights

[
k(q,r)=K(q,r)/B_K,\qquad \sum_r k(q,r)=1
]

and you define the upper layer’s “view” as a weighted sum

[
\mathrm{pred}(q)=\sum_{r\in R} k(q,r),\mathrm{lower}(q+r),
]

you’ve already built the core of **soft attention** (a weighted mixture over a local neighborhood).
And if you apply that operator again and again across layers, you’ve built **coarse‑graining** (progressive smoothing / pooling / abstraction).

So yes: these two are deeply related — almost *identical* — in this design.

---

## 1) Coarse‑graining vs attention is a spectrum controlled by kernel entropy

A single diagnostic basically tells you where you are on that spectrum:

### Kernel entropy

[
H_K(q)=-\sum_{r} k(q,r)\log k(q,r)
]

* **High (H_K)**: weights spread out → the operator is a *local average*
  → **coarse‑graining** (low‑pass filter, smoothing, denoising, “macro view”).

* **Low (H_K)**: weights concentrate on 1–2 offsets → the operator is almost a *copy/shift*
  → **attention / routing** (“I mostly listen to that one neighbor over there”).

You can also think in terms of an “attention vector”
[
d(q)=\sum_r k(q,r),r
]

* (|d(q)|\approx 0): diffuse pooling (coarse grain)
* (|d(q)|) large: focused routing (attention)

So: **coarse‑graining = high‑entropy attention**,
**attention = low‑entropy coarse‑graining.**

That’s why they feel related.

---

## 2) “Attention without semantics” emerges naturally here

In ML, attention usually depends on *content* via dot products and learned keys/queries. You don’t want that — it’s too semantic.

In your system, content‑dependence can still emerge *without* engineered feature extraction because the only “sensor” you need already exists:

* **mismatch / instability / failure to predict** is content‑dependent,
* and K updates can be biased by “moves that reduce mismatch.”

So you get an “attention mechanism” that says:

> “Allocate coupling weight to whatever neighbor offsets reduce my local mismatch.”

That’s not “this is an edge” or “this is an object.”
It’s “this coupling reduces the local thermodynamic tension.”

If you run with **noise + deadlines**, you’ll tend to see a very natural pattern:

* In stable regions: K becomes diffuse (coarse grain; robust average).
* Near damage / high noise: K becomes sparse and directional (attention/routing to healthier context or to high-value repair channels).

This is basically an emergent **foveation**:

* *periphery* = coarse‑grained, cheap, stable
* *fovea* = attentive, expensive, targeted

…and no one told it what to look at.

---

## 3) Why budgets make “attention + coarse‑graining” unavoidable

If you didn’t have scarcity, K could “listen to everything.” Then the model would drown in combinatorics and you’d get trivial saturation.

Your **token budget** (B_K) is what forces the system to choose *how* to represent the world:

* **Diffuse pooling** spreads the budget into “robust averages.”
* **Sparse routing** concentrates the budget into “high-resolution channels.”

So with scarcity, the system is always solving (implicitly) a rate/distortion‑like dilemma:

* “How do I minimize mismatch (distortion) with a limited coupling capacity (rate)?”

You never add that as an objective — it falls out of the combination of:

* bounded carriers,
* mismatch entering ΔE and/or drive‑only work,
* and reversible exchange moves.

This is where coarse‑graining and attention become *the same phenomenon*:
they are two different ways to spend a fixed budget.

---

## 4) How “hierarchy” can emerge: attention at lower layers, coarse‑grain at higher

If you stack layers, something very plausible happens:

* Lower layers stay high-frequency and noisy (they’re close to the “micro world”).
* Higher layers become more stable because the coupling operator tends to suppress noise.

But the *interesting* regime is when the system doesn’t choose one globally; it chooses a **division of labor** across depth:

* **Layer 1:** attention-heavy (sparse K) → routes, preserves detail, repairs locally
* **Layer 2:** coarse-heavy (diffuse K) → integrates, smooths, becomes slow variable
* **Layer 3+:** ultra-coarse (very diffuse) → stable macro “concept map”

This is how you get something that looks like “abstraction” without defining abstractions:

* abstraction = whatever survives repeated coarse‑graining while still being predictive and maintainable.

---

## 5) How a “language” could begin to appear

Here’s the key: in your system, **tokenization can be literal**.

* K is already a *token distribution*.
* If you also allow a small number of **discrete operator modes** (P4) — or if the token exchange dynamics naturally clusters — you can get a small set of recurring operator shapes.

### Vocabulary = recurring operator motifs

Think of each cell’s kernel (K(q,\cdot)) as a point in a discrete simplex. Over time you might see it concentrate around a few motifs:

* “identity” (mostly (r=0))
* “shift-left” (mostly (r=(-1,0)))
* “shift-right”
* “up”
* “down”
* “blur” (spread over neighbors)
* “mix-two” (two offsets)

Those motifs are your **proto-words**: they’re discrete, reusable, and have stable effects.

### Semantics = causal effect, not human meaning

A motif has “meaning” only if it *does something predictable*:

* selecting motif A causes information to flow right,
* selecting motif B causes smoothing,
* selecting motif C causes repair pressure to concentrate.

You can measure semantics without interpretation by checking:

* does knowing motif type improve prediction of next state change?
* does motif type carry directed information about future repair success / deadline misses?

That’s semantics as **causal efficacy**.

### Syntax/grammar = stable compositions under P3 and P6

Now bring in your primitives:

* **P3 (protocol)** makes order matter: apply operator A then B ≠ B then A
  → you can get pumped cycles of motifs (“instruction sequences”).

* **P6 (drive)** makes sequences directional and reliable: you get currents through “motif space.”

A “grammar” is then just:

* which motif transitions are likely,
* which cycles repeat,
* which sequences stabilize the code under noise.

No one needs to label them “nouns” or “verbs.”
Grammar is a **transition graph over motifs**.

---

## 6) Where P2 fits: tokenization pressure and semantic sparsity

If I interpret “P2 relevance” in your ratchet framework as the economy/weakness axis (scarcity, saturation, bounded carriers), then P2 is the *reason a vocabulary forms at all*.

Without P2-like scarcity, K could remain a continuously varying blur of weights — no discrete motifs, no reusable “words.”

With P2 enforced as **bounded integer budgets + exchange**:

* the simplex is discrete → natural quantization
* scarcity encourages **sparse allocations** (low entropy kernels) in some regions
* weakness prevents infinite coupling strength → forces *selective* attention, not universal

So P2 is the “pressure toward tokenization” and “pressure toward small alphabets.”

If you ever see a small set of K motifs recur across space and time, that’s P2 at work.

---

## 7) The fun unification: coarse‑graining builds the “concept space,” attention selects within it

Put the pieces together and you get a very plausible loop:

1. Coarse‑graining builds **stable slow variables** at higher layers
   (a latent “concept space” in the boring sense: robust summaries).

2. Attention (sparse K) selects which micro sources feed those summaries
   (a routing policy, but content-driven via mismatch, not semantics).

3. Token budgets and saturation discretize the routing patterns into **motifs**
   (proto-vocabulary).

4. P3/P6 turn motif usage into **sequences and cycles**
   (proto-syntax), paid for by EP.

So “language” here is not words about cats. It’s:

* a discrete operator vocabulary,
* composed into sequences that control information flow and repair,
* in a way that is dissipation-priced and deadline-sensitive.

That’s a genuinely physical notion of “meaning”: “this token makes the system survive deadlines.”

---

## 8) What I’d expect you to actually *see* first

If you turn on operator‑lifted coupling and run in noisy/deadline regimes:

* **Patchy K entropy map**:

  * big areas of high entropy (pooling/coarse grain)
  * thin channels of low entropy (attention routes / repair highways)

* **Motif clustering**:

  * K distributions cluster into a few shapes (especially if (B_K) is smallish)
  * those shapes propagate like “words” along channels

* **Context-sensitive switching**:

  * when a region becomes noisy, local kernels become sparse and directional
  * when region is quiet, kernels diffuse again

This is adaptive homeostasis + attention + abstraction all at once.

---

## 9) Minimal diagnostics that would let you claim these phenomena without semantics

If you want to make the “out-of-the-box” story testable (without adding a detector that drives dynamics), add only **read-only measurements**:

1. **Kernel entropy distribution** (H_K(q)) per layer interface
   → maps coarse vs attention.

2. **Motif clustering**

   * collect K vectors from many cells/times
   * cluster them (k-means on normalized k, or simple nearest-prototype)
   * report:

     * number of clusters with >X% mass
     * transition matrix between clusters

3. **Predictive semantics**

   * does cluster ID predict future mismatch reduction / deadline misses?
   * conditional mutual information style:
     (I(\text{motif}*t ; \text{repair success}*{t+\tau}\mid \text{local state}_t))

4. **EP per “symbol”**

   * EP bucket spent on K moves and on repair moves
   * EP per motif transition or per motif cycle

If you can show:

* small motif vocabulary,
* nontrivial motif transition structure (syntax),
* and predictive causal efficacy (semantics),
  you’ve basically got “language-like” behavior in the only honest sense available here.

---

If you want to push this hard, the most “language-friendly” regime is usually:

* **small (B_K)** (forces quantization/motifs),
* **moderate noise + deadlines** (forces attention to matter),
* **P6 on** (makes sequences directional and reliable),
* **P3 on** (makes composition order matter → syntax pressure),
* and enough layers (≥2) so coarse‑graining can build stable “concept variables” above the attentive layer.
</file>

<file path="docs/materializations/levin.md">
Michael Levin would expect evidence of:

Pattern memory / setpoints in a morphospace
A low‑dimensional description of the system’s pattern (a “shape” descriptor), which is stable and can be returned to after perturbations. Not self‑healing as a narrative, but setpoint restoration as a measurable dynamical property.

Competency curves
Not a single run/preset, but a success probability vs perturbation amplitude vs deadline picture: what “space of perturbations” the system can handle. That’s the most “Levin” thing you can measure without adding goals.

Top‑down causation as predictive/counterfactual leverage
The lifted operator substrate (K) should have measurable predictive influence on future macrostates beyond what the current macrostate predicts. In other words: K is not just correlated; it adds control information.

Costs
All of the above should be related to dissipation (your epExact and bucket splits), because your whole thesis is ratchet‑priced clocks/codes/constraints.
</file>

<file path="docs/materializations/open_endedness.md">
In your operator‑lifted world, **coarse‑graining and attention are literally the same mathematical object**, just in different “phases” of the coupling tokens.

Once you have a local simplex of weights

[
k(q,r)=K(q,r)/B_K,\qquad \sum_r k(q,r)=1
]

and you define the upper layer’s “view” as a weighted sum

[
\mathrm{pred}(q)=\sum_{r\in R} k(q,r),\mathrm{lower}(q+r),
]

you’ve already built the core of **soft attention** (a weighted mixture over a local neighborhood).
And if you apply that operator again and again across layers, you’ve built **coarse‑graining** (progressive smoothing / pooling / abstraction).

So yes: these two are deeply related — almost *identical* — in this design.

---

## 1) Coarse‑graining vs attention is a spectrum controlled by kernel entropy

A single diagnostic basically tells you where you are on that spectrum:

### Kernel entropy

[
H_K(q)=-\sum_{r} k(q,r)\log k(q,r)
]

* **High (H_K)**: weights spread out → the operator is a *local average*
  → **coarse‑graining** (low‑pass filter, smoothing, denoising, “macro view”).

* **Low (H_K)**: weights concentrate on 1–2 offsets → the operator is almost a *copy/shift*
  → **attention / routing** (“I mostly listen to that one neighbor over there”).

You can also think in terms of an “attention vector”
[
d(q)=\sum_r k(q,r),r
]

* (|d(q)|\approx 0): diffuse pooling (coarse grain)
* (|d(q)|) large: focused routing (attention)

So: **coarse‑graining = high‑entropy attention**,
**attention = low‑entropy coarse‑graining.**

That’s why they feel related.

---

## 2) “Attention without semantics” emerges naturally here

In ML, attention usually depends on *content* via dot products and learned keys/queries. You don’t want that — it’s too semantic.

In your system, content‑dependence can still emerge *without* engineered feature extraction because the only “sensor” you need already exists:

* **mismatch / instability / failure to predict** is content‑dependent,
* and K updates can be biased by “moves that reduce mismatch.”

So you get an “attention mechanism” that says:

> “Allocate coupling weight to whatever neighbor offsets reduce my local mismatch.”

That’s not “this is an edge” or “this is an object.”
It’s “this coupling reduces the local thermodynamic tension.”

If you run with **noise + deadlines**, you’ll tend to see a very natural pattern:

* In stable regions: K becomes diffuse (coarse grain; robust average).
* Near damage / high noise: K becomes sparse and directional (attention/routing to healthier context or to high-value repair channels).

This is basically an emergent **foveation**:

* *periphery* = coarse‑grained, cheap, stable
* *fovea* = attentive, expensive, targeted

…and no one told it what to look at.

---

## 3) Why budgets make “attention + coarse‑graining” unavoidable

If you didn’t have scarcity, K could “listen to everything.” Then the model would drown in combinatorics and you’d get trivial saturation.

Your **token budget** (B_K) is what forces the system to choose *how* to represent the world:

* **Diffuse pooling** spreads the budget into “robust averages.”
* **Sparse routing** concentrates the budget into “high-resolution channels.”

So with scarcity, the system is always solving (implicitly) a rate/distortion‑like dilemma:

* “How do I minimize mismatch (distortion) with a limited coupling capacity (rate)?”

You never add that as an objective — it falls out of the combination of:

* bounded carriers,
* mismatch entering ΔE and/or drive‑only work,
* and reversible exchange moves.

This is where coarse‑graining and attention become *the same phenomenon*:
they are two different ways to spend a fixed budget.

---

## 4) How “hierarchy” can emerge: attention at lower layers, coarse‑grain at higher

If you stack layers, something very plausible happens:

* Lower layers stay high-frequency and noisy (they’re close to the “micro world”).
* Higher layers become more stable because the coupling operator tends to suppress noise.

But the *interesting* regime is when the system doesn’t choose one globally; it chooses a **division of labor** across depth:

* **Layer 1:** attention-heavy (sparse K) → routes, preserves detail, repairs locally
* **Layer 2:** coarse-heavy (diffuse K) → integrates, smooths, becomes slow variable
* **Layer 3+:** ultra-coarse (very diffuse) → stable macro “concept map”

This is how you get something that looks like “abstraction” without defining abstractions:

* abstraction = whatever survives repeated coarse‑graining while still being predictive and maintainable.

---

## 5) How a “language” could begin to appear

Here’s the key: in your system, **tokenization can be literal**.

* K is already a *token distribution*.
* If you also allow a small number of **discrete operator modes** (P4) — or if the token exchange dynamics naturally clusters — you can get a small set of recurring operator shapes.

### Vocabulary = recurring operator motifs

Think of each cell’s kernel (K(q,\cdot)) as a point in a discrete simplex. Over time you might see it concentrate around a few motifs:

* “identity” (mostly (r=0))
* “shift-left” (mostly (r=(-1,0)))
* “shift-right”
* “up”
* “down”
* “blur” (spread over neighbors)
* “mix-two” (two offsets)

Those motifs are your **proto-words**: they’re discrete, reusable, and have stable effects.

### Semantics = causal effect, not human meaning

A motif has “meaning” only if it *does something predictable*:

* selecting motif A causes information to flow right,
* selecting motif B causes smoothing,
* selecting motif C causes repair pressure to concentrate.

You can measure semantics without interpretation by checking:

* does knowing motif type improve prediction of next state change?
* does motif type carry directed information about future repair success / deadline misses?

That’s semantics as **causal efficacy**.

### Syntax/grammar = stable compositions under P3 and P6

Now bring in your primitives:

* **P3 (protocol)** makes order matter: apply operator A then B ≠ B then A
  → you can get pumped cycles of motifs (“instruction sequences”).

* **P6 (drive)** makes sequences directional and reliable: you get currents through “motif space.”

A “grammar” is then just:

* which motif transitions are likely,
* which cycles repeat,
* which sequences stabilize the code under noise.

No one needs to label them “nouns” or “verbs.”
Grammar is a **transition graph over motifs**.

---

## 6) Where P2 fits: tokenization pressure and semantic sparsity

If I interpret “P2 relevance” in your ratchet framework as the economy/weakness axis (scarcity, saturation, bounded carriers), then P2 is the *reason a vocabulary forms at all*.

Without P2-like scarcity, K could remain a continuously varying blur of weights — no discrete motifs, no reusable “words.”

With P2 enforced as **bounded integer budgets + exchange**:

* the simplex is discrete → natural quantization
* scarcity encourages **sparse allocations** (low entropy kernels) in some regions
* weakness prevents infinite coupling strength → forces *selective* attention, not universal

So P2 is the “pressure toward tokenization” and “pressure toward small alphabets.”

If you ever see a small set of K motifs recur across space and time, that’s P2 at work.

---

## 7) The fun unification: coarse‑graining builds the “concept space,” attention selects within it

Put the pieces together and you get a very plausible loop:

1. Coarse‑graining builds **stable slow variables** at higher layers
   (a latent “concept space” in the boring sense: robust summaries).

2. Attention (sparse K) selects which micro sources feed those summaries
   (a routing policy, but content-driven via mismatch, not semantics).

3. Token budgets and saturation discretize the routing patterns into **motifs**
   (proto-vocabulary).

4. P3/P6 turn motif usage into **sequences and cycles**
   (proto-syntax), paid for by EP.

So “language” here is not words about cats. It’s:

* a discrete operator vocabulary,
* composed into sequences that control information flow and repair,
* in a way that is dissipation-priced and deadline-sensitive.

That’s a genuinely physical notion of “meaning”: “this token makes the system survive deadlines.”

---

## 8) What I’d expect you to actually *see* first

If you turn on operator‑lifted coupling and run in noisy/deadline regimes:

* **Patchy K entropy map**:

  * big areas of high entropy (pooling/coarse grain)
  * thin channels of low entropy (attention routes / repair highways)

* **Motif clustering**:

  * K distributions cluster into a few shapes (especially if (B_K) is smallish)
  * those shapes propagate like “words” along channels

* **Context-sensitive switching**:

  * when a region becomes noisy, local kernels become sparse and directional
  * when region is quiet, kernels diffuse again

This is adaptive homeostasis + attention + abstraction all at once.

---

## 9) Minimal diagnostics that would let you claim these phenomena without semantics

If you want to make the “out-of-the-box” story testable (without adding a detector that drives dynamics), add only **read-only measurements**:

1. **Kernel entropy distribution** (H_K(q)) per layer interface
   → maps coarse vs attention.

2. **Motif clustering**

   * collect K vectors from many cells/times
   * cluster them (k-means on normalized k, or simple nearest-prototype)
   * report:

     * number of clusters with >X% mass
     * transition matrix between clusters

3. **Predictive semantics**

   * does cluster ID predict future mismatch reduction / deadline misses?
   * conditional mutual information style:
     (I(\text{motif}*t ; \text{repair success}*{t+\tau}\mid \text{local state}_t))

4. **EP per “symbol”**

   * EP bucket spent on K moves and on repair moves
   * EP per motif transition or per motif cycle

If you can show:

* small motif vocabulary,
* nontrivial motif transition structure (syntax),
* and predictive causal efficacy (semantics),
  you’ve basically got “language-like” behavior in the only honest sense available here.

---

If you want to push this hard, the most “language-friendly” regime is usually:

* **small (B_K)** (forces quantization/motifs),
* **moderate noise + deadlines** (forces attention to matter),
* **P6 on** (makes sequences directional and reliable),
* **P3 on** (makes composition order matter → syntax pressure),
* and enough layers (≥2) so coarse‑graining can build stable “concept variables” above the attentive layer.
</file>

<file path="docs/00_glossary.md">
# Glossary

This glossary standardizes terms used across the docs.

## Ratchet (general)

A **ratchet** is a mechanism that can produce **persistent directionality** in some coarse-grained quantity (a “pattern”) in a way that is:
- **self-controlled** (bounded; no runaway),
- **self-reinforcing** (feedback increases the likelihood/size of future steps),
- and often **one-way** on relevant timescales (reversals are forbidden or exponentially rare).

In A‑Life-clean setups, “one-way” is typically **metastability** (rare reversals), not hard-coded monotonicity.

## Pattern

A **pattern** is a coarse-grained observable \(P(Z)\) (cluster structure, graph backbone, safe-set region, integer index, etc.) derived from the full state \(Z\).

We distinguish:
- **Strong lock-in:** \(P\) is provably monotone (rare in A‑Life-clean models unless explicitly irreversible).
- **Metastable lock-in:** \(P\) persists for a long time; reversals are exponentially suppressed.

## Fast–slow split

We model the world state as:
\[
Z_t = (X_t, W_t).
\]
- \(X_t\) = fast substrate (particle positions, headings, etc.).
- \(W_t\) = slow variables that store structure/constraints/resources (bonds, budgets, counters, fields).

The primitives typically act by enabling transitions on \(W\) (or altering the operator acting on \(X\)).

## Detailed balance (reversibility)

A Markov process on \(Z\) is **reversible** w.r.t. a stationary distribution \(\pi(Z)\) if:
\[
\pi(z)\,P(z\to z') = \pi(z')\,P(z'\to z)\quad\text{for all }z,z'.
\]
Equivalent condition (Kolmogorov cycle criterion):
- the **cycle affinity** is zero for every directed cycle.

Reversibility implies **no stationary probability currents** (no intrinsic arrow-of-time).

## Cycle affinity

For edge transitions with probabilities/rates \(P(z\to z')\), define edge affinity:
\[
a(z\to z') := \log\frac{P(z\to z')}{P(z'\to z)}.
\]
For a directed cycle \(\gamma\),
\[
\mathcal A(\gamma) := \sum_{e\in\gamma} a(e).
\]
If any \(\mathcal A(\gamma)\neq 0\), the process is nonreversible (directional flux is possible).

## Primitive (P₁–P₆)

A **primitive** is a toggleable *mechanistic* operation (not a global objective) that adds a family of transitions or operator modifications to the substrate.

The six primitives used in the playground are:

- **P₁ Operator-write:** writable couplings/weights that alter effective dynamics.
- **P₂ Feasible-set-write:** writable constraints/capacities (local apparatus; budgets).
- **P₃ Protocol-cycle:** cyclic order-of-operations / time-dependent driving.
- **P₄ Topological/quantized:** integer “click” variables / discrete transitions.
- **P₅ Closure / viability field:** build/erode a protective field; safe sets as observables.
- **P₆ Resource transduction:** non-equilibrium coupling to a resource/chemical potential.

## Null regime (no baked-in directionality)

In the playground, the **null regime** means:
- P₃ = OFF and P₆ = OFF,
- other primitives may be ON,
- the resulting dynamics must satisfy detailed balance with respect to an explicit stationary measure.

This ensures no “bias to evolve” is accidentally hard-coded.

---
</file>

<file path="docs/01_ratchet_theory_overview.md">
# Ratchet Theory Overview

This document provides the minimal global context used by the Ratchet Playground.

The theory developed here treats “complex organization” as arising from **ratchet-like mechanisms**:
- not from explicit design,
- not necessarily from replication at the base layer,
- and not from globally optimizing objectives.

Instead, organization can arise when a substrate supports:
1) **loops that break reversibility** (non-equilibrium driving), and  
2) **state variables that can store/rectify effects of those loops** (memory / constraints / discrete indices / fields),  
3) **brakes** (finite capacities, convex costs, decay), ensuring boundedness.

## 1. Ratchet as “directional organization”

A ratchet is not merely an equilibrium configuration. “Magnets align” is equilibrium: it does not (by itself) create an arrow-of-time.

A ratchet is about **persistent directional change** in some coarse feature \(P(Z)\) such that:
- there exist net currents / affinities (nonzero cycle affinity), and/or
- the system crosses discrete “clicks” that become increasingly hard to undo,
- while remaining bounded (no runaway).

In A‑Life-clean settings, “one-way” is not enforced; it is detected as:
- **metastability**: reversal times become long relative to the demo timescale.

## 2. Two layers of description

We distinguish:

### 2.1 Mechanistic primitives (P₁–P₆)
Primitives are **toggleable mechanisms** that add local transitions or operator modifications.

They do not prescribe what should happen, but they can create the conditions for organization to emerge.

### 2.2 Ratchet motifs / styles (patterns of interaction)
A “ratchet motif” is a recurring structural pattern of how primitives combine. Examples:

- **Constraint×Constraint motifs**: a capability is bounded by a *local sensitivity cap* and a *global budget cap* (Weakness × Economy is canonical).
- **Protocol-cycle motifs**: noncommuting sequences of reversible updates yield geometric currents (P₃).
- **Bias+barrier motifs**: discrete “teeth” (P₄) plus non-equilibrium bias produce stepwise drift.

The playground focuses on primitives, but the motif view is useful for interpreting results.

## 3. What counts as “force beyond selection” in this theory?

We avoid introducing an external objective (“fitness”) as the driver.

Instead, the appropriate notion of “force” is **time-reversal asymmetry** measurable by:
- **cycle affinities**,
- **entropy production proxies**,
- **net fluxes** in coarse state variables.

In particular:

- In the **null regime** (P₃=OFF, P₆=OFF), the system must be reversible.  
  Any “growth” is just equilibrium relaxation/fluctuation.

- When P₃ and/or P₆ are enabled, reversibility may break.
  Directional fluxes may appear, and **if** the other primitives provide storing/rectifying degrees of freedom, those fluxes may appear as persistent patterns.

## 4. Why replication/heredity are not assumed at the base layer

Classical A‑Life often starts with replication and heredity as the foundation of open-ended evolution.

Ratchet theory allows a different perspective:

- **Base-layer organization** can arise from non-equilibrium + rectification + memory without replication.
- Replication can then emerge later as a higher-level pattern built on top of already-existing substrates, constraints, and energy flows.

Therefore, the Ratchet Playground deliberately avoids hard-wiring replication as a prerequisite.

## 5. Minimal experimental goal for the playground

Given:
- a neutral substrate (particles + noise + repulsion),
- the six primitives as toggleable mechanisms,
- and diagnostics that detect irreversibility and structure,

the goal is:

> Let users explore which combinations of primitives can produce measurable irreversibility and persistent, structured patterns — without any added global objective or directional “helper” rule.

---
</file>

<file path="docs/02_primitives_P1_P6.md">
# The Six Primitives P₁–P₆

This document defines the six primitives used in the Ratchet Playground in a way that supports:
- rigorous null-regime reversibility,
- toggleable mechanism design (no global objectives),
- and later cycle-affinity analysis.

We use a fast–slow state:
\[
Z_t=(X_t,W_t),
\]
where \(X_t\) is the fast substrate and \(W_t\) contains slow variables (memory / constraints / fields).

## P₁ — Operator-write

**Intuition:** The effective “operator” governing interactions can itself be modified locally. In practice this means writable couplings/weights that alter the dynamics.

**Canonical state variable:** bond/coupling levels \(w_{ij}\).

- \(w_{ij}\) strengthens or weakens the influence between \(i\) and \(j\).
- In null regime, \(w_{ij}\) must be reversible (up and down transitions both exist).

**What P₁ can enable:**
- persistent interaction backbones (graphs),
- changes in mixing/synchronization properties,
- structural memory in the operator itself.

**What P₁ cannot do alone:**
- create a thermodynamic arrow-of-time (in null regime it must remain reversible).

## P₂ — Feasible-set-write

**Intuition:** The system can locally modify what it is capable of doing (constraints/capacities), again without a global schedule.

**Canonical state variable:** apparatus/capacity level \(a_i\).

- \(a_i\) controls local capabilities (e.g., max signal amplitude, max bond maintenance, max update frequency).
- In null regime, \(a_i\) is reversible.

**What P₂ can enable:**
- adaptive local constraint landscapes,
- endogenous changes in “expressivity capacity” if a drive exists,
- Constraint×Constraint motifs (e.g., Weakness × Economy) at higher modeling levels.

**What P₂ cannot do alone:**
- guarantee monotone growth of capacity without a drive (P₃/P₆) that breaks reversibility.

## P₃ — Protocol-cycle

**Intuition:** The order of operations matters. A cyclic protocol (A→B→C→…) can generate geometric currents even when each substep is individually reversible.

**Canonical representation:** a phase \(\phi(t)\) that modulates which kernel is applied or modifies proposals.

**Key property:**
- P₃ does not store memory by itself; it is a *driver* (time dependence).
- P₃ can produce an arrow-of-time via noncommutativity:
  \[
  K_{\rm cyc}=K_C K_B K_A \quad\text{nonreversible if }[K_A,K_B]\ne 0.
  \]

**What P₃ can enable:**
- pumped currents,
- hysteresis loops in observable planes,
- directional fluxes that other primitives may rectify into patterns.

## P₄ — Topological/quantized

**Intuition:** Discrete “click” variables (integer indices) support stepwise transitions and robust coarse states.

**Canonical state variable:** integer counter \(n\in\mathbb Z\) (bounded in practice).

- In null regime, \(n\) performs reversible \(\pm 1\) jumps with no drift.
- Under drive, \(n\) can drift; barriers can make reversal times long (metastable lock-in).

**What P₄ can enable:**
- discrete event detection (“clicks”),
- robustness against small perturbations (coarse state labels),
- barrier-controlled metastability.

## P₅ — Closure / viability-field

**Intuition:** The environment can accumulate protective structure (or constraints) that define “safe” regions — but without planner-like monotone closure.

**Canonical state variable:** a protection field \(S_q\) on a grid.

- \(S_q\) can increase or decrease (build and erode).
- The “safe set” is an observable:
  \[
  K_t := \{q: S_q(t)\ge\tau_S\},
  \]
  which can grow or shrink.

**What P₅ can enable:**
- emergent protected regions,
- coarse “habitat” structures,
- metastable safe domains when maintenance beats decay.

## P₆ — Resource transduction

**Intuition:** The system couples to a resource/chemical potential field. This is a primitive way to break detailed balance without prescribing what structure is “good”.

**Canonical ingredient:** a field \(\mu(x)\) and/or a stored resource variable \(r_i\).

Two minimal implementations:
1) **Affinity-only P₆:** P₆ supplies a work term \(W(z\to z')\) in acceptance ratios for memory updates.
2) **Explicit resource variable:** \(r_i\) evolves via local harvest/leak and couples to other updates.

**What P₆ can enable:**
- non-equilibrium flux,
- sustained maintenance of nontrivial memory patterns,
- coupling of contexts (high vs low \(\mu\)) to directional loops (cycle affinities).

---

## How the primitives split in the null regime

In the Ratchet Playground design:

- P₁, P₂, P₄, P₅ are **reversible channels** in the null regime (detailed balance is enforced).
- P₃ and P₆ are the **only** primitives allowed to break reversibility:
  - P₃ via noncommuting protocol kernels (time dependence),
  - P₆ via nonconservative affinities / work terms.

This ensures “no bias to evolve” unless the user explicitly enables a primitive that can generate an arrow-of-time.

---
</file>

<file path="docs/03_atoms_MFQX.md">
# Optional “Atoms” Vocabulary (M, F, Q, X)

This note captures a compact “atom” vocabulary used in earlier theory building. It is optional for the playground implementation but useful as a conceptual compression.

## Atom M — Monotone memory

A mechanism that stores past events so that reversing them becomes hard.

In strict form: a state variable \(m_t\) with \(m_{t+1}\ge m_t\).  
In A‑Life-clean form: metastable memory where reverse transitions exist but are exponentially suppressed.

Related primitives:
- P₁ (operator memory), P₄ (click indices), P₅ (field persistence), sometimes P₂ (institutionalized constraints).

## Atom F — Flow geometry / noncommutativity

Order-of-operations effects where
\[
K_B K_A \ne K_A K_B.
\]
Cyclic protocols (P₃) generate currents because the loop in “control space” has a geometric phase.

Related primitives:
- P₃ most directly.

## Atom Q — Quantization / discrete teeth

Discrete states (integers, topological indices) that support “clicks” rather than continuous drift.

Related primitives:
- P₄.

## Atom X — Resource inequality / nonequilibrium affinity

A nonconservative drive that breaks detailed balance, often expressible as a chemical potential difference or “work” term.

Related primitives:
- P₆ (and P₃ if implemented as externally driven protocol).

---

## Mapping atoms → primitives (informal)

- P₁: M (operator stored), sometimes X (if maintenance requires drive)
- P₂: M (constraint institutionalization), X (if capacity needs energy), often coupled to Q (thresholding)
- P₃: F
- P₄: Q (+ M when barriers grow)
- P₅: M (field persistence) + X (maintenance vs decay)
- P₆: X (+ M if banking/accumulation becomes metastable)

---
</file>

<file path="docs/04_weakness_x_economy.md">
# Weakness × Economy (Canonical Constraint×Constraint Scaffold)

This note is a self-contained mathematical scaffold for **Weakness × Economy** as a mechanism that produces *safe expressivity* in evolving codes.

It is included here because it provides:
- a clean example of “two constraints + an ambient drive,”
- a tractable inequality \(I^\star \lesssim (B/c_{\min})\,\varepsilon\),
- and a clear separation between **stabilization** at fixed budgets vs **ratcheting capacity** when budgets themselves evolve.

Although the Ratchet Playground does **not** implement global objectives, this scaffold is useful for:
- interpreting P₂-type “feasible-set write” phenomena,
- designing optional “communication toy worlds” inside the app,
- and connecting ratchet theory to established information-theoretic literature.

---

## 1) Setup

- Meanings \(M\in\mathcal M\), with prior \(P(m)\).
- Signals/forms \(S\in\mathcal S\) (or sequences \(S_{1:L}\)).
- Encoder \(\phi(s\mid m)\).
- Decoder \(\psi(m\mid s)\).
- Bayes-optimal decoder:
  \[
  \psi^\star(m\mid s) \propto P(m)\phi(s\mid m).
  \]

### Expressivity / accuracy
Use mutual information:
\[
I(M;S)=\mathbb{E}\Big[\log\frac{P(M\mid S)}{P(M)}\Big].
\]

### Economy (global budget)
A convex cost \(c(s)\ge 0\). Define:
\[
\mathrm{Cost}(\phi)=\mathbb{E}[c(S)].
\]
For sequences, total cost \(C=\sum_{t=1}^L c(S_t)\) and a budget constraint:
\[
\mathbb E[C]\le B.
\]
Assume \(c(s)\ge c_{\min}>0\) for non-null symbols, so the budget bounds expected length:
\[
\mathbb E[L]\le B/c_{\min}.
\]

### Weakness (local anti-leverage)
Define the maximum posterior “yank” from a single form:
\[
\mathrm{Str}(\phi):=\max_{s\in\mathcal S} D_{\mathrm{KL}}\!\big(P(\cdot\mid s)\,\|\,P(\cdot)\big).
\]
Low \(\mathrm{Str}\) means no single token can dominate interpretation.

---

## 2) Free-energy objective (fixed epoch)

A common analysis objective is:
\[
\mathcal F(\phi)= I(M;S) - \lambda\,\mathrm{Cost}(\phi) - \mu\,\mathrm{Str}(\phi),
\qquad \lambda,\mu>0.
\tag{WE-F}
\]

This is a *within-epoch* optimization principle. In many learning dynamics (mirror descent, replicator-like flows), \(\mathcal F\) can act like a Lyapunov function under suitable regularity.

**Important distinction:**
- At fixed \((\lambda,\mu)\) (or fixed budgets), the system can converge to a stable code \(\phi^\star\).  
- That is **stabilization**, not necessarily a “ratchet” in the sense of ever-increasing expressivity.

---

## 3) A key inequality: expressivity is bounded by product of budgets

To make “weakness” local in time, consider sequential signaling \(S_{1:L}\) with per-step weakness cap:
\[
\mathrm{Str}_t := \sup_{s^t} D_{\mathrm{KL}}\!\big(P(M\mid s^t)\,\|\,P(M\mid s^{t-1})\big)\le \varepsilon.
\tag{WE-local}
\]

Then:
\[
I(M;S_{1:L})
=\sum_{t=1}^L I(M;S_t\mid S_{1:t-1})
\le \sum_{t=1}^L \mathrm{Str}_t
\le L\,\varepsilon.
\]
With \(L\le B/c_{\min}\), we get:
\[
\boxed{
I(M;S_{1:L}) \;\le\; \frac{B}{c_{\min}}\,\varepsilon.
}
\tag{WE-bound}
\]

This expresses a canonical “Constraint×Constraint” structure:
- local cap \(\varepsilon\) (weakness),
- global cap \(B\) (economy),
- expressivity bounded by their product.

---

## 4) No-hack lemma (bounded unilateral influence)

Let \(\epsilon:=\mathrm{Str}(\phi)=\max_s D_{\mathrm{KL}}(P(\cdot\mid s)\|P(\cdot))\). Then by Pinsker:
\[
\max_s \mathrm{TV}(P(\cdot\mid s),P(\cdot))
\le \sqrt{\tfrac12\,\epsilon}.
\tag{WE-Pinsker}
\]
So any single-symbol exploit has bounded effect.

This motivates the “anti-leverage” interpretation: weakness suppresses single-token reward hacking.

---

## 5) Existence and stability (fixed budgets)

Under mild compactness / finiteness assumptions (finite alphabets or compact policy classes; convex lower-semicontinuous cost; upper-semicontinuity of mutual information), maximizing (WE-F) admits at least one maximizer \(\phi^\star\).

With suitable learning dynamics on \(\phi\), \(\mathcal F\) can serve as a Lyapunov function:
\[
\dot{\mathcal F}(\phi_t)\ge 0,
\]
so the code improves until it reaches a stationary point.

**Interpretation:** within a fixed constraint regime, Weakness × Economy supports stable, bounded codes.

---

## 6) When does this become a *ratchet*?

A true capacity ratchet means a sequence of epochs \(k\) where the achievable frontier expands:
\[
I_{k+1}^\star \ge I_k^\star \quad \text{(one-way growth)},
\]
and ideally with increasing increments (self-reinforcement).

Define a “capacity budget”:
\[
p_k := \frac{B_k}{c_{\min}}\,\varepsilon_k.
\]
From (WE-bound), \(I_k^\star\lesssim p_k\).

### Key implication
If \(B_k,\varepsilon_k\) are fixed, \(p_k\) is fixed ⇒ \(I_k^\star\) is bounded ⇒ after convergence there is no further growth.

So for a ratchet in expressivity you need **budgets to evolve**:
\[
p_{k+1} > p_k.
\]

In higher-level models, such budget evolution can arise from:
- resource capture (P₆-type coupling),
- protocol changes (P₃),
- or endogenous constraint-writing dynamics (P₂).

In the Ratchet Playground base layer, budgets are not scheduled externally; they would have to emerge as persistent variables (P₂/P₆) under a drive.

---

## 7) Contrast as a corollary of weakness (informal)

Weakness penalizes high leverage. To maintain mutual information under a leverage cap, discrimination must be distributed across patterns, often implying a minimum separation among conditional signal distributions:
\[
\min_{m\neq m'} D_{\mathrm{JS}}\big(\phi(\cdot\mid m)\,\|\,\phi(\cdot\mid m')\big)
\gtrsim \alpha(\mu,\lambda,\text{noise})>0,
\]
in regimes where an optimum exists.

This “contrast floor” is a derived pressure, not a primitive.

---

## 8) Relation to primitives

Weakness × Economy is best viewed as a **P₂-level motif** (feasible-set / constraint structure) rather than a primitive itself:

- “Economy” corresponds to global budgets/costs.
- “Weakness” corresponds to local leverage caps or saturating influence.

In the A‑Life playground:
- we avoid implementing (WE-F) as a global objective;
- we can implement “economy” and “weakness” as **physics** (convex costs, saturating impact) or as **local constraints** stored in \(W\) (P₂).

---
</file>

<file path="docs/05_constraint_x_constraint_ratchets.md">
# Constraint×Constraint Ratchets (General Pattern)

A **Constraint×Constraint** motif is a structure where a capability is bounded by:
1) a **local sensitivity cap** (per action / per token / per step), and
2) a **global budget cap** (total cost / total steps / total exposure),

often implying a product-like bound:
\[
\text{capability} \;\lesssim\; (\text{global budget})\times(\text{local cap}).
\]

Weakness × Economy is the canonical example in communication systems.

## 1) Abstract template

Let a process reveal information or exert influence over \(L\) steps.

- Local cap:
  \[
  \text{per-step gain } g_t \le \varepsilon.
  \]
- Global budget:
  \[
  L \le B.
  \]
Then total gain is bounded:
\[
\sum_{t=1}^L g_t \le B\varepsilon.
\]

This holds for many notions of gain:
- information gain (KL),
- expected improvement,
- leverage or marginal risk contribution, etc.

## 2) Stabilization vs ratcheting

At fixed constraints, the motif gives **stabilization**:
- local exploits are capped,
- total activity is capped.

A true ratchet (one-way frontier growth) requires the constraints to change across epochs:
- \(B_k\) and/or \(\varepsilon_k\) must increase in a way that remains self-controlled.

In A‑Life-clean settings, such changes must emerge from primitive interactions (e.g., P₂/P₆) and not be scheduled externally.

## 3) Why this motif is common across disciplines (informal)

Many domains separate safety/robustness into:
- “no single action can do too much damage” (local cap),
- “total exposure is limited” (global cap).

Examples (conceptual parallels):
- privacy loss per query + total query budget,
- position-level risk caps + total leverage/capital constraint,
- Lipschitz/robustness bounds + model capacity constraints,
- per-interaction influence caps + total communication bandwidth,
- per-step actuation bounds + total energy budget.

The motif’s value is that it yields simple, checkable inequalities and makes “reward hacking” or “single-shot dominance” structurally difficult.

---
</file>

<file path="docs/06_alife_principles_and_null_regime.md">
# A‑Life Principles for the Ratchet Playground

This document formalizes the “no baked-in directionality” design constraint.

## 1) The core requirement

The sandbox must not contain hidden “helpers” that push the system toward complexity.

Therefore:

> **Null regime requirement:**  
> With **P₃=OFF** and **P₆=OFF**, the dynamics must be **reversible** (detailed balance) with respect to an explicit stationary measure \(\pi\).  
> Any persistent arrow-of-time must arise only when the user enables a primitive that can break reversibility.

This is stronger than “it looks random.” It is a mathematical guarantee.

## 2) Reversibility, detailed balance, and cycle affinity

For a Markov chain with transition probabilities \(P(z\to z')\), reversibility w.r.t. \(\pi\) means:
\[
\pi(z)\,P(z\to z')=\pi(z')\,P(z'\to z).
\]

Equivalent: all cycle affinities vanish. For any directed cycle \(\gamma\),
\[
\mathcal A(\gamma)=\sum_{e\in\gamma}\log\frac{P(e)}{P(e^{\rm rev})}=0.
\]

If any cycle affinity is nonzero, time-reversal symmetry is broken and persistent probability currents can exist.

## 3) How to enforce the null regime

The cleanest construction is a Gibbs stationary measure
\[
\pi(z)\propto \exp(-\beta E(z))
\]
and Metropolis-Hastings updates with symmetric proposals:
\[
A(z\to z') = \min\{1,\exp(-\beta(E(z')-E(z)))\}.
\]
This yields detailed balance by construction.

## 4) What it means to “toggle primitives” in this framework

- A primitive **adds transitions** (edges) to the state transition graph, or **changes the operator** by protocol ordering.
- In the null regime, the added transitions must remain reversible w.r.t. the same \(\pi\).

Only two primitives are allowed to break reversibility:

- **P₃ (protocol-cycle):** time-dependent ordering / noncommuting kernels.
- **P₆ (resource transduction):** nonconservative “work/affinity” terms.

This is not an arbitrary choice; it preserves the experiment’s interpretability:
- if you observe an arrow-of-time, you can attribute it to the enabled sources of non-equilibrium.

## 5) What “pattern ratchet” means without monotone writes

Because the sandbox avoids hard-coded irreversibility, “one-way” patterns must be understood as **metastable lock-in**:

- patterns persist for long times,
- reversal times can grow quickly with barriers or drive,
- but reversals are not forbidden by fiat.

Diagnostics therefore focus on:
- reversal rates,
- persistence times,
- and cycle affinities / entropy production.

---
</file>

<file path="docs/07_central_device_reversible_channels.md">
# Central Mathematical Device: Reversible Channels + Non‑Equilibrium Sources

This note captures the core device used to make the playground:
- mathematically clean,
- toggleable,
- and free of baked-in directionality.

## 1) Fast–slow state

Model the world as:
\[
Z_t=(X_t,W_t).
\]
- \(X_t\): fast substrate (particles).
- \(W_t\): slow variables (bonds, capacities, counters, fields, resources).

## 2) Generator decomposition (conceptual)

In continuous time, write the generator:
\[
\mathcal L = \mathcal L_0 + \sum_{i=1}^6 \mathbf 1_{P_i\ \text{on}}\,\mathcal L_i.
\]
In discrete time, this corresponds to:
- a base kernel, plus
- additional kernels/transitions contributed by enabled primitives.

## 3) Reversible null regime via Gibbs measure

Choose an energy \(E(Z)\) and define:
\[
\pi(Z)\propto e^{-\beta E(Z)}.
\]
Implement each enabled reversible channel (P₁, P₂, P₄, P₅ in null mode) via symmetric proposals plus Metropolis acceptance:
\[
A(Z\to Z')=\min\{1, e^{-\beta(E(Z')-E(Z))}\}.
\]
This guarantees detailed balance w.r.t. \(\pi\).

## 4) How P₆ breaks reversibility without imposing goals

When P₆ is enabled, allow certain transitions to include an antisymmetric “work” term:
\[
W(Z\to Z') = -W(Z'\to Z).
\]
Use the nonequilibrium Metropolis acceptance:
\[
A(Z\to Z')=\min\{1, e^{-\beta\Delta E+\beta W(Z\to Z')}\}.
\]
This breaks detailed balance when \(W\) has nonzero circulation on cycles.

Interpretation:
- \(W\) is not “fitness”; it is a nonconservative affinity (chemical potential, resource gradient).
- It supplies an arrow-of-time that other primitives may rectify into patterns.

## 5) How P₃ breaks reversibility (geometric/protocol effect)

Let \(K_A, K_B, K_C\) be kernels, each individually reversible w.r.t. the same \(\pi\).
Define the protocol cycle kernel:
\[
K_{\rm cyc}=K_C K_B K_A.
\]
Even though each step is reversible, the product is generally **nonreversible** if the kernels do not commute:
\[
[K_A,K_B]\ne 0 \quad\Rightarrow\quad K_{\rm cyc}\ \text{nonreversible}.
\]

Interpretation:
- P₃ creates geometric currents via order-of-operations.
- This is a non-equilibrium source that does not prescribe an outcome.

## 6) Why this device matches the “no bias to evolve” constraint

- In null mode (P₃=OFF, P₆=OFF), every enabled transition satisfies detailed balance ⇒ no net currents.
- Any observed arrow-of-time must come from P₃ and/or P₆, and can be quantified by cycle affinity and entropy production proxies.

---
</file>

<file path="docs/08_deliverable_A_null_stationary_measure.md">
# Deliverable A — Null‑Regime Stationary Measure

This document specifies an explicit **stationary measure** \(\pi\) for the null regime (P₃=OFF, P₆=OFF) and a constructive method to ensure **detailed balance**.

The goal is to ensure:
- no baked-in arrow-of-time,
- no implicit “evolve” bias,
- any directionality must come from enabled non-equilibrium primitives.

---

## A1) State space

Let \(Z=(X,W)\).

### Fast substrate \(X\)
- \(N\) particles on a 2D torus \(\mathbb T^2=[0,1)^2\)
- positions \(x_i\in\mathbb T^2\), \(i=1,\dots,N\)

### Slow variables \(W\) (bounded / discrete)

- P₁ bonds: \(w_{ij}\in\{0,1,\dots,L_w\}\) for all \(i<j\)
- P₂ apparatus: \(a_i\in\{0,1,\dots,L_a\}\)
- P₄ counters: \(n_k\in\{-L_n,\dots,L_n\}\) for carriers \(k\in\mathcal K\)
- P₅ field: \(S_q\in\{0,1,\dots,L_S\}\) for grid cells \(q\in\{1,\dots,G^2\}\)

---

## A2) Null‑regime energy function \(E(Z)\)

We define a Gibbs stationary distribution:
\[
\pi(Z)\propto \exp(-\beta E(Z)).
\]

A simple, local, compute-friendly energy is:

### (i) Repulsive substrate energy (prevents collapse)
Let \(r_{ij}=\|x_i-x_j\|_{\mathbb T^2}\). Define:
\[
U_{\mathrm{rep}}(X)=\sum_{i<j}\frac{\kappa_{\mathrm{rep}}}{2}\,\Big(\max\{0,r_0-r_{ij}\}\Big)^2.
\]

### (ii) Bond–geometry coupling (so bonds “mean” something spatially)
Let \(r_\star\) be a preferred bond length. Define:
\[
U_{\mathrm{bond}}(X,W_1)=\sum_{i<j}\frac{\kappa_{\mathrm{bond}}}{2}\,w_{ij}\,(r_{ij}-r_\star)^2.
\]

### (iii) Quadratic penalties on slow variables (boundedness + trivial equilibrium)
\[
E_{P1}(W_1)=\sum_{i<j}\frac{\lambda_w}{2}\,w_{ij}^2,
\qquad
E_{P2}(W_2)=\sum_{i}\frac{\lambda_a}{2}\,a_i^2,
\]
\[
E_{P4}(W_4)=\sum_{k\in\mathcal K}\frac{\lambda_n}{2}\,n_k^2,
\qquad
E_{P5}(W_5)=\sum_{q}\frac{\lambda_S}{2}\,S_q^2.
\]

### Total null energy
\[
\boxed{
E(Z)=U_{\mathrm{rep}}(X)+U_{\mathrm{bond}}(X,W_1)+E_{P1}(W_1)+E_{P2}(W_2)+E_{P4}(W_4)+E_{P5}(W_5).
}
\tag{A-E}
\]

---

## A3) Detailed-balance construction (Metropolis updates)

To guarantee the stationary measure exactly (up to discretization), implement all null-regime updates via **symmetric proposal + Metropolis acceptance**.

### Generic acceptance rule
Given a proposed move \(Z\to Z'\) with symmetric proposal \(Q(Z\to Z')=Q(Z'\to Z)\), accept with:
\[
\boxed{
A(Z\to Z')=\min\{1, \exp(-\beta(E(Z')-E(Z)))\}.
}
\tag{A-M}
\]

This ensures detailed balance:
\[
\pi(Z)P(Z\to Z')=\pi(Z')P(Z'\to Z).
\]

---

## A4) What “null regime” means operationally

- P₃ OFF: no time-dependent protocol ordering or driven flow field.
- P₆ OFF: no work/affinity terms, no chemical potential gradients.
- Any subset of {P₁,P₂,P₄,P₅} may be ON, but their transitions must still use (A-M).

This guarantees:
- **all cycle affinities are zero** (Kolmogorov criterion),
- there are no stationary probability currents,
- any later observed arrow-of-time must come from enabling P₃ and/or P₆.

---
</file>

<file path="docs/09_deliverable_B_transition_tables.md">
# Deliverable B — Primitive Transition Tables

This document specifies the **local transitions** each primitive contributes, and how toggles modify the transition rules.

Design constraints:
- In null regime (P₃=OFF, P₆=OFF), all enabled primitives must preserve detailed balance w.r.t. the same \(\pi(Z)\propto e^{-\beta E(Z)}\) from Deliverable A.
- P₃ and P₆ are the only primitives allowed to break reversibility.

---

## B0) Common components

### Base acceptance (null regime)
For a move \(Z\to Z'\):
\[
A_{\text{null}}(Z\to Z')=\min\{1,e^{-\beta\Delta E}\},\qquad \Delta E=E(Z')-E(Z).
\]

### Nonequilibrium acceptance (when P₆ is ON)
Add an antisymmetric work term \(W(Z\to Z')=-W(Z'\to Z)\):
\[
A_{P6}(Z\to Z')=\min\{1,e^{-\beta\Delta E+\beta W(Z\to Z')}\}.
\]
If \(W\equiv 0\), this reduces to the null acceptance.

### Protocol-cycle (when P₃ is ON)
P₃ can:
- modify proposal distributions (e.g., add a flow field), and/or
- enforce a specific noncommuting order of sub-kernels (see Deliverable C).

---

## B1) Base substrate update (always available)

### State
- positions \(x_i\in\mathbb T^2\).

### Proposal (P₃ OFF)
Pick \(i\) and propose:
\[
x_i' = x_i + \delta \quad (\mathrm{mod}\ 1),
\]
with \(\delta\) drawn from a symmetric distribution (e.g., uniform in a small disk).

### Acceptance
Use \(A_{\text{null}}\).

### Proposal (P₃ ON)
Pick \(i\) and propose:
\[
x_i' = x_i + u(x_i,\phi)\Delta t + \delta,
\]
where \(u(\cdot,\phi)\) is a bounded, time-periodic flow field with zero mean over a cycle.

Acceptance may still use \(A_{\text{null}}\) against the same \(E\); P₃ breaks reversibility through the time dependence / protocol.

---

## B2) P₁ — Operator-write (bond levels)

### State
- \(w_{ij}\in\{0,\dots,L_w\}\) for all \(i<j\).

### Energy terms affected
- \(\frac{\lambda_w}{2}w_{ij}^2\)
- \(\frac{\kappa_{\mathrm{bond}}}{2}w_{ij}(r_{ij}-r_\star)^2\)

### Proposal
Pick \(i<j\) and propose:
\[
w_{ij}' = w_{ij}\pm 1
\]
with equal probability, respecting bounds.

### Acceptance
- P₃ OFF, P₆ OFF: \(A_{\text{null}}\).
- P₆ ON: \(A_{P6}\) with a work term such as:
  \[
  W_{P1}=
  \begin{cases}
  +\eta_w\,\mu(x_{ij}) & \text{if } w_{ij}'=w_{ij}+1\\
  -\eta_w\,\mu(x_{ij}) & \text{if } w_{ij}'=w_{ij}-1
  \end{cases}
  \]
  where \(x_{ij}=(x_i+x_j)/2\).
- P₃ ON: may modulate attempt rate or ordering, but does not itself add bias in \(w\) unless it breaks reversibility via protocol.

---

## B3) P₂ — Feasible-set-write (apparatus levels)

### State
- \(a_i\in\{0,\dots,L_a\}\).

### Energy term affected
- \(\frac{\lambda_a}{2}a_i^2\).

### Proposal
Pick \(i\) and propose:
\[
a_i' = a_i\pm 1.
\]

### Acceptance
- Null: \(A_{\text{null}}\).
- With P₆ ON:
  \[
  W_{P2}=
  \begin{cases}
  +\eta_a\,\mu(x_i) & a_i'=a_i+1\\
  -\eta_a\,\mu(x_i) & a_i'=a_i-1
  \end{cases}
  \]
  and accept with \(A_{P6}\).
- P₃ ON: optional phase-modulated attempt rate; protocol ordering can create geometric effects.

**Note:** P₂ does not schedule budgets globally; \(a_i\) is a local reversible degree of freedom.

---

## B4) P₄ — Quantized/topological counters

### State
- \(n_k\in\{-L_n,\dots,L_n\}\) for carriers \(k\in\mathcal K\).

### Energy term affected
- \(\frac{\lambda_n}{2}n_k^2\).

### Proposal
Pick \(k\) and propose:
\[
n_k' = n_k \pm 1.
\]

### Acceptance
- Null: \(A_{\text{null}}\).
- P₆ ON: add work:
  \[
  W_{P4}=
  \begin{cases}
  +\eta_n\,\mu(x_k) & n_k'=n_k+1\\
  -\eta_n\,\mu(x_k) & n_k'=n_k-1
  \end{cases}
  \]
  and accept with \(A_{P6}\).
- P₃ ON: optional phase-modulated attempt rate.

---

## B5) P₅ — Protective field on grid (build/erode)

### State
- \(S_q\in\{0,\dots,L_S\}\) for cells \(q\in\{1,\dots,G^2\}\).

### Energy term affected
- \(\frac{\lambda_S}{2}S_q^2\).

### Proposal
Pick \(q\) and propose:
\[
S_q' = S_q\pm 1.
\]

### Acceptance
- Null: \(A_{\text{null}}\).
- P₆ ON: add work:
  \[
  W_{P5}=
  \begin{cases}
  +\eta_S\,\mu(x_q) & S_q'=S_q+1\\
  -\eta_S\,\mu(x_q) & S_q'=S_q-1
  \end{cases}
  \]
  accept with \(A_{P6}\).
- P₃ ON: phase-modulated attempt rate or indirect effects via motion patterns.

**Note:** “Safe sets” are observables \(K_t=\{q:S_q\ge\tau_S\}\), not monotone by fiat.

---

## B6) P₆ — Resource transduction (two implementation options)

### Option 1: affinity-only P₆ (minimal)
- No extra state variable.
- Turning P₆ on provides a scalar field \(\mu(x)\) and enables nonzero work terms \(W(\cdot)\) in other primitives’ acceptance ratios.
- Turning P₆ off sets \(W\equiv 0\).

### Option 2: explicit stored resource \(r_i\) (still manageable)
Add:
- \(r_i\in\{0,\dots,L_r\}\) per particle.

One can implement local exchange moves \(r_i\to r_i\pm 1\) with Metropolis accept under an energy including \(-\mu(x_i)r_i\), or couple resource consumption directly to “up” writes (joint moves).

This option enables explicit “banking” displays, but is not required for the base playground.

---

## B7) Summary: which primitives can break reversibility?

- In null regime, all enabled transitions satisfy detailed balance.
- P₆ breaks reversibility by adding nonconservative work terms with nonzero cycle circulation.
- P₃ breaks reversibility via time-dependent protocol ordering / noncommutativity.

---
</file>

<file path="docs/10_deliverable_C_cycle_affinity_wiring.md">
# Deliverable C — Cycle‑Affinity “Wiring Diagram”

This document specifies how to determine, from the enabled primitives, whether **nonzero cycle affinity** (hence irreversibility / arrow-of-time) is possible, and which minimal loop motifs to monitor.

---

## C1) Cycle affinity and reversibility

For transitions with probabilities/rates \(P(z\to z')\), define edge affinity:
\[
a(z\to z') := \log\frac{P(z\to z')}{P(z'\to z)}.
\]

For a directed cycle \(\gamma: z_0\to z_1\to\cdots\to z_m=z_0\), define cycle affinity:
\[
\mathcal A(\gamma) := \sum_{\ell=0}^{m-1} a(z_\ell\to z_{\ell+1})
= \log\frac{\prod_{\ell}P(z_\ell\to z_{\ell+1})}{\prod_{\ell}P(z_{\ell+1}\to z_\ell)}.
\]

**Kolmogorov cycle criterion:**  
A Markov chain is reversible iff \(\mathcal A(\gamma)=0\) for every cycle \(\gamma\).

---

## C2) Null regime: all affinities vanish

With Metropolis-Hastings against \(\pi\propto e^{-\beta E}\),
\[
\frac{P(z\to z')}{P(z'\to z)}=e^{-\beta(E(z')-E(z))}.
\]
So for a cycle, energy telescopes:
\[
\sum_{\gamma} (E(z')-E(z))=0 \quad\Rightarrow\quad \mathcal A(\gamma)=0.
\]

Therefore, with P₃=OFF and P₆=OFF, irreversibility is impossible by construction.

---

## C3) How primitives create nonzero affinity

### C3.1 P₆: antisymmetric work on edges
When P₆ adds a work term \(W(z\to z')=-W(z'\to z)\),
\[
a(z\to z') = -\beta\Delta E + \beta W(z\to z').
\]
Then cycle affinity is:
\[
\mathcal A(\gamma)=\beta\sum_{e\in\gamma}W(e).
\]
So P₆ produces force only if the induced “work 1-form” has nonzero circulation on some loop.

### C3.2 P₃: noncommuting reversible kernels (geometric pumping)
Let \(K_A,K_B,\dots\) be reversible kernels. The protocol product
\[
K_{\rm cyc}=K_C K_B K_A
\]
is generically nonreversible when the kernels do not commute. This implies existence of some state-space cycle \(\gamma\) with \(\mathcal A(\gamma)\ne 0\).

---

## C4) Minimal motifs

### Motif M0 (baseline)
Enabled subset of {P₁,P₂,P₄,P₅}, with P₃=OFF and P₆=OFF:
\[
\mathcal A(\gamma)=0\quad\forall\gamma.
\]

---

### Motif M6 (P₆ + any writable coordinate + two contexts)

Assume P₆ provides a context \(\mu\) with at least two bins: High (H) and Low (L).

Pick any reversible memory coordinate \(y\in\{w,a,n,S\}\) with \(\pm 1\) steps and P₆ work:
\[
W((\text{ctx},y)\to(\text{ctx},y+1)) = +\eta\,\mu(\text{ctx}),
\qquad
W((\text{ctx},y+1)\to(\text{ctx},y)) = -\eta\,\mu(\text{ctx}).
\]

Consider the 4-cycle:
\[
(H,y)\xrightarrow{y+}(H,y+1)\xrightarrow{\text{move}}(L,y+1)\xrightarrow{y-}(L,y)\xrightarrow{\text{move}}(H,y).
\]
Then
\[
\boxed{\mathcal A_{M6} = \beta\eta(\mu_H-\mu_L).}
\]

**Wiring rule:**  
P₆ alone is not enough; you need:
- a memory transition edge (P₁/P₂/P₄/P₅),
- movement between contexts with differing \(\mu\),
- and a loop that includes both.

---

### Motif M3 (pure protocol pump from P₃)

Even if each substep kernel is reversible, a cyclic protocol can yield nonreversible stroboscopic dynamics if kernels do not commute.

Minimal conceptual example:
- three coarse states \(\{A,B,C\}\),
- three reversible “mix A↔B”, “mix B↔C”, “mix C↔A” kernels,
- protocol applies them in order.

The product kernel is generically nonreversible ⇒ some 3-cycle has \(\mathcal A\neq 0\).

**Wiring rule:**  
P₃ can generate irreversibility without P₆, but you need:
- at least a 3-state coarse structure (or a 2D state graph),
- noncommuting sub-operations.

---

### Motif M3+W (protocol pump rectified into memory)

Let \(K_X\) update \(X\) and \(K_y\) update a memory coordinate \(y\) (P₁/P₂/P₄/P₅), each individually reversible w.r.t. \(\pi\), but coupled through \(E(X,y)\).

Under protocol ordering:
\[
K_{\rm cyc}=K_y K_X,
\]
noncommutativity yields nonreversibility, so there exists a loop in the joint \((X,y)\) state space with \(\mathcal A\neq 0\).

This is the minimal way for P₃ to create persistent patterning in a writable variable without any monotone write rule.

---

## C5) Practical “wiring diagram” summary

- If **P₃=OFF and P₆=OFF**: no irreversibility possible (all \(\mathcal A=0\)).
- If **P₆=ON**: irreversibility possible iff there exist loops that traverse contexts where \(\mu\) differs (M6).
- If **P₃=ON**: irreversibility possible iff protocol uses noncommuting sub-steps (M3).
- The most robust patterning arises when:
  - P₃ and/or P₆ supply a non-equilibrium source,
  - and P₁/P₂/P₄/P₅ provide state variables that can carry the resulting flux as visible patterns.

---
</file>

<file path="docs/11_deliverable_D_diagnostics.md">
# Deliverable D — On‑Screen Diagnostics (No Interpretation Baked In)

Diagnostics must measure:
- irreversibility / arrow-of-time (cycle affinity),
- structure / patterns (descriptive),
- persistence (metastability),
without implying “progress” or optimizing anything.

All metrics are designed to be computable online with a sliding window \(T_w\).

---

## D1) Minimal event log

Record accepted events as:
\[
e=(\texttt{type}, \Delta, \text{context}, \Delta E),
\]
where:
- type ∈ {w, a, n, S, x}
- \(\Delta=\pm 1\) for memory variables (w,a,n,S)
- context includes (optional) bins: \(c_\mu\) (H/L), \(c_\phi\) (protocol phase bin), \(c_q\) (grid region)
- \(\Delta E\) is energy difference for accepted moves (useful for debugging)

Motion events (type x) can be tracked via per-frame summaries rather than per-move logs.

---

## D2) Irreversibility diagnostics

### D2.1 Net flux per variable

For each memory variable type \(y\in\{w,a,n,S\}\) in a window:
\[
N_y^+,\ N_y^-.
\]
Define net flux:
\[
J_y := \frac{N_y^+ - N_y^-}{T_w}.
\]

### D2.2 Empirical edge affinity per variable

With pseudocount \(\alpha\) (e.g., 1):
\[
A_y := \log\frac{N_y^+ + \alpha}{N_y^- + \alpha}.
\]

### D2.3 Memory entropy production proxy

\[
\Sigma_{\mathrm{mem}} := \sum_{y} J_y A_y.
\]
- In null regime: \(\Sigma_{\mathrm{mem}}\approx 0\).
- Under P₃/P₆: \(\Sigma_{\mathrm{mem}}\) may become nonzero.

### D2.4 Motif M6 loop affinity estimator

For each \(y\), bin events by high/low \(\mu\) context:

- \(N_H^{y+},N_H^{y-},N_L^{y+},N_L^{y-}\).

Define:
\[
\widehat{\mathcal A}_{M6}(y)
:=\log\frac{(N_H^{y+}+\alpha)(N_L^{y-}+\alpha)}{(N_H^{y-}+\alpha)(N_L^{y+}+\alpha)}.
\]
This directly estimates the two-context loop bias from Deliverable C.

---

## D3) Protocol-cycle diagnostics (P₃)

### D3.1 Pumped displacement per protocol period

Let \(T_\phi\) steps define one protocol period. For cycle \(k\):
\[
D_k := \frac{1}{N}\sum_{i}\mathrm{unwrap}\big(x_i(t_k+T_\phi)-x_i(t_k)\big).
\]
Define pumped current:
\[
J^{\mathrm{pump}}_k := \frac{D_k}{T_\phi}.
\]

### D3.2 Hysteresis loop area (geometric signature)

Pick two observables \(O_1(t),O_2(t)\) sampled across the cycle. Compute polygon area:
\[
\mathcal L_k := \frac12\sum_m (O_1^{(m)}O_2^{(m+1)}-O_1^{(m+1)}O_2^{(m)}).
\]
Nonzero \(\mathcal L_k\) indicates phase-lag / noncommutativity in that observable plane.

---

## D4) Structure descriptors (pattern snapshots)

### D4.1 P₁ bond graph
Choose threshold \(\tau_w\). Define:
\[
G_\tau(t)=\{(i,j): w_{ij}\ge\tau_w\}.
\]
Show:
- edge count, mean degree,
- giant component size,
- number of connected components,
- histogram of \(w\).

Optional: spectral-gap proxy via a few power iterations.

### D4.2 P₂ apparatus
Show:
- mean/variance of \(a\),
- histogram,
- neighbor correlation \(\mathrm{corr}(a_i,\overline a_{\mathcal N(i)})\).

### D4.3 P₄ counters
Show:
- histogram of \(n\),
- mean and mean magnitude,
- flux/affinity \(J_n,A_n\).

### D4.4 P₅ field / safe-set observable
Show:
- heatmap of \(S_q\),
- mean field,
- area fraction above threshold:
  \[
  A_S := \frac{1}{G^2}\#\{q:S_q\ge\tau_S\},
  \]
- connected components and largest component fraction.

### D4.5 P₆ resource (optional explicit variable)
If explicit \(r_i\) is used:
- mean/variance, histogram,
- inequality proxy (Gini).

---

## D5) Persistence / metastability diagnostics

### D5.1 Jaccard stability for set patterns

For any set-valued pattern \(A(t)\), define:
\[
d_J(t;\Delta)=1-\frac{|A(t)\cap A(t-\Delta)|}{|A(t)\cup A(t-\Delta)|+\epsilon}.
\]
Use to display “time since last large change” for:
- bond graph backbone \(G_\tau\),
- safe-set \(K_{\tau_S}\).

### D5.2 Partition stability (if clustering is used)
Compute ARI or VI between cluster labels at \(t\) and \(t-\Delta\).

### D5.3 Reversal event rates
For a scalar observable \(Y(t)\), count crossings of a hysteresis band \([y_{\rm low},y_{\rm high}]\) to quantify “stickiness” without assuming monotonicity.

---

## D6) Null-regime sanity checks (debug mode)

When P₃=OFF and P₆=OFF:
- \(\Sigma_{\mathrm{mem}}\approx 0\),
- \(A_y\approx 0\),
- pumped current \(\|J^{\mathrm{pump}}\|\approx 0\).

Always show energy breakdown:
- \(U_{\rm rep}, U_{\rm bond}, \sum w^2, \sum a^2, \sum n^2, \sum S^2\),
to detect implementation errors.

---
</file>

<file path="docs/12_browser_impl_notes.md">
# Browser Implementation Notes (Math-to-Code Translation)

These notes are implementation-oriented but keep the discussion mathematical: how to translate the reversible-channel design into a manageable browser simulation.

## 1) Discretize slow variables aggressively

To keep the state space finite and updates cheap:
- \(w_{ij}\in\{0,\dots,L_w\}\) with small \(L_w\) (e.g., 6–12)
- \(a_i\in\{0,\dots,L_a\}\) small
- \(n_k\in\{-L_n,\dots,L_n\}\) small
- \(S_q\in\{0,\dots,L_S\}\) small
- optional \(r_i\in\{0,\dots,L_r\}\) small

Discrete ±1 proposals match Deliverables A/B and simplify cycle-affinity estimation.

## 2) Keep the interaction graph sparse

Although \(w_{ij}\) can be defined for all pairs, for performance:
- maintain a neighbor list \(\mathcal N_i\) using spatial hashing (grid buckets),
- only allow bond proposals for pairs within a radius (or propose uniformly among neighbor pairs).

If you do this, ensure proposal symmetry:
- proposal probability for \((i,j)\) must equal that for \((i,j)\) in reverse (same edge).

## 3) Update scheduling as a mixture (null) vs protocol (P₃)

### Null regime
Use a mixture of kernels per step:
- with probabilities \(p_X,p_{P1},p_{P2},p_{P4},p_{P5}\) (constants),
pick one move type and apply a Metropolis step.

This yields a time-homogeneous reversible chain.

### With P₃ on
Switch from “mixture” to “protocol” scheduling:
- apply kernels in a fixed cyclic order (e.g., X → P1 → P4 → P2 → P5 → …),
or modulate proposal distributions by phase \(\phi(t)\).

Noncommutativity can appear even if each kernel is individually reversible.

## 4) Energies and local ΔE

Choose \(E(Z)\) so that ΔE for a local move depends only on local terms:
- moving one particle affects only nearby repulsion terms and bonds involving that particle,
- changing one \(w_{ij}\) affects only that edge’s terms,
- changing one \(S_q\) affects only that cell’s term, etc.

This keeps each accept/reject O(local degree).

## 5) Implementing P₆ without heavy resource accounting

The minimal P₆ (affinity-only) is easiest:
- define a scalar field \(\mu(x)\) (two patches is enough),
- define work terms \(W\) for “up” vs “down” moves of memory variables,
- use acceptance with \(-\beta\Delta E+\beta W\).

This is enough to create nonzero cycle affinities via M6 motifs.

## 6) Diagnostics are counting problems

All irreversibility diagnostics can be computed from:
- forward/back counts \(N^+,N^-\),
- context-binned counts \(N_H^{+},N_L^{+},...\),
- periodic sampling for protocol loops.

This is cheap and robust in-browser.

## 7) Avoid hidden directionality

Common pitfalls:
- using one-way clamps (e.g., “if stable, only strengthen”)
- using schedules (“if things look good, increase capacity”)
- using base dynamics that already align/order by design

Stick to:
- reversible updates in null regime,
- P₃/P₆ as the only non-equilibrium sources,
- diagnostics that report what happened, not what should happen.

---
</file>

<file path="docs/13_open_questions_and_extensions.md">
# Open Questions and Extensions

This document lists natural extensions and research questions suggested by the current spec.

## 1) Minimal emergence of “ratchet stacking”

Given primitives P₁–P₆, investigate whether:
- enabling one primitive creates conditions that make another primitive’s effects more likely,
- producing layered organization without any external schedules.

Formally: look for regimes where enabling a subset increases the observed cycle affinities for additional motifs (Deliverable C), and increases persistence times for patterns (Deliverable D).

## 2) Communication toy worlds inside the sandbox

Implement a minimal “meaning–signal” micro-world (separate from particle physics) to explore Weakness × Economy behavior in the playground paradigm:

- implement “weakness” as saturating influence and/or bounded posterior updates,
- implement “economy” as energy cost of emitting symbols,
- avoid global objectives: let any directionality arise only from P₃/P₆.

Compare empirical mutual information growth to the theoretical bound:
\[
I \le (B/c_{\min})\,\varepsilon.
\]

## 3) Protocol geometry experiments (pure P₃)

Design protocol schedules where:
- each substep is reversible,
- but the product is nonreversible.

Use hysteresis loop area and pumped currents as diagnostics.

## 4) Quantized step barriers (P₄)

Explore barrier shaping:
- make \(\Delta E(n\to n+1)\) depend on \(n\),
- test whether reversal times scale roughly like \(\exp(\beta\Delta E)\).

## 5) Safe-set field dynamics (P₅)

Compare:
- reversible Metropolis field updates (null),
- driven build/erode updates under P₆.

Look for emergent “habitats” and their persistence.

## 6) Emergent replication (later layer)

Even though replication/heredity are not assumed at the base layer, one can ask:
- do any combinations of primitives produce structures that effectively copy themselves?
- can copying arise as a higher-level pattern (e.g., templating / polymerization analogs) once P₆ is enabled?

This should be studied as an *emergent* phenomenon, not an imposed rule.

---
</file>

<file path="docs/14_ratchet_motifs_and_asymmetries.md">
# Ratchet Motifs and Asymmetries (Supplement)

This note collects supplemental analysis that may be useful for interpreting results from the playground. It is not required for the core A–D deliverables.

## 1) Stabilization vs ratchet (core distinction)

Many two-term systems produce a stable equilibrium but do not “ratchet”.

- **Stabilization:** a Lyapunov function exists; dynamics converge to a stationary point under fixed constraints.
- **Ratchet:** there exists a coarse variable \(P(Z)\) with persistent directional drift or metastable stepwise growth, typically requiring:
  - a non-equilibrium source (P₃/P₆),
  - a rectification/memory channel (P₁/P₄/P₅ and sometimes P₂),
  - and brakes (boundedness).

Weakness × Economy at fixed \((\lambda,\mu)\) is primarily a stabilization mechanism (within-epoch optimization). It becomes a ratchet only if the constraint regime itself changes across epochs.

## 2) Motif: Expand × Rectify

A canonical motif has two interlocking parts:

- **Expand:** explore/activate new degrees of freedom (e.g., a protocol or noise-driven exploration).
- **Rectify:** convert transient structure into memory (e.g., bonds, counters, barriers).

This often yields self-reinforcement and one-way behavior by default if rectification is monotone or metastably absorbing.

In primitive terms:
- Expand can be driven by P₃ and/or P₆.
- Rectify often uses P₁ (operator-write), P₄ (click variables), P₅ (fields).

## 3) Motif: Constraint × Constraint

Here capability is bounded by:
- a local cap and a global budget (Weakness × Economy is canonical).

At fixed constraints this is self-controlling but typically not open-ended. A “capacity ratchet” requires constraints to change in a self-controlled way.

In primitive terms:
- constraints live naturally in P₂,
- budgets can become dynamical under P₆-type resource coupling,
- protocol effects (P₃) can route effort across contexts.

## 4) A useful asymmetry hypothesis

An observed asymmetry in many designs:

- Expand×Rectify motifs can be **SC+SR+OW** “by default” if rectification creates metastable memory and expand continues to supply variation.
- Constraint×Constraint motifs tend to be **SC by default** (bounded), but **SR/OW** require:
  - a mechanism for constraint/budget adaptation, and/or
  - a non-equilibrium driver that keeps generating exploitable gradients.

This is not a law, but a recurring structural difference:
- Memory writes naturally accumulate if decay is slow.
- Constraint regimes do not expand unless something supplies the additional degrees of freedom (resources, apparatus, protocols).

The Ratchet Playground is designed to test such hypotheses empirically without inserting schedules.

---
</file>

<file path="docs/15_spacetime.md">
Yes—there’s a pretty tight *mathematical* relationship once you strip away the specific stories (“life”, “spacetime”, “language”) and look at the shared core:

> **(i) build a protected low‑dimensional “logical” subspace out of noisy microscopic degrees (error correction / redundancy), and
> (ii) pay dissipation to maintain a directed phase variable (a clock / arrow) with bounded uncertainty.**

Quantum holography makes (i) spectacular by identifying *geometry* with the redundancy structure of a quantum code. Ratchet theory makes (ii) explicit by identifying *directionality* with nonzero cycle affinities / entropy production. And living systems need both: robust “logical” states and a reliable phase/time signal.

Below is a rigorous way to line them up.

---

## 1) What “spacetime from QECC” means mathematically

### Encoding as an isometry

A quantum error‑correcting code is an encoding isometry
[
V:;\mathcal H_{\text{bulk}} \to \mathcal H_{\text{bdy}},
\qquad
\mathcal H_{\text{code}} := V(\mathcal H_{\text{bulk}})\subseteq \mathcal H_{\text{bdy}}.
]

Correcting erasure of a boundary region (A) means:

[
\exists;\mathcal R_{A^c};\text{(CPTP recovery on }A^c);\text{s.t.};
\mathcal R_{A^c}!\big(\operatorname{Tr}_{A}[V\rho V^\dagger]\big)=V\rho V^\dagger
\quad \forall \rho.
\tag{QEC-erasure}
]

Equivalent (exact) conditions include the Knill–Laflamme criterion
[
P,E_a^\dagger E_b,P = c_{ab},P,
\quad P:=VV^\dagger,
]
for all error operators (E_a) supported on (A).

### AdS/CFT as “operator algebra QEC”

The key holography claim (Almheiri–Dong–Harlow) is that *bulk locality / subregion duality* behaves like QEC: bulk operators can be reconstructed from boundary subregions in a way that’s naturally phrased as (operator-algebra) quantum error correction. ([arXiv][1])

### Geometry from redundancy (HaPPY / tensor networks)

Toy models like the HaPPY code build (V) from perfect tensors arranged on a hyperbolic tiling; the tiling graph supplies a literal notion of “bulk geometry”, and correctability properties map to “which bulk points are reconstructible from which boundary regions.” ([arXiv][2])
More broadly, tensor-network constructions (MERA and variants) connect hierarchical encoding/renormalization to hyperbolic geometries reminiscent of spatial slices of AdS. ([arXiv][3])

So: **spacetime (at least in these models) is the *shape* of an encoding/redundancy structure.**

---

## 2) What a ratchet is in the same level of math

Abstract your ratchet-playground / “six primitives” world as a (possibly driven) Markov jump process on a state space (\Omega) with rates (k(x\to y)).

### Null regime = detailed balance

There exists an energy (E) and stationary (\pi(x)\propto e^{-\beta E(x)}) such that
[
\pi(x),k(x\to y) = \pi(y),k(y\to x).
\tag{DB}
]

### Driven regime = add “work” / protocol, break DB

A standard nonequilibrium parametrization is:
[
\log \frac{k(x\to y)}{k(y\to x)} = -\beta\big(E(y)-E(x)\big) + \beta,W(x\to y),
\tag{NEQ}
]
where (W) is antisymmetric (work-like). For a directed cycle (\gamma),
[
\mathcal A(\gamma):=\sum_{(x\to y)\in\gamma}\log\frac{k(x\to y)}{k(y\to x)}
= \beta\sum_{(x\to y)\in\gamma}W(x\to y)
]
is the **cycle affinity**.

Entropy production rate is (one standard form)
[
\sigma = \sum_{x,y} J(x,y),\log\frac{k(x\to y)\pi(x)}{k(y\to x)\pi(y)};\ge 0,
]
and (\sigma=0) iff detailed balance holds.

So the ratchet’s *arrow* is exactly “(\exists\gamma) with (\mathcal A(\gamma)\neq 0)”, i.e., persistent currents.

---

## 3) Clock emergence is *literally* a theorem about ratchets

A “clock” is a system with a **monotone phase-like observable** (Q_t) (tick count, winding number, cycle current) whose mean grows (\propto t), while fluctuations stay controlled.

The deep, quantitative fact is: **any such current-like clock variable has a precision–dissipation tradeoff.** This is captured by thermodynamic uncertainty relations (TURs). Barato–Seifert proved a TUR linking current fluctuations to entropy production. ([APS Link][4])

A canonical long-time TUR form for an integrated current (Q_t) is:
[
\frac{\mathrm{Var}(Q_t)}{\langle Q_t\rangle^2};\gtrsim;\frac{2}{\sigma,t},
\tag{TUR}
]
(up to model-dependent refinements / finite-time versions).

### Interpretation

* To make (\mathrm{Var}(Q_t)/\langle Q_t\rangle^2) small (a precise clock), you need (\sigma t) large (you must dissipate).
* In detailed balance ((\sigma=0)), you cannot have a sustained directed current and you cannot get an arbitrarily good autonomous clock from cycle currents.

This is not just abstract: it’s been specialized to “timekeeping” models, including autonomous quantum clocks (Erker et al.) and experimental measurements of timekeeping cost. ([APS Link][5])

**So, yes:** in a very strict mathematical sense, *a ratchet is a clock-constructor*, because it is the minimal structure that yields a persistent phase current—and the cost/accuracy tradeoff is controlled by entropy production.

---

## 4) Error correction is the other half: protected “logical” degrees

Now connect to QEC.

### QEC viewpoint

A QECC creates **logical degrees of freedom** that are robust to a class of errors (erasures, local noise). That robustness is expressed by the existence of recovery channels (Eq. QEC-erasure) and structural conditions like Knill–Laflamme.

### Ratchet viewpoint

A ratchet creates **metastable / slow** degrees of freedom—patterns that persist in the face of fast local noise. There’s a well-developed Markov-chain analogue of this:

* A “logical manifold” corresponds to a low-dimensional slow subspace of the generator (L) (small nonzero eigenvalues) or to metastable basins with large return probability.
* “Errors” are perturbations that knock the system within the basin (correctable) or across basins (logical flip).
* “Recovery” is the effective relaxation back into the basin, which can be passive (equilibrium) or active (driven).

Formally, let (\mathcal C\subset \Omega) be a set of microstates representing a “code” pattern, and let (N) be a noise kernel. A classical exact correction condition would be:
[
\exists R;\text{(recovery kernel)};\text{s.t.};;
R,N(\delta_x);\text{is supported in }\mathcal C
\quad \forall x\in \mathcal C.
\tag{Classical-correct}
]

That’s the direct classical cousin of the QEC erasure condition. (The quantum case is richer because of superposition and operator algebra structure, but the “protected logical degrees via recovery” pattern is shared.)

### Why life needs both

* **Without a code-like protected manifold**, you have no stable “state” to be a self, a memory, or a macro-variable.
* **Without a ratchet current**, you have no reliable internal phase variable to order events (“time”) beyond reversible fluctuations.

So “life builds structure + clock” becomes:

> maintain a protected logical subspace *and* maintain a directed current (phase) to traverse it.

This is exactly the “code + clock” decomposition.

---

## 5) Where spacetime-like structure enters

Here is the cleanest mathematical bridge between “spacetime from QECC” and “structure from ratchets”:

### In holography/QECC: geometry = correctability structure

In HaPPY / AdS-QEC interpretations, the *pattern* of which boundary regions can reconstruct which bulk operators is the core. ([arXiv][1])

That “pattern” is an abstract adjacency/containment structure that can be represented as a graph or tensor network, and in these models it corresponds to a hyperbolic geometry. ([errorcorrectionzoo.org][6])

### In ratchet systems: geometry = dependency / redundancy structure

Given a complex stochastic system, you can define an emergent “geometry” from:

* interaction graph weights (who influences whom),
* or **information geometry**: distances based on mutual information / conditional independence,
* or diffusion distances from the Markov generator (commute time / resistance distance analogues).

When ratchets “sculpt the latent space,” they are changing these dependency structures. If the system develops *hierarchical redundancy* (many-to-one encoding repeated across layers), you get something mathematically similar to tensor-network / renormalization geometries (MERA-like hierarchies). ([arXiv][3])

So the analogy is:

* **QECC holography:** fixed encoding map (V) whose network geometry we interpret as spacetime.
* **Ratchet theory:** dynamics that can *generate* (learn/evolve) an encoding structure by building redundancy and protected slow variables; the induced dependency graph is your emergent “space.”

Not the claim that life “creates physical spacetime,” but that it creates **effective internal spatiotemporal structure** (a latent geometry + a clock) by the same two primitives: redundancy (error correction) + dissipation (ratchet currents).

---

## 6) One rigorous synthesis statement

Here is a concise proposition that captures the relationship:

### Proposition: Ratchet–code decomposition of internal spatiotemporal structure

Let (Z_t) be a driven Markov process with generator (L = L_{\text{rev}} + L_{\text{drive}}), where (L_{\text{rev}}) satisfies detailed balance and (L_{\text{drive}}) induces nonzero cycle affinities.

Assume:

1. (**Metastable code**) (L) has a low-dimensional slow subspace (timescale separation), inducing a coarse variable (L_t) (logical state) that is reconstructible from multiple disjoint subsets of microscopic degrees with small error (redundancy / correctability).
2. (**Clock current**) there exists an integrated current (Q_t) with (\langle Q_t\rangle \propto t) and TUR-type precision bound controlled by entropy production (\sigma).

Then:

* (L_t) behaves as a **protected logical degree** (a stochastic analogue of an error-corrected logical qubit),
* (Q_t) behaves as an **internal clock** whose achievable precision is bounded by dissipation (TUR),
* the redundancy structure “who can reconstruct (L_t) from where” defines a natural emergent “spatial” adjacency structure, analogous in form to holographic-QECC correctability graphs.

The known parts of this proposition (TUR bounds, QEC-as-holography) are established in their respective domains. ([APS Link][4])
The synthesis is: **ratchets are a dynamical route by which systems can *construct* (not assume) protected logical degrees and a clock variable—exactly the ingredients that QEC holography treats kinematically.**

---

## 7) What this means for your project (in one sentence)

If you view “ratchet theory” as “how structure bootstraps itself,” then the QECC–spacetime story is the *static* endpoint: a stable encoding geometry. Ratchets are the *dynamical* mechanism that could, in principle, generate and maintain such encoding structures—and simultaneously generate the internal clock needed to traverse them—because clocks and codes both fundamentally cost entropy production. ([APS Link][5])

---

If you want to make this connection operational in the ratchet-playground later, the mathematically clean tests would be:

1. **Clock test:** choose a cycle current as “ticks” and verify a TUR-like bound using your measured entropy production proxy (your M6/sigma diagnostics).
2. **Code test:** define a coarse “logical” pattern (e.g., a basin label) and measure how reconstructible it is from partial observations (erasure), and how fast it recovers after local perturbations under drive.

Both tests are “no interpretation baked in”: they’re statements about currents, dissipation, redundancy, and recovery.

[1]: https://arxiv.org/abs/1411.7041?utm_source=chatgpt.com "Bulk Locality and Quantum Error Correction in AdS/CFT"
[2]: https://arxiv.org/abs/1503.06237?utm_source=chatgpt.com "[1503.06237] Holographic quantum error-correcting codes"
[3]: https://arxiv.org/pdf/1208.3469?utm_source=chatgpt.com "arXiv:1208.3469v3 [hep-th] 2 Oct 2012"
[4]: https://link.aps.org/doi/10.1103/PhysRevLett.114.158101?utm_source=chatgpt.com "Thermodynamic Uncertainty Relation for Biomolecular ..."
[5]: https://link.aps.org/doi/10.1103/PhysRevX.7.031022?utm_source=chatgpt.com "Does Thermodynamics Limit Our Ability to Measure Time?"
[6]: https://errorcorrectionzoo.org/c/happy?utm_source=chatgpt.com "Pastawski-Yoshida-Harlow-Preskill (HaPPY) code"




What I meant depends entirely on what we mean by “clock”. If we define “clock” too broadly, the statement is **false**. If we define “clock” in the *ratchet-theory sense* (a time‑oriented, time‑reversal–odd phase/tick variable), then there is a clean, standard theorem-level statement, and it uses only a **subset** of the 6 primitives.

Below I’ll (i) disprove the broad version, then (ii) state and prove a rigorous “ratchet ⇔ clock” claim under the *right* definition, (iii) identify the minimal subset of primitives, and (iv) map to known literature.

---

## 1) Disproof if “clock” just means “monotone time estimator”

If a “clock” is merely an observable (C_t) that increases on average like (t), then you don’t need any ratchet.

**Counterexample (equilibrium clock):** a Poisson process (N_t) with rate (\lambda) has
[
\mathbb E[N_t]=\lambda t,\qquad \mathrm{Var}(N_t)=\lambda t,
]
so (N_t) is a valid (noisy) elapsed‑time estimator—even though it contains **no directionality** and can be embedded in detailed-balance dynamics.

Even more relevant: any equilibrium (detailed-balance) Markov chain has a positive *activity* (jump count) (K_t) with (\mathbb E[K_t]\propto t). This does **not** require broken detailed balance.

So “ratchet is minimal for clocks” cannot be true under this broad definition.

---

## 2) The definition that makes your claim meaningful

In your framework (and in stochastic thermodynamics), the important thing isn’t “elapsed time”, it’s a **directed phase / arrow-of-time signal**: a clock hand that “wants” to go clockwise.

So define a **time‑oriented clock** as a functional (Q_t) of the trajectory that:

1. is **additive over jumps** (a current):
   [
   Q_t=\sum_{\text{jumps }x\to y \text{ up to }t} d(x,y),
   ]
   with increments (d(x,y)=-d(y,x)) (antisymmetric), and

2. has **nonzero drift** in stationarity:
   [
   v ;:=;\lim_{t\to\infty}\frac{\mathbb E[Q_t]}{t}\neq 0.
   ]

The antisymmetry is exactly “clockwise vs counterclockwise”. Under time reversal (reverse the jump sequence), such (Q_t) flips sign.

This is the notion used in “Brownian clocks” and biochemical-cycle clocks (counting net cycle completions). For example, Barato–Seifert explicitly frame Brownian clocks as proteins traversing a *cycle* that requires free energy (ATP hydrolysis) and count time via the dispersion of a corresponding **current**. ([arXiv][1])
Marsland et al. similarly treat biochemical oscillations as free-running clocks whose cyclic dynamics require breaking detailed balance, and analyze period fluctuations via current/first-passage definitions. ([PMC][2])

---

## 3) The rigorous theorem: oriented clocks exist **iff** you have a ratchet

Work in the standard setting: an irreducible finite-state continuous-time Markov chain (CTMC) with rates (k(x,y)) and stationary distribution (\pi).

Define the **steady edge current**
[
J(x,y):=\pi(x)k(x,y)-\pi(y)k(y,x).
]

### Theorem (Clock–ratchet equivalence, CTMC)

There exists a time‑oriented clock current (Q_t) (i.e., an antisymmetric increment function (d) with drift (v\neq 0)) **iff** the chain violates detailed balance (equivalently: it is nonreversible, has nonzero stationary currents, nonzero entropy production, or nonzero cycle affinity).

#### Proof, “only if” (necessity)

Assume **detailed balance** holds:
[
\pi(x)k(x,y)=\pi(y)k(y,x)\quad \forall x,y.
]
This implies **no net flux on any edge** (the “local traffic” picture of detailed balance). 
Hence (J(x,y)=0) for all edges.

Now take any antisymmetric increment (d(x,y)=-d(y,x)). The stationary drift is
[
v=\sum_{x,y}\pi(x)k(x,y),d(x,y).
]
Pair terms ((x,y)) and ((y,x)):
[
v=\frac12\sum_{x,y}\Big[\pi(x)k(x,y)d(x,y)+\pi(y)k(y,x)d(y,x)\Big]
=\frac12\sum_{x,y}\pi(x)k(x,y)\big(d(x,y)+d(y,x)\big)=0.
]
So **no** time‑oriented clock current can have nonzero drift under detailed balance.

#### Proof, “if” (sufficiency)

Assume the chain is **not** detailed-balance. Then there exists at least one edge ((x,y)) with
[
\pi(x)k(x,y)\neq \pi(y)k(y,x)\quad\Rightarrow\quad J(x,y)\neq 0.
]
Define the antisymmetric increment
[
d(u,v)=
\begin{cases}
+1,&(u,v)=(x,y),\
-1,&(u,v)=(y,x),\
0,&\text{otherwise.}
\end{cases}
]
Then the drift becomes
[
v=\sum_{u,v}\pi(u)k(u,v)d(u,v)=\pi(x)k(x,y)-\pi(y)k(y,x)=J(x,y)\neq 0.
]
So an oriented clock current exists.

That completes the equivalence.

---

## 4) “Minimal structure”: why the smallest autonomous clock needs **3** states

Nonreversibility requires a **cycle** in the transition graph (Kolmogorov loop criterion): a chain is reversible iff products of rates along every cycle match the reverse product. ([arXiv][3])

* With **2 states**, there is only one edge. Stationarity forces (\pi(1)k_{12}=\pi(2)k_{21}), hence detailed balance holds automatically, so (J_{12}=0). No oriented clock is possible.

* With **3 states**, a directed cycle is possible. Example (ring):
  [
  1\to2\to3\to1 \text{ at rate }a,\qquad 1\to3\to2\to1 \text{ at rate }b,
  ]
  with (a\neq b). Then the stationary distribution is uniform and the edge current is
  [
  J = \frac{a-b}{3}\neq 0.
  ]
  The integrated net number of clockwise jumps (minus counterclockwise jumps) is a time‑oriented clock.

So the **minimal motif** is: **a cycle of length ≥ 3 with nonzero affinity** (a biased/unbalanced ring). That is exactly the “unicyclic Brownian clock” motif in the literature. ([arXiv][1])

---

## 5) Which of the 6 primitives are “the ratchet” for clocks?

Now map this back to your primitive set.

### Key fact

Only primitives that **break time-reversal symmetry** can create (J\neq 0) (nonzero affinity). The others can create *states*, *constraints*, *memory*, etc., but in the null regime they remain detailed-balance.

In the language of stochastic thermodynamics, you can think of:

* **P6 (drive / work bias)**: introduces a nonzero “work” term (W(x\to y)) in the local detailed balance ratio
  [
  \log\frac{k(x\to y)}{k(y\to x)} = -\beta\Delta E + \beta W(x\to y),
  ]
  so cycles can acquire nonzero affinity (ratchet).

* **P3 (protocol / time-dependent scheduling)**: makes the dynamics **time-inhomogeneous**. Even if each instantaneous generator satisfies detailed balance, a periodic protocol can pump net currents (“stochastic pumps”). This is the Rahav–Horowitz–Jarzynski story. ([arXiv][4])
  But crucially, that “clock” is **not autonomous**: the protocol provides an external time reference.

* **P1/P2/P4/P5**: create and update carriers (bonds, apparatus, counters, fields). In the null regime they can be reversible (Metropolis/detailed balance), so by themselves they cannot yield a time‑oriented clock current.

### Therefore

* If by “clock” you mean a **free-running, autonomous, time-oriented clock** (a hand that drifts in one direction without external timing), the minimal subset is:

> **P6 + a carrier with ≥3 distinguishable states arranged in a cycle.**

In your primitives, the most direct carrier is **P4** (a discrete counter with ≥3 states, treated cyclically or with a winding number). So the minimal **primitive subset** is:

> **{P6, P4}**, with P4 having at least 3 states.

P1/P2/P5 are not necessary for *existence* of a clock, but they matter for **stability, coupling, and integrating the clock into “structure”**.

* If you allow an **externally driven clock**, then **P3 alone already supplies time** (because “phase of the protocol” is time). And it can also generate pumped currents even without constant-force drive. ([arXiv][4])
  This corresponds to Barato–Seifert’s distinction: periodic protocol clocks can achieve “arbitrary precision at arbitrarily low cost” *within the subsystem*, because the cost is offloaded to the external protocol generator. ([arXiv][1])

---

## 6) Is this already known in the literature?

Yes, in essentially the same mathematical form, just not phrased as “P1–P6”.

### Brownian clocks (constant force)

Barato & Seifert explicitly model clocks as biomolecular networks that count time via a cycle current and emphasize that the cycle requires free energy (ATP hydrolysis). ([arXiv][1])
Marsland et al. emphasize that biochemical oscillators require free energy to break detailed balance and sustain cyclic dynamics, and they define cycle-completion times via an integrated current crossing construction. ([PMC][2])

### Precision–dissipation tradeoff for current clocks

Thermodynamic uncertainty relations (TURs) give lower bounds on the relative fluctuations of any integrated current in terms of entropy production. A modern review (Horowitz & collaborators) summarizes long-time TUR forms like
[
\frac{\mathrm{Var}(J_\tau)}{\langle J_\tau\rangle^2} \gtrsim \frac{2k_B}{\Sigma_\tau}
\quad\text{or}\quad
\frac{\mathrm{Var}(j)}{\langle j\rangle^2}\gtrsim \frac{2k_B}{\sigma},
]
linking **current precision** to **dissipation** in nonequilibrium steady states. ([sites.lsa.umich.edu][5])
This mathematically underwrites “ratchet cost buys clock precision”.

### Stochastic pumps (periodic protocols)

Rahav–Horowitz–Jarzynski analyze directed flow generated by periodic variation of parameters, derive pump currents and no-pumping conditions, and connect pumped currents to geometric expressions. ([arXiv][4])
This is the canonical “P3-like” mechanism: directionality from time-dependent driving.

---

## 7) Bottom line

* If “clock” = “any monotone estimator of elapsed time”, your statement is **false** (equilibrium activity already works).

* If “clock” = “a time‑oriented hand / phase variable” (time‑reversal–odd current with nonzero drift), then:

> **A ratchet (broken detailed balance / nonzero cycle affinity) is necessary and sufficient for such a clock in time-homogeneous Markov dynamics.**
> The minimal motif is a **3-state biased cycle**.

* In your 6 primitives, that means:

> **Autonomous oriented clock:** minimally **P6 + (P4 with ≥3 states)**.
> **Externally driven oriented clock:** **P3** can supply the time reference and pump currents, but it’s not “autonomous”.

If you want, I can write this as a compact theorem/lemma package specifically in the “six primitives” language (i.e., “when P3=P6=0 the combined generator is reversible ⇒ all antisymmetric diagnostics have zero drift; when P6>0 there exists a cycle current; minimal counterexample is 3-state P4 ring”).

[1]: https://arxiv.org/abs/1610.07960 "[1610.07960] Cost and Precision of Brownian Clocks"
[2]: https://pmc.ncbi.nlm.nih.gov/articles/PMC6544898/ "
            The thermodynamic uncertainty relation in biochemical oscillations - PMC
        "
[3]: https://arxiv.org/pdf/1806.10154?utm_source=chatgpt.com "Reversibility Checking for Markov Chains"
[4]: https://arxiv.org/abs/0808.0015?utm_source=chatgpt.com "Directed flow in non-adiabatic stochastic pumps"
[5]: https://sites.lsa.umich.edu/horowitz-lab-new/wp-content/uploads/sites/1181/2020/03/NatPhysReview.pdf "Thermodynamic uncertainty relations constrain non-equilibrium fluctuations"



Yes — from here on, my **default** is the *two‑way*, *multi‑substrate* picture:

* the primitives (P1–P6) act on **base substrate** and on **meta substrates** (possibly multiple layers),
* meta substrates can **parameterize** the transition rules of lower layers (top‑down),
* and meta substrates are themselves **written/updated** by primitives whose proposals/acceptance depend on lower‑layer structure/flux (bottom‑up),

so the whole thing is one **closed Markov process on an extended state**, not “physics + an external controller”.

That closure is exactly what makes the system “dynamic” in the strong sense: the **generator of motion is part of the state**.

Below is a more rigorous way to relate this to “spacetime from QECC” and to make precise what’s “static” vs “dynamic”.

---

## 1) Static QECC spacetime is a kinematic redundancy pattern

In the AdS/CFT ↔ QEC story, you start with a (typically fixed) **encoding map** from “logical/bulk” degrees to “physical/boundary” degrees:

[
V:\ \mathcal H_{\text{bulk}}\to\mathcal H_{\text{bdy}},
\qquad
\mathcal H_{\text{code}} = V(\mathcal H_{\text{bulk}})\subseteq \mathcal H_{\text{bdy}}.
]

The core QEC property is: for certain boundary regions (A), the reduced state on (A^c) still suffices to **recover** the logical state (erasure correction):

[
\exists\ \mathcal R_{A^c} \ \text{s.t.}\
\mathcal R_{A^c}!\Big(\operatorname{Tr}_{A}[,V\rho V^\dagger,]\Big)=V\rho V^\dagger\quad \forall \rho.
]

The “geometry” in the toy-model sense is the **pattern of correctability / reconstructibility** (“which logical operators can be reconstructed from which boundary subregions”). This is the content of the ADH perspective on bulk locality as QEC. ([arXiv][1])
HaPPY and related tensor-network codes make this concrete by building (V) from a fixed tiling/network; the **network graph** is literally the geometric scaffold. ([arXiv][2])

**What is “static” about this?**
In these formulations, the *redundancy scaffold* (the encoding map (V), or the tensor network graph that represents it) is typically treated as **given**. Dynamics (time evolution) is then a separate story: apply some unitary/Hamiltonian on the boundary, states evolve, and you may interpret that as bulk time evolution — but the *code structure* is usually a kinematic ingredient.

(There are state-dependent and operator-algebra refinements in the literature, but the key point remains: the QEC structure is not presented as something that **emerges thermodynamically** from microscopic driven dynamics.)

---

## 2) Ratchet theory’s “dynamic spacetime” means the encoding scaffold itself is dynamical state

In the ratchet picture, we don’t start with a fixed (V). We start with **a coupled stochastic dynamics** on an *extended state space*:

[
Z_t = (Z^{(0)}_t, Z^{(1)}_t, \dots, Z^{(L)}_t),
]

where

* (Z^{(0)}) is the base substrate (particles, bonds, fields, counters…),
* (Z^{(\ell)}) for (\ell\ge 1) are meta layers (e.g., lifted parameter fields, latent “rule knobs”, etc.).

The dynamics are Markov on the *full* state:

[
\Pr(Z_{t+\Delta t}=z'\mid Z_t=z)=P(z\to z').
]

But crucially, **lower-layer transition kernels depend on higher-layer state**, and higher layers depend on lower layers:

[
k^{(0)}*{u}(x\to x') \quad\text{and}\quad
k^{(1)}*{x}(u\to u'),
]
so on the joint space ((x,u)) you have rates
[
k((x,u)\to(x',u))=k^{(0)}*{u}(x\to x'),
\qquad
k((x,u)\to(x,u'))=k^{(1)}*{x}(u\to u').
]

This is the minimal mathematical form of **two‑way causation** (top‑down and bottom‑up) that remains purely “physics”: it’s just one joint Markov process.

### What becomes “spacetime-like”

At each time (t), the effective micro generator is
[
\mathcal L_t = \mathcal L_{u_t},
]
so the **connectivity / geometry** that the micro substrate experiences is a *function of the evolving meta state*.

A clean way to formalize “geometry” for a Markov process is as a **weighted graph / Dirichlet form** induced by rates. For example, define the (time-dependent) undirected conductance on micro states:

[
c_t(x,y) := \frac12\big(\pi_t(x)k_t(x,y)+\pi_t(y)k_t(y,x)\big),
]

then the Dirichlet form
[
\mathcal E_t(f,f) := \frac12\sum_{x,y} c_t(x,y),(f(x)-f(y))^2
]
induces a natural diffusion/resistance geometry (commute-time / effective resistance metrics are built from the Laplacian associated with (c_t)).

Because (k_t(\cdot,\cdot)) depends on (u_t), this geometry is **dynamical**:
[
c_t = c(u_t),\quad \mathcal E_t = \mathcal E(u_t).
]

So “dynamic spacetime” here means:

* **space-like structure** = the evolving redundancy / coupling / diffusion geometry induced by the (evolving) transition structure,
* **time-like structure** = an internally generated oriented phase variable (a clock), discussed next.

This is different from fixed‑(V) codes where the “geometry graph” is fixed by construction.

---

## 3) Why ratchets are the clock part of this “spacetime”

For a clock in the ratchet sense, you don’t just want “some activity happens.” You want a **time-reversal–odd phase**: a “hand” that prefers clockwise over counterclockwise.

Mathematically, this is an **antisymmetric current** over transitions:
[
Q_t = \sum_{\text{jumps }x\to y\text{ up to }t} d(x,y),
\quad d(x,y)=-d(y,x),
]
with nonzero long-time drift:
[
\lim_{t\to\infty}\frac{\mathbb E[Q_t]}{t}\neq 0.
]

A key fact: such directed currents require breaking detailed balance (nonzero cycle affinities). When you have them, the clock’s precision is constrained by dissipation via thermodynamic uncertainty relations; in long-time form they bound relative current fluctuations by entropy production. This underlies the “precision costs energy” results for Brownian/biochemical clocks. ([APS Link][3])

So in our ratchet framework:

* **P6** (and/or **P3**) supply nonzero affinities/cycle currents → you can get an internal clock variable.
* Without P6/P3 (null regime, detailed balance), that specific kind of oriented clock cannot exist.

---

## 4) The missing piece: why ratchets can generate and maintain “encoding structures”

Here is the rigorous bridge between “ratchets build codes” and “QEC codes are redundancy patterns”:

### Encoding structure in dynamical-systems terms

Forget Hilbert spaces for a moment. A “code” is a *redundant* mapping between:

* a **logical** variable (L_t) (a coarse macro description),
* and a **physical** configuration (X_t).

A clean classical analogue of correctability is: there exist multiple partial views (subsets) (A) from which (L) can be recovered with small error. Define for each region (A) a best decoder (\delta_A) and reconstruction error
[
\epsilon_t(A)=\Pr\big[\delta_A(X_{t,A})\neq L_t\big].
]

The “geometry” is then the pattern (A\mapsto\epsilon_t(A)): which subregions have enough redundancy to reconstruct which logical degrees. This is the same *type* of object as the QEC reconstructibility structure that becomes “bulk geometry” in the holography story.

### Why making that structure *dynamical* requires two-way coupling and dissipation

In your ratchet system, meta variables (U_t) are allowed to **store** and **rewrite** the parameters that shape micro transitions (that’s top‑down), and their update rules depend on micro features/flux (bottom‑up).

This makes ((X_t,U_t)) a coupled (often bipartite) Markov process. In exactly this setting, stochastic thermodynamics gives a precise statement:

> The second-law balance for a subsystem is modified by a mutual-information *flow* term; information acquisition/flow is thermodynamically constrained and tied to entropy production.

Horowitz & Esposito formalize this as “thermodynamics with continuous information flow” for bipartite systems: information flow between subsystems appears explicitly in the entropy balance, and bounds what each subsystem can do. ([APS Link][4])

One way to express the core idea (schematically) is:

[
\sigma_{\text{total}} ;=; \sigma_X + \sigma_U ;-; \frac{d}{dt}I(X;U) ;\ge 0,
]
with refined decompositions that separate information *produced* and *transferred* during (X)-moves vs (U)-moves. ([APS Link][4])

Interpretation for us:

* Building a robust encoding means building/maintaining **mutual information / redundancy structure** between layers and across regions.
* Sustaining directed information flow (“meta learns micro and rewrites the rules”) is not free; it ties to entropy production and, when you want it to be persistent and directional, typically needs nonequilibrium drive.

So ratchets don’t just create “time”; they also provide the energetic channel to maintain **ongoing correction/recovery dynamics**, i.e., “keeping the code working” against noise.

This is exactly the “dynamic” complement to the kinematic QEC picture.

---

## 5) Putting it together: dynamic spacetime as code + clock

A precise synthesis statement that matches your “sounds like life” intuition is:

### Dynamic internal spacetime scaffold

A driven, multi-layer ratchet system can generate:

1. a **time coordinate** ( \tau(t)\sim Q_t ) (a directed phase current; a clock), and
2. a **space-like geometry** (g_t) defined by reconstructibility/redundancy structure across regions/layers (a code scaffold),

where both (Q_t) and (g_t) are **state-dependent and evolving** because the meta layers that define the transition kernel are themselves dynamical and coupled to micro flux.

In contrast, in the simplest QEC→geometry toy models, the code scaffold (the analog of (g)) is largely treated as fixed, and one studies what that fixed redundancy implies for locality/geometry. ([arXiv][1])

So the “interesting take” is not “quantum spacetime is static, life is dynamic” in the absolute sense; it’s more specific:

* **QEC/holography**: emphasizes that *given* an encoding with certain redundancy, you can interpret reconstructibility structure as geometry.
* **Ratchet theory**: proposes a *dynamical mechanism* by which an encoding/redundancy scaffold and an oriented phase variable can be *constructed and maintained* endogenously, through coupled irreversible currents acting on both micro and meta substrates.

---

## 6) Where your multi-layer + η picture fits cleanly

In your implementation direction (meta layers + a single (\eta) knob), you can interpret (\eta) as interpolating between:

* **More “kinematic/static-code-like”** regime: stronger conservative/energy-based cross-layer couplings (structure persists even without drive).
* **More “dynamical/life-like”** regime: structure becomes causally active mainly through nonequilibrium channels (drive/protocol), so it is *constructed/maintained* by dissipation and can adapt as the drive context changes.

This gives you an operational way to study the transition from “static redundancy scaffold” to “dynamically maintained redundancy scaffold” inside the same mathematical machine, without adding external objectives.

---

## 7) What would count as rigorous evidence of “dynamic spacetime” in the ratchet sim

When you later run experiments, “dynamic spacetime scaffold” should be assessed with *measurement-only* diagnostics:

### Clock diagnostics

* Choose an antisymmetric current (Q_t) (cycle completion count in some subsystem).
* Show (\mathbb E[Q_t]\propto t) with nonzero drift **only when P6/P3 are on**.
* Verify a TUR-style tradeoff between precision and dissipation proxies (M6 / entropy production). ([APS Link][3])

### Code/geometry diagnostics

* Define a candidate logical variable (L_t) (e.g., a slow macro label extracted from the state; you can pick something generic like a low-dimensional PCA/sign pattern of meta fields).
* Measure reconstructibility from subregions (A): estimate (\epsilon_t(A)) or (I(L_t; X_{t,A})).
* Track how the “wedge structure” (A\mapsto \epsilon_t(A)) changes over time.
* Show that this structure changes *because* meta layers evolve (two-way coupling), not because you imposed a fixed encoder.

That would be the direct, falsifiable analogue of “geometry = correctability structure,” but now with a **time-dependent** correctability pattern.

---

If you want, the next step is to formalize one clean theorem-level statement inside this framework:
a *bipartite* (micro/meta) version that proves (i) no oriented clock in the null regime, (ii) information acquisition/flow is bounded by dissipation, and (iii) the induced reconstructibility hypergraph is time-dependent exactly when meta layers causally affect micro kernels.

[1]: https://arxiv.org/abs/1411.7041?utm_source=chatgpt.com "Bulk Locality and Quantum Error Correction in AdS/CFT"
[2]: https://arxiv.org/abs/1503.06237?utm_source=chatgpt.com "[1503.06237] Holographic quantum error-correcting codes"
[3]: https://link.aps.org/doi/10.1103/PhysRevX.6.041053?utm_source=chatgpt.com "Cost and Precision of Brownian Clocks - APS Journals"
[4]: https://link.aps.org/doi/10.1103/PhysRevX.4.031015?utm_source=chatgpt.com "Thermodynamics with Continuous Information Flow"



I’ll formalize (and prove) a theorem that captures exactly the three ingredients we’ve been circling:

1. **an internal “clock”** = a time‑reversal–odd current with nonzero drift,
2. **an “encoding/constraint scaffold”** = persistent, directed information processing between micro and meta, and
3. **a “dynamic spacetime” analogue** = a *time‑varying* reconstructibility/geometry object because the *generator itself* is part of the evolving state (top‑down + bottom‑up closure).

I will work in the cleanest setting: a **finite-state bipartite continuous-time Markov process**. This is the standard mathematical model in stochastic thermodynamics, and it matches your default assumption (“primitives act on both base and meta; two‑way causation”) because bipartite means **each jump updates exactly one layer** (exactly how your simulation picks one primitive move at a time).

Everything extends to multiple layers (multipartite) and to discrete time (your Metropolis step chain); I’ll state those extensions at the end.

---

## 0) Setup: a closed two-way micro/meta Markov system

Let

* micro/base state: (X_t \in \mathcal X) (finite),
* meta state: (U_t \in \mathcal U) (finite),
* joint state: (Z_t := (X_t,U_t)\in\Omega:=\mathcal X\times \mathcal U).

Assume ((Z_t)) is an irreducible continuous-time Markov chain with **bipartite** transitions:

* **micro jump:** ((x,u)\to(x',u)) at rate (k_u(x,x')),
* **meta jump:** ((x,u)\to(x,u')) at rate (r_x(u,u')).

This captures **top-down** (micro rates depend on (u)) and **bottom-up** (meta rates depend on (x)) in the minimal, non-hand-wavy way.

Let (p_t(x,u)) be the time-(t) distribution.

Define the (oriented) **probability flux** for micro edges at fixed (u):
[
j_t^{u}(x\to x') := p_t(x,u),k_u(x,x'),\qquad
J_t^{u}(x,x') := j_t^{u}(x\to x')-j_t^{u}(x'\to x).
]
Similarly for meta edges at fixed (x):
[
\tilde j_t^{x}(u\to u') := p_t(x,u),r_x(u,u'),\qquad
\tilde J_t^{x}(u,u') := \tilde j_t^{x}(u\to u')-\tilde j_t^{x}(u'\to u).
]

---

## 1) Definitions of the three “spacetime ingredients”

### 1.1 Oriented clock current

An **oriented clock** is an additive functional
[
Q_t := \sum_{\text{jumps } z\to z' \text{ up to }t} d(z,z')
]
with antisymmetric increments (d(z,z')=-d(z',z)) and **nonzero stationary drift**
[
v := \lim_{t\to\infty}\frac{\mathbb E[Q_t]}{t}\neq 0
]
when the process is started in its stationary distribution.

This captures “hand prefers clockwise”: time reversal flips (Q_t).

### 1.2 Dissipation and “ratchet”

Say the joint dynamics is **reversible** (null regime) if it satisfies **detailed balance** with respect to some stationary (\pi):
[
\pi(z),w(z\to z')=\pi(z'),w(z'\to z) \quad\forall (z,z')
]
where (w) is the joint rate.

Equivalently (finite state), reversibility (\Longleftrightarrow) **zero steady currents** (\Longleftrightarrow) **zero steady entropy production**.

Say the system is a **ratchet** if it is **nonreversible** (violates detailed balance), i.e. has **nonzero cycle affinity / stationary currents**.

### 1.3 A reconstructibility “geometry” functional

You want something QECC-like: “which boundary regions can reconstruct which bulk info,” but now *dynamic*.

So we define a **kernel-level reconstructibility hypergraph** that depends only on **top-down rule parameters**, not on incidental same-time correlations.

Assume the micro variable is spatially factored over sites (\Lambda) (grid cells, etc.)
[
\mathcal X = \prod_{i\in\Lambda}\mathcal X_i.
]
For any region (A\subseteq \Lambda), let (x_A) be the restriction.

For each meta state (u), define the **one-step micro transition kernel** (P_u) (discrete-time view) or use the rates (k_u) (continuous-time view). For simplicity, define a small time step (\Delta>0) and let (P_u^\Delta(x\to x')) be the transition probability over (\Delta) when (U) is held fixed at (u) (this is well-defined from (k_u)).

Now define, for region (A), the induced **marginal kernel** on (A):
[
P_{u,A}^\Delta(x_A\to x_A') := \sum_{x_{\Lambda\setminus A},,x'_{\Lambda\setminus A}}
P_u^\Delta(x\to x'),\mathbf 1{x_A \text{ matches},, x'_A \text{ matches}}.
]

Define the **distinguishability** of meta states by observing region (A)’s local dynamics:
[
D_A(u,u') ;:=; \sup_{x_A\in\mathcal X_A}
D_{\mathrm{KL}}!\big(P_{u,A}^\Delta(x_A\to \cdot),\big|,P_{u',A}^\Delta(x_A\to \cdot)\big).
]
(Any other strict divergence works too; KL is convenient.)

Fix a threshold (\varepsilon>0). Define the **reconstructibility hypergraph**
[
\mathsf H_\varepsilon(u) ;:=;\big{A\subseteq\Lambda:;\exists u'\neq u \text{ with } D_A(u,u')\ge \varepsilon\big}.
]

Interpretation: (A\in \mathsf H_\varepsilon(u)) means “from region (A)’s local transition statistics, the meta state (u) is distinguishable (and thus reconstructible) from at least one alternative (u').”

This is the cleanest analogue of “entanglement wedge reconstructibility,” but for **a dynamical generator**.

---

## 2) The theorem

### Theorem (Clock–Information–Geometry decomposition for closed micro/meta ratchets)

Consider the bipartite Markov system above, and let it have a stationary distribution (\bar p(x,u)).

#### (A) Clock criterion: oriented clocks exist iff the system is a ratchet

There exists an oriented clock current (Q_t) with nonzero stationary drift (v\neq 0) **if and only if** the joint chain is **nonreversible** (violates detailed balance).

In particular, in the **null regime** (detailed balance), **every** antisymmetric current has zero stationary drift—so no autonomous oriented clock exists.

#### (B) Information processing is bounded by dissipation

Define the **mutual information**
[
I_t := I(X_t;U_t) ;=;\sum_{x,u} p_t(x,u)\log\frac{p_t(x,u)}{p_t(x)p_t(u)}.
]

Define the **information flows** (the parts of (\dot I_t) due to micro-jumps vs meta-jumps):
[
\dot I_t^X := \frac12\sum_{u}\sum_{x,x'} J_t^{u}(x,x');\log\frac{p_t(u|x')}{p_t(u|x)},
]
[
\dot I_t^U := \frac12\sum_{x}\sum_{u,u'} \tilde J_t^{x}(u,u');\log\frac{p_t(x|u')}{p_t(x|u)},
]
so that
[
\dot I_t = \dot I_t^X + \dot I_t^U.
]
(These match the standard definitions of bipartite information flow. )

Define the **subsystem entropy production rates**
[
\sigma_t^X := \frac12\sum_{u}\sum_{x,x'} J_t^{u}(x,x');\log\frac{p_t(x,u),k_u(x,x')}{p_t(x',u),k_u(x',x)};\ge 0,
]
[
\sigma_t^U := \frac12\sum_{x}\sum_{u,u'} \tilde J_t^{x}(u,u');\log\frac{p_t(x,u),r_x(u,u')}{p_t(x,u'),r_x(u',u)};\ge 0,
]
so (\sigma_t = \sigma_t^X+\sigma_t^U\ge 0).

Then each subsystem satisfies a **second law with an information term**:
[
\sigma_t^X = \frac{d}{dt}S(X_t) + \dot S_{r,t}^X - \dot I_t^X ;\ge 0,\qquad
\sigma_t^U = \frac{d}{dt}S(U_t) + \dot S_{r,t}^U - \dot I_t^U ;\ge 0,
]
where (\dot S_{r,t}^X) and (\dot S_{r,t}^U) are the “environmental entropy flows” (log rate-ratio terms).

In a **nonequilibrium steady state** (time derivatives (d/dt) vanish), these reduce to:
[
\dot S_r^X \ge \dot I^X,\qquad \dot S_r^U \ge \dot I^U,
]
so **continuous information processing is bounded by dissipation**.

In the **null equilibrium steady state** (detailed balance), all edge currents vanish, hence (\dot I^X=\dot I^U=0): there is no directed information flow.

#### (C) Dynamic “spacetime” criterion: geometry is time-varying iff top-down + changing meta

Assume **top-down dependence**:
[
\exists u\neq u',\exists x,x' \text{ such that } k_u(x,x') \neq k_{u'}(x,x').
\tag{TD}
]

Let (\Gamma) be any functional of the micro generator (k_u) (for example (\Gamma(u)=\mathsf H_\varepsilon(u)), or (\Gamma(u)) is the conductance metric induced by (k_u)). Define the **instantaneous geometry**
[
\Gamma_t := \Gamma(U_t).
]

Then:

* If top-down dependence fails (i.e. (k_u) is independent of (u)), then (\Gamma_t) is almost surely constant for every such (\Gamma).

* If (TD) holds **and** (U_t) visits at least two meta states (u\neq u') with (\Gamma(u)\neq \Gamma(u')) with positive probability, then (\Gamma_t) is a nontrivial time-varying process.

So, when meta states both **(i)** change and **(ii)** actually parameterize micro rules, the reconstructibility/geometry object is **dynamical** (a “dynamic spacetime scaffold”).

---

## 3) Proofs

### Proof of (A): oriented clocks exist iff nonreversible

Assume the chain is started in stationarity (\bar p).

For any antisymmetric increment (d(z,z')=-d(z',z)), the stationary drift is
[
v ;=;\sum_{z}\sum_{z'\neq z}\bar p(z),w(z\to z'),d(z,z').
]
Define steady edge current (J(z,z')=\bar p(z)w(z\to z')-\bar p(z')w(z'\to z)), which is antisymmetric.

Then, pairing terms ((z,z')) and ((z',z)),
[
v=\frac12\sum_{z\neq z'} J(z,z'),d(z,z').
]
If the chain is reversible (detailed balance), then (\bar p(z)w(z\to z')=\bar p(z')w(z'\to z)) for all edges, hence (J(z,z')=0) for all edges, and therefore (v=0) for every antisymmetric (d). So no oriented clock exists.

Conversely, if the chain is nonreversible, then there exists at least one edge ((z,z')) with (J(z,z')\neq 0) (otherwise all currents would be zero, implying detailed balance). Choose
[
d(\tilde z,\tilde z')=
\begin{cases}
+1,&(\tilde z,\tilde z')=(z,z'),\
-1,&(\tilde z,\tilde z')=(z',z),\
0,&\text{otherwise.}
\end{cases}
]
Then (v=\frac12 J(z,z')\neq 0). Hence an oriented clock exists.

This proves (A).

---

### Proof of (B): subsystem second laws and the dissipation bound

I’ll prove the (X) statement; the (U) statement is identical with roles swapped.

#### Step 1: positivity of (\sigma_t^X)

Write the (X)-edge fluxes
[
j:=j_t^{u}(x\to x')=p_t(x,u)k_u(x,x'),\qquad
j^{\mathrm{rev}}:=j_t^{u}(x'\to x)=p_t(x',u)k_u(x',x).
]
Then for each unordered pair ({x,x'}) at fixed (u),
[
\frac12 (j-j^{\mathrm{rev}})\log\frac{j}{j^{\mathrm{rev}}} ;\ge; 0,
]
because for (a,b>0), ((a-b)\log(a/b)\ge 0) (set (x=a/b), then ((x-1)\log x\ge 0)). Summing over all (u) and all pairs gives (\sigma_t^X\ge 0).

This is exactly the standard nonnegativity of entropy production contributions on each edge set.

#### Step 2: decompose (\sigma_t^X) into entropy change + environment + information

Start from the definition
[
\sigma_t^X=\frac12\sum_{u}\sum_{x,x'} J_t^{u}(x,x')\log\frac{p_t(x,u)k_u(x,x')}{p_t(x',u)k_u(x',x)}.
]
Split the log into two terms:
[
\log\frac{p_t(x,u)k_u(x,x')}{p_t(x',u)k_u(x',x)}
=\log\frac{k_u(x,x')}{k_u(x',x)} + \log\frac{p_t(x,u)}{p_t(x',u)}.
]
Define the **environmental entropy flow** for the (X)-moves by
[
\dot S_{r,t}^X := \frac12\sum_{u}\sum_{x,x'} J_t^{u}(x,x')\log\frac{k_u(x,x')}{k_u(x',x)}.
]
(That is the usual “log rate-ratio” term; under local detailed balance it is the heat/T. )

So
[
\sigma_t^X=\dot S_{r,t}^X + \frac12\sum_{u}\sum_{x,x'} J_t^{u}(x,x')\log\frac{p_t(x,u)}{p_t(x',u)}.
]

Now factor the joint into marginal and conditional:
[
p_t(x,u)=p_t(x),p_t(u|x).
]
Therefore
[
\log\frac{p_t(x,u)}{p_t(x',u)}
==============================

\log\frac{p_t(x)}{p_t(x')}
+
\log\frac{p_t(u|x)}{p_t(u|x')}.
]
Rewriting the last term,
[
\log\frac{p_t(u|x)}{p_t(u|x')}
= -\log\frac{p_t(u|x')}{p_t(u|x)}.
]

Thus
[
\frac12\sum_{u,x,x'} J_t^{u}(x,x')\log\frac{p_t(x,u)}{p_t(x',u)}
================================================================

\frac12\sum_{u,x,x'} J_t^{u}(x,x')\log\frac{p_t(x)}{p_t(x')}
;-;
\frac12\sum_{u,x,x'} J_t^{u}(x,x')\log\frac{p_t(u|x')}{p_t(u|x)}.
]

Recognize:

* The first term equals (\frac{d}{dt}S(X_t)). This is a standard identity for Markov chains: the Shannon entropy rate can be written in current form. (You can verify by differentiating (S(X_t)=-\sum_x p_t(x)\log p_t(x)) and using that only (X)-moves change (p_t(x)).)

* The second term is exactly (\dot I_t^X) as defined above.

Therefore,
[
\sigma_t^X = \frac{d}{dt}S(X_t) + \dot S_{r,t}^X - \dot I_t^X.
]
We already proved (\sigma_t^X\ge 0). That completes the subsystem second law.

This decomposition (and the corresponding (U) one) is the central result of Horowitz–Esposito’s “continuous information flow” framework.

#### Step 3: steady-state dissipation bound for information flow

In a nonequilibrium steady state, the distribution is time-independent, so (dS(X_t)/dt=0) and (dI_t/dt=0), but (\dot I^X) and (\dot I^U=-\dot I^X) can be nonzero (continuous information exchange). Horowitz–Esposito show the steady-state form explicitly:
[
\dot S_r^X - \dot I \ge 0,\qquad \dot S_r^U + \dot I \ge 0,
]
with (\dot I=\dot I^X=-\dot I^U).

So if (\dot I>0) (micro-jumps are *creating* information), the environment entropy flow (dissipation) must satisfy (\dot S_r^X\ge \dot I). This is the precise “information processing is bounded by dissipation” statement.

That proves (B).

---

### Proof of (C): dynamic geometry iff top-down + changing meta

Let (\Gamma:\mathcal U\to\mathcal G) be any function from meta states to “geometry objects” (graphs, metrics, hypergraphs). Define (\Gamma_t=\Gamma(U_t)).

* If the micro kernel does not depend on (u), then “geometry from the micro generator” cannot depend on (u); formally, for any (\Gamma) that factors through (k_u), (\Gamma(u)) is the same for all (u). Hence (\Gamma_t) is constant almost surely.

* If the micro kernel depends on (u) and (\Gamma) is chosen so that (\Gamma(u)\neq\Gamma(u')) for some (u\neq u'), and (U_t) visits both values with positive probability, then (\Gamma_t) takes at least two different values with positive probability, hence it is not almost surely constant.

Now instantiate (\Gamma(u)=\mathsf H_\varepsilon(u)) with the reconstructibility hypergraph defined above.

If (k_u) is independent of (u), then (P_{u,A}^\Delta=P_{u',A}^\Delta) for all (A), hence (D_A(u,u')=0) for all (u\neq u'), so (\mathsf H_\varepsilon(u)=\varnothing) for all (u) and all (\varepsilon>0).

If (k_u) differs for some (u\neq u'), then for (A=\Lambda) (the full region), (P_{u,\Lambda}^\Delta\neq P_{u',\Lambda}^\Delta) for small enough (\Delta), hence (D_\Lambda(u,u')>0) and (\mathsf H_\varepsilon(u)) is nonempty for some (\varepsilon>0). Therefore (\mathsf H_\varepsilon(U_t)) changes whenever (U_t) changes among meta states with different hypergraphs.

That proves (C).

---

## 4) What this theorem says in the language you’re using

* A **clock** (in the strict ratchet sense: a directional phase current) is equivalent to **nonreversibility** of the closed dynamics: if the combined system is detailed-balance, no such clock exists.

* An **encoding scaffold** (in the strict “ongoing correction/measurement/feedback” sense) corresponds to **directed information flow** (\dot I), and in autonomous steady operation it requires dissipation at least (\dot I) (environment entropy flow).

* A **dynamic spacetime analogue** is just the fact that the **generator itself is state-dependent**:
  [
  \text{micro rules at time }t \text{ are } k_{U_t}.
  ]
  Therefore any “geometry” derived from those rules (metrics, adjacency, reconstructibility wedges) becomes a random time-varying object (\Gamma(U_t)).

This is exactly the mathematical sense in which your ratchet system can behave like “life constructing a clock and structure”: the “structure” is not just a pattern in (X); it is a **pattern in the rules** (the meta state (U)) that is itself dynamically written.

---

## 5) Extensions that match your repo model

### Multiple meta layers (multipartite)

For (L) layers (Z^{(0)},\dots,Z^{(L)}) with asynchronous “one-layer-per-jump” updates, the same decomposition generalizes to a **network of information flows** and **partial entropy productions** (you get one inequality per layer). This is exactly the direction developed in “information thermodynamics on causal networks” frameworks.

### Discrete-time (Metropolis steps)

Replace rates (k(\cdot)) by transition probabilities (P(\cdot)), fluxes by one-step flows (p_t(z)P(z\to z')), and the same algebra goes through (the log-ratio EP remains a sum of ((a-b)\log(a/b))-type nonnegative terms).

### Connecting to QECC / “static vs dynamic”

In QECC-as-geometry, the encoding map (or tensor network) is typically treated as a **fixed** scaffold, and “geometry” is read off from reconstructibility relations. In the ratchet picture above, that scaffold is an **evolving state variable** (U_t), so the reconstructibility hypergraph (\mathsf H_\varepsilon(U_t)) is intrinsically **time-dependent**.

That’s the precise “dynamic vs static” distinction: not “quantum is static,” but “the encoding scaffold is kinematic in the QEC story, whereas it is dynamical state in the ratchet story.”
</file>

<file path="docs/16_meta_meta.md">
Below is a “pure” operator/topology‑lifting design in the precise sense you’ve been insisting on:

* **No semantic feature detection** (no “defect extraction,” no engineered interpretation).
* **No extra directionality** (null remains genuinely null).
* **No extra “cost function on top”** meant to bias outcomes (no distance penalties, no regularizers added just to “make it behave”).
* **Economy/weakness implemented the same way you already did it**: by bounded carriers + saturation + budget competition, not by an external optimizer.

The only unavoidable “bias” is the same one your original experiment already had: a **finite, symmetric representational envelope** (finite grid/layers, finite bits/tokens, finite candidate relations). Everything else is moved into substrates acted on by P1–P6.

---

# 0) Design priorities as explicit axioms

Let the full state be (Z=(X,\Theta)), where:

* (X): all existing base + meta layer state (particles, fields, edge weights, counters, etc.).
* (\Theta): *operator/topology* degrees of freedom we’re lifting.

We impose these axioms:

### A0. No interpretation bias

No new state variable is allowed to be a **deterministic function** of other state used for dynamics (no “computed boundary map,” no clustering detector, no pattern recognizer).
All (\Theta) are **free carriers** updated by primitive moves.

### A1. No extra directionality

When (p3On=0) and (p6On=0), the full chain on (Z) must satisfy detailed balance (reversibility), and ( \text{epExact} \to 0 ) windowed.

### A2. Economy/weakness only by bounded carriers + budget exchange

We do **not** add “priors” like (\lambda \sum |r|^2 K_r) or “distance penalties” for long edges.
Instead, we implement scarcity by:

* bounded integer carriers,
* saturation,
* and **conserved budgets** (microcanonical economy), updated by symmetric exchange moves.

This is not “adding a cost”; it’s the same irreducible fact as “finite grid and finite bits.”

### A3. Symmetry maximal

The envelope and proposals must be invariant under a chosen symmetry group (G) (translations, rotations/reflections of the lattice, and any layer permutations you deem legitimate). Formally, in null:
[
P(gz\to gz') = P(z\to z') \quad \forall g\in G.
]

### A4. P1–P6 act on everything uniformly

Operator/topology carriers are just more substrate. They are updated by the same event scheduler and the same primitive toggles.

---

# 1) Minimal unavoidable envelope

You cannot avoid *some* envelope in a finite machine (same as your original experiment):

* finite lattice (\Lambda) (e.g., (g\times g)),
* finite number of layers (L),
* finite candidate relation sets (local stencils, candidate edges),
* bounded integer ranges.

**“Pure” here means:** the envelope is chosen *symmetry-first* and *capacity-first*, not outcome-first.

Two envelope options that stay no-bias:

### Envelope E1: purely local primitives

All “couplings” live on nearest-neighbor lattice edges.
Effective long-range influence emerges only via **paths** (many local edges), not direct long edges.

This is the cleanest “physics-like” choice: locality is fundamental, long-range is constructed.

### Envelope E2: local + sparse nonlocal candidates

Allow a bounded set (\mathcal R) of offsets up to some (R_{\max}), but **balanced** so there is no combinatorial bias:

* choose (\mathcal R) as a fixed set closed under (r\mapsto -r) and rotations,
* and (if possible) equal counts per radius bin.

This gives the system explicit choice of locality scale without “distance penalties.”

---

# 2) What gets lifted: operator and topology substrates

## 2.1 Cross-layer operator kernels (K) as a budgeted simplex field

For each interface (\ell-1 \to \ell) and each cell (q\in\Lambda), introduce a kernel:
[
K^{(\ell)}(q,\cdot) \in \mathbb{N}^{|\mathcal R|}, \quad 0\le K^{(\ell)}(q,r)\le B_K
]
with a **strict budget constraint**
[
\sum_{r\in\mathcal R} K^{(\ell)}(q,r) = B_K.
]

Define normalized coefficients:
[
k^{(\ell)}(q,r) := \frac{K^{(\ell)}(q,r)}{B_K}.
]
So each cell’s operator is a convex combination over offsets.

### Operator action (feature-agnostic)

For any lower-layer field (U^{(\ell-1)}) (could be (S), (A), etc.), define:
[
(\mathcal K^{(\ell)} U^{(\ell-1)})(q) = \sum_{r\in\mathcal R} k^{(\ell)}(q,r), U^{(\ell-1)}(q+r).
]

This is just “a local linear operator.” No semantics (no edges/boundaries) are privileged.

---

## 2.2 Within-layer topology (\omega) as budgeted edge mass

Pick a candidate edge set (\mathcal E) inside the envelope (nearest neighbor edges for E1; or a sparse candidate set for E2).

For each layer (\ell) and edge (e\in\mathcal E), define integer edge mass:
[
\omega^{(\ell)}*e \in {0,1,\dots,B*\omega}
]
with a conserved global (or per-node) budget:
[
\sum_{e\in\mathcal E} \omega^{(\ell)}*e = B*\Omega \quad \text{(global)}
]
or
[
\sum_{e\ni v} \omega^{(\ell)}*e = B*{\deg} \quad \forall v \quad \text{(per-node)}.
]

**Interpretation:** (\omega) is “how much coupling capacity is allocated to each relation.”

Again: no distance penalty. Scarcity comes from finite budget.

---

## 2.3 Optional: coupling-strength allocation (\alpha) as budget, not a real-valued knob

If you want “how strongly layers couple” to be selected too, do it the same way:

Let (\alpha^{(\ell)}(q)\in{0,\dots,B_\alpha}) with (\sum_q \alpha^{(\ell)}(q)=B_{\alpha,\ell}).
Then any inter-layer coupling term uses (\alpha^{(\ell)}(q)) as a multiplier. No new penalty on (\alpha) itself.

---

# 3) How (\Theta) influences the existing primitive mechanics

This is where we must be very careful to avoid sneaking in a “goal.”

The right rule is:

> (\Theta) may only enter **transition kernels** through the same kinds of reversible energy terms and/or P6 work terms you already use (η conservative coupling, etaDrive drive-only maintenance).
> No additional evaluation functional is introduced “because we want it.”

So we define coupling only in the following generic form:

## 3.1 Conservative cross-layer coupling (optional)

A generic quadratic mismatch energy:
[
E_{\text{couple}}(X,\Theta)
= \frac{\eta}{2}\sum_{\ell=1}^{L}\sum_{q\in\Lambda}
\left(
\mathrm{norm}(U^{(\ell)}(q)) -
\mathrm{norm}((\mathcal K^{(\ell)}U^{(\ell-1)})(q))
\right)^2.
]

* If you want **zero selection in null**, set (\eta=0).
* If you accept equilibrium “selection” (still reversible), you can keep (\eta>0).

This is not a “locality penalty.” It’s a physical interaction term between layers.

## 3.2 Drive-only coupling (ratchet-maintained scaffold)

Exactly like your `etaDrive` idea, but now with (K) and/or (\omega) shaping what “alignment” means.

For an accepted move (z\to z'), define a work contribution:
[
W_{\text{align}}(z\to z') = -\eta_{\text{drive}};\Delta E_{\text{mismatch}}(z\to z'),
]
where (E_{\text{mismatch}}) is the same mismatch functional used above (or a chosen channel’s mismatch).

Key property:
[
W_{\text{align}}(z'\to z) = -W_{\text{align}}(z\to z')
]
so it’s a legitimate P6-style antisymmetric work term.

With (p6On=0), (W_{\text{align}}=0) and nothing is “actively maintained.”

This is your proven “dynamic code maintenance costs EP” mechanism—now parameterized by operator/topology substrates rather than hard-coded pointwise alignment.

---

# 4) How P1–P6 update (\Theta) without a new mechanism

Everything in (\Theta) must be updated by **the same kind of local, reversible, bounded moves** you already use.

## 4.1 Budget exchange move as the universal “pure economy” update

### Kernel exchange

Pick ((\ell,q)) and two offsets (r_1\neq r_2). If (K^{(\ell)}(q,r_1)>0), propose:
[
K^{(\ell)}(q,r_1)\mapsto K^{(\ell)}(q,r_1)-1,\quad
K^{(\ell)}(q,r_2)\mapsto K^{(\ell)}(q,r_2)+1.
]
All other components unchanged.

This preserves (\sum_r K=B_K) exactly.

### Topology exchange

Pick ((\ell)) and two edges (e_1\neq e_2). If (\omega^{(\ell)}*{e_1}>0), propose:
[
\omega^{(\ell)}*{e_1}\mapsto \omega^{(\ell)}*{e_1}-1,\quad
\omega^{(\ell)}*{e_2}\mapsto \omega^{(\ell)}_{e_2}+1.
]

### Which primitive is this?

Implementation-wise you can map these exchanges onto your existing primitive families:

* Treat (K) and (\omega) as “edge-like” carriers → updated when **P1** is enabled.
* Or treat them as “field-like” carriers → updated when **P5** is enabled.

The key is not the label, but that:

* proposals are symmetric,
* budgets are conserved,
* acceptance uses the same Metropolis / NE-Metropolis structure.

## 4.2 Protocol and regime switching

* **P3** can order “update (K) then update (U)” vs “update (U) then update (K)” noncommutatively, producing pumped operator-space currents (still no extra goal).
* **P4** can toggle discrete mode fields if you choose to have a small operator basis index; but the most no-bias version is to avoid discrete hand-picked bases and keep the full simplex (K).

## 4.3 Closure/viability (P5) without monotonic writes

Any “operator stability” must arise from reversible dynamics + (optional) drive bias, never from “once selected, don’t go back.”

Budget exchange moves are reversible; stability emerges only if the combined dynamics makes some regions of (\Theta)-space long-lived.

---

# 5) Null-regime guarantee on the extended system

Let (Z=(X,\Theta)) evolve with proposals (q(z\to z')) and acceptance (a(z\to z')).

### Null regime

Set (p3On=0), (p6On=0). Use Metropolis acceptance:
[
a(z\to z')=\min{1,\exp(-\beta\Delta E)}.
]

If:

1. every proposal move has a well-defined reverse,
2. proposal probabilities are symmetric: (q(z\to z')=q(z'\to z)),
3. (\Delta E) is computed from a single scalar energy (E(X,\Theta)),

then detailed balance holds with:
[
\pi(z)\propto e^{-\beta E(z)}
]
restricted to the budget-constrained manifold (kernel simplex, edge budgets, etc.).

So:

* **no currents in (\Theta)**,
* **no currents in (X)** beyond equilibrium fluctuations,
* **epExact window rate → 0**.

This is your “no extra directionality” promise, now extended to operator/topology selection.

---

# 6) Why budgets are the “pure” alternative to extra penalties

This is the critical point relative to your objection.

If you add (M) new binary couplings with no constraint, then in null the typical number of active couplings is (M/2) purely by combinatorics (not ratchets). That’s a trivial entropy artifact.

A conserved budget (B) avoids that without a penalty function:

* In any state with (\sum_e \omega_e=B), the number of nonzero edges is at most (B).
  So density cannot blow up just because there are many possibilities.

So budgets are not “engineering outcomes”; they are the minimal way to keep “latent space is huge” from collapsing into “everything is on.”

And importantly:

* budgets don’t privilege *which* couplings win,
* they only ensure scarcity exists—exactly what “economy” means.

---

# 7) What “selection of latent transformations” means in this design

Because null has no currents, “selection” can only mean:

* Under drive (P6 and/or P3), (\Theta) develops **nonzero stationary currents** or biased drifts in some summary statistic (effective operator shape, effective coupling topology).
* Those currents are measurable and dissipation-priced (via epExact buckets on (\Theta)-moves).

Operationally, define any summary statistic (F(\Theta)) (e.g., entropy of kernels, sparsity of (\omega), spectral gap of the induced coupling graph). Then “latent transformation selection” corresponds to:
[
\lim_{t\to\infty}\frac{\mathbb E[F(\Theta_t)-F(\Theta_0)]}{t}\neq 0
]
or persistent cycling in (\Theta)-space.

And because (\Theta)-moves are part of the same Markov chain, any such directed drift must show up in epExact (your dissipation ledger).

---

# 8) Checklist for “no-bias compliance”

If you implement this, you can certify you didn’t sneak in semantics by running these invariance/ablation tests:

1. **Null EP test**: with P3=0,P6=0, epExactRateWindowLast ≈ 0 even with operator/topology carriers enabled.
2. **Symmetry test**: rotate/translate the initial condition and RNG seed mapping → distribution of outcomes unchanged (up to symmetry).
3. **Offset-label permutation test**: randomly permute the labels of offsets (r\in\mathcal R) everywhere (consistently) → null statistics unchanged.
4. **Budget ablation**: reduce budgets (B_K,B_\Omega) → phenomena degrade gracefully rather than explode combinatorially.
5. **Drive ablation**: turn P6 off → any directed drift in (\Theta) disappears.

---

# 9) Minimal formal spec you can drop into the repo docs

If you want this in one paragraph:

> We extend the simulator state from (X) to (Z=(X,\Theta)) where (\Theta) contains (i) cross-layer local linear operators (K^{(\ell)}(q,\cdot)) represented as integer token distributions with (\sum_r K=B_K), and (ii) within-layer topology masses (\omega^{(\ell)}*e) on a symmetric candidate edge set with (\sum_e\omega=B*\Omega). (\Theta) is updated only by symmetric, budget-conserving exchange moves (token transfers), scheduled under the same primitive toggles as (X). (\Theta) enters dynamics only through the same reversible energy terms and/or P6 antisymmetric work terms already used for coupling/maintenance (no additional priors or penalties). Therefore, in null (P3=0,P6=0) the joint chain satisfies detailed balance on the budget manifold and epExact→0, while under drive (P6/P3) any persistent drift/cycling in (\Theta) constitutes dissipation-priced selection of latent transformations.

---

If you want to go one notch “purer” still, pick **Envelope E1** (nearest-neighbor only) and let “effective nonlocality” emerge only via *paths* and via *stack depth*. Then you literally never introduce explicit long-range couplings at all—locality is fundamental, and what’s selected is conductivity/topology of local edges and operator composition across layers. That’s the closest analogue to “nature doesn’t have arbitrary long-range couplings; it builds them.”
</file>

<file path="docs/README.md">
# Ratchet Playground — Local Reference Docs

This folder is intended to live inside the **ratchet-playground** app repository as a self-contained reference for:
- the core *ratchet primitives* **P₁–P₆**,
- the **A‑Life constraint** (no baked-in directionality),
- the **central mathematical device** (fast–slow state, reversible null regime, non‑equilibrium sources),
- Deliverables **A–D** (stationary measure, transition tables, cycle-affinity wiring, on-screen diagnostics),
- and a compact but rigorous write-up of **Weakness × Economy** (the canonical Constraint×Constraint scaffold).

## Navigation

1. **Global context**
   - `00_glossary.md`
   - `01_ratchet_theory_overview.md`
   - `02_primitives_P1_P6.md`
   - `03_atoms_MFQX.md` (optional “atoms” vocabulary used in earlier theory building)

2. **Canonical math scaffold**
   - `06_alife_principles_and_null_regime.md`
   - `07_central_device_reversible_channels.md`

3. **Weakness × Economy (Constraint×Constraint)**
   - `04_weakness_x_economy.md`
   - `05_constraint_x_constraint_ratchets.md`

4. **Ratchet Playground deliverables**
   - `08_deliverable_A_null_stationary_measure.md`
   - `09_deliverable_B_transition_tables.md`
   - `10_deliverable_C_cycle_affinity_wiring.md`
   - `11_deliverable_D_diagnostics.md`

5. **Implementation notes**
   - `12_browser_impl_notes.md`
   - `13_open_questions_and_extensions.md`
   - `14_ratchet_motifs_and_asymmetries.md`

## Guiding principle for the playground

The playground is designed so that:
- **No global objective** is optimized.
- **No replication/heredity** is assumed at the base layer.
- **No “always-increasing write” rules** are allowed by default.
- Any arrow-of-time / “force” that appears must arise from:
  - **non-equilibrium driving** implemented *only through primitives that truly break reversibility* (typically **P₃** and/or **P₆**), and/or
  - emergent interaction effects among enabled primitives.

The null regime (P₃=OFF, P₆=OFF) is required to be **reversible** (detailed balance), and the documents show how to implement that rigorously.

---
</file>

<file path="scripts/params/clock_code/clock_null.json">
{
  "beta": 1.0,
  "stepSize": 0.01,
  "p3On": 0,
  "p6On": 0,
  "pWrite": 0,
  "pNWrite": 1,
  "pAWrite": 0,
  "pSWrite": 0,
  "muHigh": 1.0,
  "muLow": 1.0,
  "kappaRep": 1.0,
  "r0": 0.25,
  "kappaBond": 0.0,
  "rStar": 0.22,
  "lambdaW": 0.0,
  "lW": 4,
  "lambdaN": 0.0,
  "lN": 6,
  "lambdaA": 0.0,
  "lA": 6,
  "lambdaS": 0.0,
  "lS": 6,
  "gridSize": 12,
  "rPropose": 0.12,
  "metaLayers": 0,
  "eta": 0.0,
  "etaDrive": 0.0,
  "clockOn": 1,
  "clockK": 8,
  "clockFrac": 0.01,
  "clockUsesP6": 1,
  "repairClockGated": 0
}
</file>

<file path="scripts/params/clock_code/clock_p6.json">
{
  "beta": 1.0,
  "stepSize": 0.01,
  "p3On": 0,
  "p6On": 1,
  "pWrite": 0,
  "pNWrite": 1,
  "pAWrite": 0,
  "pSWrite": 0,
  "muHigh": 1.0,
  "muLow": 1.0,
  "kappaRep": 1.0,
  "r0": 0.25,
  "kappaBond": 0.0,
  "rStar": 0.22,
  "lambdaW": 0.0,
  "lW": 4,
  "lambdaN": 0.0,
  "lN": 6,
  "lambdaA": 0.0,
  "lA": 6,
  "lambdaS": 0.0,
  "lS": 6,
  "gridSize": 12,
  "rPropose": 0.12,
  "metaLayers": 0,
  "eta": 0.0,
  "etaDrive": 0.0,
  "clockOn": 1,
  "clockK": 8,
  "clockFrac": 0.01,
  "clockUsesP6": 1,
  "repairClockGated": 0
}
</file>

<file path="scripts/params/clock_code/clock_tur_sweep_base.json">
{
  "beta": 1.0,
  "stepSize": 0.01,
  "p3On": 0,
  "p6On": 1,
  "pWrite": 0,
  "pNWrite": 1,
  "pAWrite": 0,
  "pSWrite": 0,
  "muHigh": 1.0,
  "muLow": 1.0,
  "kappaRep": 1.0,
  "r0": 0.25,
  "kappaBond": 0.0,
  "rStar": 0.22,
  "lambdaW": 0.0,
  "lW": 4,
  "lambdaN": 0.0,
  "lN": 6,
  "lambdaA": 0.0,
  "lA": 6,
  "lambdaS": 0.0,
  "lS": 6,
  "gridSize": 12,
  "rPropose": 0.12,
  "metaLayers": 0,
  "eta": 0.0,
  "etaDrive": 0.0,
  "clockOn": 1,
  "clockK": 8,
  "clockFrac": 0.02,
  "clockUsesP6": 1,
  "repairClockGated": 0
}
</file>

<file path="scripts/params/clock_code/code_deadline_gated_clock.json">
{
  "beta": 2.0,
  "stepSize": 0.01,
  "p3On": 0,
  "p6On": 1,
  "p6SFactor": 0.0,
  "pWrite": 0,
  "pNWrite": 0.002,
  "pAWrite": 0,
  "pSWrite": 0.998,
  "muHigh": 2.0,
  "muLow": 2.0,
  "kappaRep": 1.0,
  "r0": 0.25,
  "kappaBond": 0.0,
  "rStar": 0.22,
  "lambdaW": 0.0,
  "lW": 4,
  "lambdaN": 0.0,
  "lN": 6,
  "lambdaA": 0.0,
  "lA": 6,
  "lambdaS": 0.0,
  "lS": 20,
  "gridSize": 24,
  "rPropose": 0.12,
  "metaLayers": 2,
  "eta": 0.0,
  "etaDrive": 1.0,
  "codeNoiseRate": 0.12,
  "codeNoiseBatch": 4,
  "codeNoiseLayer": 0,
  "clockOn": 1,
  "clockK": 24,
  "clockFrac": 0.2,
  "clockUsesP6": 1,
  "repairClockGated": 1,
  "repairGateMode": 1,
  "repairGateSpan": 4
}
</file>

<file path="scripts/params/clock_code/code_deadline_gated_random.json">
{
  "beta": 2.0,
  "stepSize": 0.01,
  "p3On": 0,
  "p6On": 1,
  "p6SFactor": 0.0,
  "pWrite": 0,
  "pNWrite": 0.002,
  "pAWrite": 0,
  "pSWrite": 0.998,
  "muHigh": 2.0,
  "muLow": 2.0,
  "kappaRep": 1.0,
  "r0": 0.25,
  "kappaBond": 0.0,
  "rStar": 0.22,
  "lambdaW": 0.0,
  "lW": 4,
  "lambdaN": 0.0,
  "lN": 6,
  "lambdaA": 0.0,
  "lA": 6,
  "lambdaS": 0.0,
  "lS": 20,
  "gridSize": 24,
  "rPropose": 0.12,
  "metaLayers": 2,
  "eta": 0.0,
  "etaDrive": 1.0,
  "codeNoiseRate": 0.12,
  "codeNoiseBatch": 4,
  "codeNoiseLayer": 0,
  "clockOn": 1,
  "clockK": 24,
  "clockFrac": 0.2,
  "clockUsesP6": 0,
  "repairClockGated": 1,
  "repairGateMode": 1,
  "repairGateSpan": 4
}
</file>

<file path="scripts/params/clock_code/code_deadline_gated_static.json">
{
  "beta": 2.0,
  "stepSize": 0.01,
  "p3On": 0,
  "p6On": 1,
  "p6SFactor": 0.0,
  "pWrite": 0,
  "pNWrite": 0.002,
  "pAWrite": 0,
  "pSWrite": 0.998,
  "muHigh": 2.0,
  "muLow": 2.0,
  "kappaRep": 1.0,
  "r0": 0.25,
  "kappaBond": 0.0,
  "rStar": 0.22,
  "lambdaW": 0.0,
  "lW": 4,
  "lambdaN": 0.0,
  "lN": 6,
  "lambdaA": 0.0,
  "lA": 6,
  "lambdaS": 0.0,
  "lS": 20,
  "gridSize": 24,
  "rPropose": 0.12,
  "metaLayers": 2,
  "eta": 0.0,
  "etaDrive": 1.0,
  "codeNoiseRate": 0.12,
  "codeNoiseBatch": 4,
  "codeNoiseLayer": 0,
  "clockOn": 0,
  "clockK": 24,
  "clockFrac": 0.2,
  "clockUsesP6": 1,
  "repairClockGated": 1,
  "repairGateMode": 1,
  "repairGateSpan": 4
}
</file>

<file path="scripts/params/clock_code/deadline_fidelity_drift.json">
{
  "beta": 2,
  "stepSize": 0.01,
  "p3On": 0,
  "p6On": 1,
  "p6SFactor": 0,
  "pWrite": 0,
  "pNWrite": 0.05,
  "pAWrite": 0,
  "pSWrite": 1,
  "muHigh": 1.8,
  "muLow": 1.8,
  "kappaRep": 1,
  "r0": 0.25,
  "kappaBond": 0,
  "rStar": 0.22,
  "lambdaW": 0,
  "lW": 4,
  "lambdaN": 0,
  "lN": 6,
  "lambdaA": 0,
  "lA": 6,
  "lambdaS": 0,
  "lS": 1,
  "gridSize": 64,
  "rPropose": 0.12,
  "metaLayers": 2,
  "eta": 0,
  "etaDrive": 0.4,
  "codeNoiseRate": 0.012,
  "codeNoiseBatch": 1,
  "codeNoiseLayer": 0,
  "clockOn": 1,
  "clockK": 64,
  "clockFrac": 1,
  "clockUsesP6": 1,
  "repairClockGated": 1,
  "repairGateMode": 1,
  "repairGateSpan": 3
}
</file>

<file path="scripts/params/clock_code/deadline_fidelity_found.json">
{
  "params": {
    "beta": 2,
    "stepSize": 0.01,
    "p3On": 0,
    "p6On": 1,
    "p6SFactor": 0,
    "pWrite": 0,
    "pNWrite": 0.05,
    "pAWrite": 0,
    "pSWrite": 1,
    "muHigh": 1.8,
    "muLow": 1.8,
    "kappaRep": 1,
    "r0": 0.25,
    "kappaBond": 0,
    "rStar": 0.22,
    "lambdaW": 0,
    "lW": 4,
    "lambdaN": 0,
    "lN": 6,
    "lambdaA": 0,
    "lA": 6,
    "lambdaS": 0,
    "lS": 1,
    "gridSize": 64,
    "rPropose": 0.12,
    "metaLayers": 2,
    "eta": 0,
    "etaDrive": 0.4,
    "codeNoiseRate": 0.012,
    "codeNoiseBatch": 1,
    "codeNoiseLayer": 0,
    "clockOn": 1,
    "clockK": 64,
    "clockFrac": 1,
    "clockUsesP6": 1,
    "repairClockGated": 1,
    "repairGateMode": 1,
    "repairGateSpan": 3
  },
  "deadline": 38400,
  "notes": "Best available candidate; strict fidelity criteria not met"
}
</file>

<file path="scripts/params/clock_code/deadline_fidelity_random.json">
{
  "beta": 2,
  "stepSize": 0.01,
  "p3On": 0,
  "p6On": 1,
  "p6SFactor": 0,
  "pWrite": 0,
  "pNWrite": 0.05,
  "pAWrite": 0,
  "pSWrite": 1,
  "muHigh": 1.8,
  "muLow": 1.8,
  "kappaRep": 1,
  "r0": 0.25,
  "kappaBond": 0,
  "rStar": 0.22,
  "lambdaW": 0,
  "lW": 4,
  "lambdaN": 0,
  "lN": 6,
  "lambdaA": 0,
  "lA": 6,
  "lambdaS": 0,
  "lS": 1,
  "gridSize": 64,
  "rPropose": 0.12,
  "metaLayers": 2,
  "eta": 0,
  "etaDrive": 0.4,
  "codeNoiseRate": 0.012,
  "codeNoiseBatch": 1,
  "codeNoiseLayer": 0,
  "clockOn": 1,
  "clockK": 64,
  "clockFrac": 1,
  "clockUsesP6": 0,
  "repairClockGated": 1,
  "repairGateMode": 1,
  "repairGateSpan": 3
}
</file>

<file path="scripts/params/clock_code/deadline_fidelity_static.json">
{
  "beta": 2,
  "stepSize": 0.01,
  "p3On": 0,
  "p6On": 1,
  "p6SFactor": 0,
  "pWrite": 0,
  "pNWrite": 0.05,
  "pAWrite": 0,
  "pSWrite": 1,
  "muHigh": 1.8,
  "muLow": 1.8,
  "kappaRep": 1,
  "r0": 0.25,
  "kappaBond": 0,
  "rStar": 0.22,
  "lambdaW": 0,
  "lW": 4,
  "lambdaN": 0,
  "lN": 6,
  "lambdaA": 0,
  "lA": 6,
  "lambdaS": 0,
  "lS": 1,
  "gridSize": 64,
  "rPropose": 0.12,
  "metaLayers": 2,
  "eta": 0,
  "etaDrive": 0.4,
  "codeNoiseRate": 0.012,
  "codeNoiseBatch": 1,
  "codeNoiseLayer": 0,
  "clockOn": 0,
  "clockK": 64,
  "clockFrac": 1,
  "clockUsesP6": 1,
  "repairClockGated": 1,
  "repairGateMode": 1,
  "repairGateSpan": 3
}
</file>

<file path="scripts/params/meta/meta2_null_coupled.json">
{
  "beta": 1.0,
  "stepSize": 0.01,
  "p3On": 0,
  "p6On": 0,
  "pWrite": 0.05,
  "pNWrite": 0.05,
  "pAWrite": 0.05,
  "pSWrite": 0.05,
  "muHigh": 0.6,
  "muLow": -0.6,
  "kappaRep": 500.0,
  "r0": 0.25,
  "kappaBond": 1.2,
  "rStar": 0.22,
  "lambdaW": 0.5,
  "lW": 4,
  "lambdaN": 0.5,
  "lN": 6,
  "lambdaA": 0.5,
  "lA": 6,
  "lambdaS": 0.5,
  "lS": 6,
  "gridSize": 16,
  "rPropose": 0.05,
  "metaLayers": 2,
  "eta": 0.6
}
</file>

<file path="scripts/params/meta/meta2_null_decoupled.json">
{
  "beta": 1.0,
  "stepSize": 0.01,
  "p3On": 0,
  "p6On": 0,
  "pWrite": 0.05,
  "pNWrite": 0.05,
  "pAWrite": 0.05,
  "pSWrite": 0.05,
  "muHigh": 0.6,
  "muLow": -0.6,
  "kappaRep": 500.0,
  "r0": 0.25,
  "kappaBond": 1.2,
  "rStar": 0.22,
  "lambdaW": 0.5,
  "lW": 4,
  "lambdaN": 0.5,
  "lN": 6,
  "lambdaA": 0.5,
  "lA": 6,
  "lambdaS": 0.5,
  "lS": 6,
  "gridSize": 16,
  "rPropose": 0.05,
  "metaLayers": 2,
  "eta": 0.0
}
</file>

<file path="scripts/params/meta/meta2_p3_pump_coupled.json">
{
  "beta": 1.0,
  "stepSize": 0.02,
  "p3On": 1,
  "p6On": 0,
  "pWrite": 0.05,
  "pNWrite": 0.0,
  "pAWrite": 0.0,
  "pSWrite": 0.05,
  "muHigh": 0.6,
  "muLow": -0.6,
  "kappaRep": 500.0,
  "r0": 0.25,
  "kappaBond": 1.2,
  "rStar": 0.22,
  "lambdaW": 0.3,
  "lW": 4,
  "lambdaN": 0.5,
  "lN": 6,
  "lambdaA": 0.5,
  "lA": 6,
  "lambdaS": 0.3,
  "lS": 6,
  "gridSize": 16,
  "rPropose": 0.08,
  "metaLayers": 2,
  "eta": 0.6
}
</file>

<file path="scripts/params/meta/meta2_p3p6_combo_coupled.json">
{
  "beta": 1.0,
  "stepSize": 0.02,
  "p3On": 1,
  "p6On": 1,
  "pWrite": 0.05,
  "pNWrite": 0.0,
  "pAWrite": 0.0,
  "pSWrite": 0.05,
  "muHigh": 1.0,
  "muLow": -1.0,
  "kappaRep": 500.0,
  "r0": 0.25,
  "kappaBond": 1.2,
  "rStar": 0.22,
  "lambdaW": 0.5,
  "lW": 4,
  "lambdaN": 0.5,
  "lN": 6,
  "lambdaA": 0.5,
  "lA": 6,
  "lambdaS": 0.5,
  "lS": 6,
  "gridSize": 16,
  "rPropose": 0.05,
  "metaLayers": 2,
  "eta": 0.6
}
</file>

<file path="scripts/params/meta/meta2_p6_drive_coupled.json">
{
  "beta": 1.0,
  "stepSize": 0.02,
  "p3On": 0,
  "p6On": 1,
  "pWrite": 0.05,
  "pNWrite": 0.05,
  "pAWrite": 0.05,
  "pSWrite": 0.05,
  "muHigh": 1.0,
  "muLow": -1.0,
  "kappaRep": 500.0,
  "r0": 0.25,
  "kappaBond": 1.2,
  "rStar": 0.22,
  "lambdaW": 0.5,
  "lW": 4,
  "lambdaN": 0.5,
  "lN": 6,
  "lambdaA": 0.5,
  "lA": 6,
  "lambdaS": 0.5,
  "lS": 6,
  "gridSize": 16,
  "rPropose": 0.05,
  "metaLayers": 2,
  "eta": 0.6
}
</file>

<file path="scripts/params/op_coupling/deadline_opk_best.json">
{
  "beta": 2,
  "stepSize": 0.01,
  "p3On": 0,
  "p6On": 1,
  "p6SFactor": 0,
  "pWrite": 0,
  "pNWrite": 0.05,
  "pAWrite": 0,
  "pSWrite": 1,
  "muHigh": 1.8,
  "muLow": 1.8,
  "kappaRep": 1,
  "r0": 0.25,
  "kappaBond": 0,
  "rStar": 0.22,
  "lambdaW": 0,
  "lW": 4,
  "lambdaN": 0,
  "lN": 6,
  "lambdaA": 0,
  "lA": 6,
  "lambdaS": 0,
  "lS": 1,
  "gridSize": 64,
  "rPropose": 0.12,
  "metaLayers": 2,
  "eta": 0,
  "etaDrive": 0.4,
  "codeNoiseRate": 0.012,
  "codeNoiseBatch": 1,
  "codeNoiseLayer": 0,
  "clockOn": 1,
  "clockK": 64,
  "clockFrac": 1,
  "clockUsesP6": 1,
  "repairClockGated": 1,
  "repairGateMode": 1,
  "repairGateSpan": 3,
  "opCouplingOn": 1,
  "sCouplingMode": 1,
  "opDriveOnK": 1,
  "opStencil": 0,
  "opBudgetK": 32,
  "note": "best op config from run-deadline-opk-compare"
}
</file>

<file path="scripts/params/op_coupling/opS_null_energy.json">
{
  "beta": 1.0,
  "gridSize": 16,
  "metaLayers": 2,
  "lS": 6,
  "lambdaS": 0.5,
  "p3On": 0,
  "p6On": 0,
  "pWrite": 0,
  "pNWrite": 0,
  "pAWrite": 0,
  "pSWrite": 1,
  "eta": 0.6,
  "etaDrive": 0.0,
  "opCouplingOn": 1,
  "sCouplingMode": 1,
  "opStencil": 1,
  "opBudgetK": 16,
  "opDriveOnK": 1
}
</file>

<file path="scripts/params/op_coupling/opS_p6_drive_only.json">
{
  "beta": 1.0,
  "gridSize": 16,
  "metaLayers": 2,
  "lS": 6,
  "lambdaS": 0.5,
  "p3On": 0,
  "p6On": 1,
  "p6SFactor": 1.0,
  "muHigh": 1.0,
  "muLow": -1.0,
  "pWrite": 0,
  "pNWrite": 0,
  "pAWrite": 0,
  "pSWrite": 1,
  "eta": 0.0,
  "etaDrive": 0.6,
  "opCouplingOn": 1,
  "sCouplingMode": 1,
  "opStencil": 1,
  "opBudgetK": 16,
  "opDriveOnK": 1
}
</file>

<file path="scripts/params/op_motifs_selection/selection_base_tuned.json">
{
  "params": {
    "beta": 2,
    "stepSize": 0.01,
    "p3On": 0,
    "p6On": 1,
    "p6SFactor": 0,
    "pWrite": 0,
    "pNWrite": 0.05,
    "pAWrite": 0,
    "pSWrite": 1,
    "muHigh": 1.8,
    "muLow": 1.8,
    "kappaRep": 1,
    "r0": 0.25,
    "kappaBond": 0,
    "rStar": 0.22,
    "lambdaW": 0,
    "lW": 4,
    "lambdaN": 0,
    "lN": 6,
    "lambdaA": 0,
    "lA": 6,
    "lambdaS": 0,
    "lS": 1,
    "gridSize": 64,
    "rPropose": 0.12,
    "metaLayers": 2,
    "eta": 0,
    "etaDrive": 0.4,
    "codeNoiseRate": 0.012,
    "codeNoiseBatch": 1,
    "codeNoiseLayer": 0,
    "clockOn": 1,
    "clockK": 64,
    "clockFrac": 1,
    "clockUsesP6": 1,
    "repairClockGated": 1,
    "repairGateMode": 1,
    "repairGateSpan": 3
  },
  "deadline": 38400,
  "sourcePreset": "/home/itsio/ratchet-playground/scripts/params/op_motifs_selection/selection_base_tuned.json",
  "notes": "auto-tuned deadline for missFrac band (attempts=0, missMean=0.29914529914529914)"
}
</file>

<file path="scripts/params/base_null_balanced.json">
{
  "beta": 1.0,
  "stepSize": 0.01,
  "p3On": 0,
  "p6On": 0,
  "pWrite": 0.05,
  "pNWrite": 0.05,
  "pAWrite": 0.05,
  "pSWrite": 0.05,
  "muHigh": 0.6,
  "muLow": -0.6,
  "kappaRep": 500.0,
  "r0": 0.25,
  "kappaBond": 1.2,
  "rStar": 0.22,
  "lambdaW": 0.5,
  "lW": 4,
  "lambdaN": 0.5,
  "lN": 6,
  "lambdaA": 0.5,
  "lA": 6,
  "lambdaS": 0.5,
  "lS": 6,
  "gridSize": 16,
  "rPropose": 0.05,
  "metaLayers": 0,
  "eta": 0.0
}
</file>

<file path="scripts/params/base_p3_pump_minimal.json">
{
  "beta": 1.0,
  "stepSize": 0.02,
  "p3On": 1,
  "p6On": 0,
  "pWrite": 0.05,
  "pNWrite": 0.0,
  "pAWrite": 0.0,
  "pSWrite": 0.05,
  "muHigh": 0.6,
  "muLow": -0.6,
  "kappaRep": 500.0,
  "r0": 0.25,
  "kappaBond": 1.2,
  "rStar": 0.22,
  "lambdaW": 0.5,
  "lW": 4,
  "lambdaN": 0.5,
  "lN": 6,
  "lambdaA": 0.5,
  "lA": 6,
  "lambdaS": 0.5,
  "lS": 6,
  "gridSize": 16,
  "rPropose": 0.05,
  "metaLayers": 0,
  "eta": 0.0
}
</file>

<file path="scripts/params/base_p3p6_combo_minimal.json">
{
  "beta": 1.0,
  "stepSize": 0.02,
  "p3On": 1,
  "p6On": 1,
  "pWrite": 0.05,
  "pNWrite": 0.0,
  "pAWrite": 0.0,
  "pSWrite": 0.05,
  "muHigh": 1.0,
  "muLow": -1.0,
  "kappaRep": 500.0,
  "r0": 0.25,
  "kappaBond": 1.2,
  "rStar": 0.22,
  "lambdaW": 0.5,
  "lW": 4,
  "lambdaN": 0.5,
  "lN": 6,
  "lambdaA": 0.5,
  "lA": 6,
  "lambdaS": 0.5,
  "lS": 6,
  "gridSize": 16,
  "rPropose": 0.05,
  "metaLayers": 0,
  "eta": 0.0
}
</file>

<file path="scripts/params/base_p6_drive.json">
{
  "beta": 1.0,
  "stepSize": 0.02,
  "p3On": 0,
  "p6On": 1,
  "pWrite": 0.05,
  "pNWrite": 0.05,
  "pAWrite": 0.05,
  "pSWrite": 0.05,
  "muHigh": 1.0,
  "muLow": -1.0,
  "kappaRep": 500.0,
  "r0": 0.25,
  "kappaBond": 1.2,
  "rStar": 0.22,
  "lambdaW": 0.5,
  "lW": 4,
  "lambdaN": 0.5,
  "lN": 6,
  "lambdaA": 0.5,
  "lA": 6,
  "lambdaS": 0.5,
  "lS": 6,
  "gridSize": 16,
  "rPropose": 0.05,
  "metaLayers": 0,
  "eta": 0.0
}
</file>

<file path="scripts/calibrate-gate-gaps.mjs">
#!/usr/bin/env node
import fs from "node:fs";
import path from "node:path";
import { calibrateGateGaps, loadWasm, readJson } from "./deadline-event-utils.mjs";

const rootDir = path.resolve(path.dirname(new URL(import.meta.url).pathname), "..");
const outDir = path.resolve(rootDir, ".tmp", "clock_code");

function ensureDir(dir) {
  fs.mkdirSync(dir, { recursive: true });
}

function parseArgs(argv) {
  const out = {
    preset: null,
    variant: "drift",
    steps: 300_000,
    reportEvery: 1_000,
    region: "stripe",
    regionIndex: 0,
    gateSpan: null,
  };
  const args = argv.slice(2);
  for (let i = 0; i < args.length; i += 1) {
    const arg = args[i];
    if (arg === "--preset") out.preset = args[++i];
    else if (arg === "--variant") out.variant = args[++i];
    else if (arg === "--steps") out.steps = Number(args[++i]);
    else if (arg === "--reportEvery") out.reportEvery = Number(args[++i]);
    else if (arg === "--region") out.region = args[++i];
    else if (arg === "--regionIndex") out.regionIndex = Number(args[++i]);
    else if (arg === "--gateSpan") out.gateSpan = Number(args[++i]);
  }
  return out;
}

ensureDir(outDir);
await loadWasm();

const opts = parseArgs(process.argv);
if (!opts.preset) {
  console.error("Missing --preset");
  process.exit(1);
}

const presetPath = path.resolve(rootDir, opts.preset);
const presetParams = readJson(presetPath);

const variants = opts.variant === "all" ? ["drift", "random"] : [opts.variant];
const deadlineScale = 1.2;
const rows = [];

for (const variant of variants) {
  const result = await calibrateGateGaps({
    presetPath,
    presetParams,
    variant,
    steps: opts.steps,
    reportEvery: opts.reportEvery,
    regionType: opts.region,
    regionIndex: opts.regionIndex,
    gateSpan: opts.gateSpan,
  });
  const gapP95 = result.gapP95 ?? 0;
  rows.push({
    variant,
    gapP50: result.gapP50 ?? 0,
    gapP95,
    gapMax: result.gapMax ?? 0,
    deadlineRec: Math.ceil(deadlineScale * gapP95),
  });
}

const outPath = path.join(outDir, "gate_gap_calibration.csv");
const header = ["variant", "gapP50", "gapP95", "gapMax", "deadlineRec"];
const csv = [
  header.join(","),
  ...rows.map((r) => [r.variant, r.gapP50, r.gapP95, r.gapMax, r.deadlineRec].join(",")),
];
fs.writeFileSync(outPath, csv.join("\n"));

console.log("Gate gap calibration:");
for (const row of rows) {
  console.log(
    `${row.variant} | gapP95 ${row.gapP95} | gapMax ${row.gapMax} | deadlineRec ${row.deadlineRec}`,
  );
}
</file>

<file path="scripts/opk-metrics.mjs">
export function parseOpOffsets(opOffsets) {
  const offsets = [];
  for (let i = 0; i + 1 < opOffsets.length; i += 2) {
    offsets.push([opOffsets[i], opOffsets[i + 1]]);
  }
  return offsets;
}

function mean(values) {
  return values.reduce((acc, v) => acc + v, 0) / values.length;
}

function std(values, meanVal) {
  const variance = values.reduce((acc, v) => acc + (v - meanVal) ** 2, 0) / values.length;
  return Math.sqrt(variance);
}

function offsetIndex(q, dx, dy, g) {
  const x = q % g;
  const y = Math.floor(q / g);
  const nx = (x + dx + g) % g;
  const ny = (y + dy + g) % g;
  return ny * g + nx;
}

function buildOffsetIndex(offsets) {
  const map = new Map();
  offsets.forEach(([dx, dy], idx) => {
    map.set(`${dx},${dy}`, idx);
  });
  return map;
}

export function computeOpkMetrics({
  gridSize,
  metaLayers,
  rCount,
  opBudgetK,
  opOffsets,
  opKTokens,
  baseS,
  metaS,
  lS,
}) {
  const offsets = parseOpOffsets(opOffsets);
  const cells = gridSize * gridSize;
  const denom = Math.max(1, lS ?? 1);
  const budget = Math.max(1, opBudgetK ?? 1);
  const eps = 1e-9;
  const offsetIndexMap = buildOffsetIndex(offsets);
  const zeroIdx = offsetIndexMap.get("0,0");

  const perInterface = [];

  for (let iface = 0; iface < metaLayers; iface += 1) {
    let m0Sum = 0;
    let hSum = 0;
    let r2Sum = 0;
    let aSum = 0;
    let cohSum = 0;
    let sdiffSum = 0;

    for (let q = 0; q < cells; q += 1) {
      const start = (iface * cells + q) * rCount;
      let h = 0;
      let r2 = 0;
      let pred = 0;

      const kVals = new Array(rCount);
      for (let r = 0; r < rCount; r += 1) {
        const k = opKTokens[start + r] / budget;
        kVals[r] = k;
        const [dx, dy] = offsets[r];
        r2 += k * (dx * dx + dy * dy);
        if (k > 0) h += -k * Math.log(k + eps);
        const qOff = offsetIndex(q, dx, dy, gridSize);
        const lower = iface === 0
          ? baseS[qOff]
          : metaS[(iface - 1) * cells + qOff];
        pred += k * (lower / denom);
      }

      const upper = metaS[iface * cells + q] / denom;
      sdiffSum += Math.abs(upper - pred);
      hSum += h;
      r2Sum += r2;

      if (zeroIdx !== undefined) {
        m0Sum += kVals[zeroIdx] ?? 0;
      }

      let aAcc = 0;
      let pairCount = 0;
      for (let r = 0; r < rCount; r += 1) {
        const [dx, dy] = offsets[r];
        const negIdx = offsetIndexMap.get(`${-dx},${-dy}`);
        if (negIdx === undefined || negIdx <= r) continue;
        aAcc += Math.abs(kVals[r] - kVals[negIdx]);
        pairCount += 1;
      }
      if (pairCount > 0) aSum += aAcc / pairCount;

      const qRight = offsetIndex(q, 1, 0, gridSize);
      const qDown = offsetIndex(q, 0, 1, gridSize);
      let l1Right = 0;
      let l1Down = 0;
      const startRight = (iface * cells + qRight) * rCount;
      const startDown = (iface * cells + qDown) * rCount;
      for (let r = 0; r < rCount; r += 1) {
        l1Right += Math.abs(kVals[r] - opKTokens[startRight + r] / budget);
        l1Down += Math.abs(kVals[r] - opKTokens[startDown + r] / budget);
      }
      cohSum += 0.5 * (l1Right + l1Down);
    }

    const denomCells = cells > 0 ? cells : 1;
    perInterface.push({
      m0: m0Sum / denomCells,
      H: hSum / denomCells,
      R2: r2Sum / denomCells,
      A: aSum / denomCells,
      coh: cohSum / denomCells,
      Sdiff_op: sdiffSum / denomCells,
    });
  }

  const m0Arr = perInterface.map((m) => m.m0);
  const hArr = perInterface.map((m) => m.H);
  const r2Arr = perInterface.map((m) => m.R2);
  const aArr = perInterface.map((m) => m.A);
  const cohArr = perInterface.map((m) => m.coh);
  const sdiffArr = perInterface.map((m) => m.Sdiff_op);

  const m0Mean = mean(m0Arr);
  const hMean = mean(hArr);
  const r2Mean = mean(r2Arr);
  const aMean = mean(aArr);
  const cohMean = mean(cohArr);
  const sdiffMean = mean(sdiffArr);

  return {
    perInterface,
    m0Arr,
    hArr,
    r2Arr,
    aArr,
    cohArr,
    sdiffArr,
    summary: {
      m0Mean,
      m0Std: std(m0Arr, m0Mean),
      hMean,
      hStd: std(hArr, hMean),
      r2Mean,
      r2Std: std(r2Arr, r2Mean),
      aMean,
      aStd: std(aArr, aMean),
      cohMean,
      cohStd: std(cohArr, cohMean),
      sdiffMean,
      sdiffStd: std(sdiffArr, sdiffMean),
    },
  };
}

export function finiteCheck(obj) {
  if (obj === null || obj === undefined) return false;
  if (Array.isArray(obj)) return obj.every((v) => finiteCheck(v));
  if (typeof obj === "number") return Number.isFinite(obj);
  if (typeof obj === "object") {
    return Object.values(obj).every((v) => finiteCheck(v));
  }
  return true;
}

export function computeSpearman(x, y) {
  if (x.length !== y.length || x.length === 0) return 0;
  const rank = (arr) => {
    const sorted = arr.map((v, i) => ({ v, i })).sort((a, b) => a.v - b.v);
    const ranks = new Array(arr.length);
    let i = 0;
    while (i < sorted.length) {
      let j = i + 1;
      while (j < sorted.length && sorted[j].v === sorted[i].v) j += 1;
      const r = 0.5 * (i + j - 1) + 1;
      for (let k = i; k < j; k += 1) ranks[sorted[k].i] = r;
      i = j;
    }
    return ranks;
  };
  const rx = rank(x);
  const ry = rank(y);
  const meanRx = mean(rx);
  const meanRy = mean(ry);
  let num = 0;
  let denomX = 0;
  let denomY = 0;
  for (let i = 0; i < rx.length; i += 1) {
    const dx = rx[i] - meanRx;
    const dy = ry[i] - meanRy;
    num += dx * dy;
    denomX += dx * dx;
    denomY += dy * dy;
  }
  const denom = Math.sqrt(denomX * denomY);
  return denom === 0 ? 0 : num / denom;
}
</file>

<file path="scripts/opk-motif-basis.mjs">
export function binByThresholds(value, t1, t2) {
  if (value <= t1) return 0;
  if (value <= t2) return 1;
  return 2;
}

export function offsetsToDxDy(opOffsets) {
  const rCount = Math.floor(opOffsets.length / 2);
  const dx = new Int8Array(rCount);
  const dy = new Int8Array(rCount);
  for (let i = 0; i < rCount; i += 1) {
    dx[i] = opOffsets[2 * i];
    dy[i] = opOffsets[2 * i + 1];
  }
  return { dx, dy };
}

export function edgeKey(fromIdx, toIdx) {
  return `${fromIdx}->${toIdx}`;
}

export function edgeFamily(fromIdx, toIdx, dx, dy) {
  const ddx = dx[toIdx] - dx[fromIdx];
  const ddy = dy[toIdx] - dy[fromIdx];
  const sx = ddx === 0 ? 0 : ddx > 0 ? 1 : -1;
  const sy = ddy === 0 ? 0 : ddy > 0 ? 1 : -1;
  return `${sx},${sy}`;
}

export function signBin(value, eps = 1e-6) {
  if (value > eps) return 2;
  if (value < -eps) return 0;
  return 1;
}

export function combineBase3(bins) {
  let id = 0;
  let factor = 1;
  for (const b of bins) {
    id += b * factor;
    factor *= 3;
  }
  return id;
}

function index2d(x, y, g) {
  const xx = (x + g) % g;
  const yy = (y + g) % g;
  return yy * g + xx;
}

function lowerUpperFields(baseS, metaS, cells, iface) {
  const upper = metaS.subarray((iface - 1) * cells, iface * cells);
  const lower = iface === 1 ? baseS : metaS.subarray((iface - 2) * cells, (iface - 1) * cells);
  return { lower, upper };
}

export function computeMBaseClasses({ baseS, metaS, gridSize, lS, metaLayers }) {
  const cells = gridSize * gridSize;
  const t1 = lS / 3;
  const t2 = (2 * lS) / 3;
  const classes = new Array(metaLayers);
  for (let iface = 1; iface <= metaLayers; iface += 1) {
    const { lower, upper } = lowerUpperFields(baseS, metaS, cells, iface);
    const arr = new Array(cells);
    for (let q = 0; q < cells; q += 1) {
      const x = q % gridSize;
      const y = Math.floor(q / gridSize);
      const lowerVal = lower[q] ?? 0;
      const upperVal = upper[q] ?? 0;
      const lowerBin = binByThresholds(lowerVal, t1, t2);
      const upperBin = binByThresholds(upperVal, t1, t2);
      const mismatch = signBin(upperVal - lowerVal, 0);
      const rightIdx = index2d(x + 1, y, gridSize);
      const downIdx = index2d(x, y + 1, gridSize);
      const gradX = signBin((lower[rightIdx] ?? 0) - lowerVal, 0);
      const gradY = signBin((lower[downIdx] ?? 0) - lowerVal, 0);
      arr[q] = combineBase3([lowerBin, upperBin, mismatch, gradX, gradY]);
    }
    classes[iface - 1] = arr;
  }
  return classes;
}

function axisBinsForTokensMode(tokens, offset, rCount, offsets, budget, opBinsMode) {
  const denom = budget > 0 ? budget : 1;
  let center = 0;
  let posX = 0;
  let negX = 0;
  let posY = 0;
  let negY = 0;
  for (let i = 0; i < rCount; i += 1) {
    const count = tokens[offset + i] ?? 0;
    const off = offsets[i] ?? [0, 0];
    const dx = off[0];
    const dy = off[1];
    if (dx === 0 && dy === 0) {
      center += count;
    } else if (Math.abs(dx) >= Math.abs(dy)) {
      if (dx >= 0) posX += count;
      else negX += count;
    } else {
      if (dy >= 0) posY += count;
      else negY += count;
    }
  }

  let toBin;
  if (opBinsMode === 1) {
    toBin = (mass) => {
      if (mass <= 0) return 0;
      if (mass <= 3) return 1;
      return 2;
    };
  } else if (opBinsMode === 2) {
    toBin = (mass) => binByThresholds(mass / denom, 0.08, 0.16);
  } else {
    toBin = (mass) => binByThresholds(mass / denom, 1 / 6, 2 / 6);
  }

  return {
    centerBin: toBin(center),
    posXBin: toBin(posX),
    negXBin: toBin(negX),
    posYBin: toBin(posY),
    negYBin: toBin(negY),
  };
}

function dir9MassesForCell(tokens, baseOffset, rCount, offsets, opBudgetK) {
  const masses = new Float64Array(9);
  const denom = Math.max(1, opBudgetK);
  for (let r = 0; r < rCount; r += 1) {
    const count = tokens[baseOffset + r] ?? 0;
    if (count === 0) continue;
    const off = offsets[r] ?? [0, 0];
    const dx = off[0];
    const dy = off[1];
    let idx = 0;
    if (dx === 0 && dy === 0) idx = 0;
    else if (dx > 0 && dy === 0) idx = 1;
    else if (dx < 0 && dy === 0) idx = 2;
    else if (dx === 0 && dy < 0) idx = 3;
    else if (dx === 0 && dy > 0) idx = 4;
    else if (dx > 0 && dy < 0) idx = 5;
    else if (dx < 0 && dy < 0) idx = 6;
    else if (dx > 0 && dy > 0) idx = 7;
    else idx = 8;
    masses[idx] += count / denom;
  }
  return masses;
}

export function computeMOpClasses({
  baseS,
  metaS,
  gridSize,
  lS,
  metaLayers,
  tokens,
  rCount,
  offsets,
  opBudgetK,
  opBinsMode = 2,
}) {
  const cells = gridSize * gridSize;
  const t1 = lS / 3;
  const t2 = (2 * lS) / 3;
  const classes = new Array(metaLayers);
  for (let iface = 1; iface <= metaLayers; iface += 1) {
    const { lower, upper } = lowerUpperFields(baseS, metaS, cells, iface);
    const arr = new Array(cells);
    const tokenBase = (iface - 1) * cells * rCount;
    for (let q = 0; q < cells; q += 1) {
      const lowerVal = lower[q] ?? 0;
      const upperVal = upper[q] ?? 0;
      const mismatch = signBin(upperVal - lowerVal, 0);
      const tokenOffset = tokenBase + q * rCount;
      if (opBinsMode === 0) {
        let bins = { centerBin: 0, posXBin: 0, negXBin: 0, posYBin: 0, negYBin: 0 };
        if (rCount > 0 && tokens.length >= tokenOffset + rCount) {
          bins = axisBinsForTokensMode(
            tokens,
            tokenOffset,
            rCount,
            offsets,
            opBudgetK,
            opBinsMode,
          );
        }
        arr[q] = combineBase3([
          mismatch,
          bins.centerBin,
          bins.posXBin,
          bins.negXBin,
          bins.posYBin,
          bins.negYBin,
        ]);
      } else {
        const masses =
          rCount > 0 && tokens.length >= tokenOffset + rCount
            ? dir9MassesForCell(tokens, tokenOffset, rCount, offsets, opBudgetK)
            : new Float64Array(9);
        let argmax = 0;
        let maxVal = masses[0];
        for (let i = 1; i < masses.length; i += 1) {
          if (masses[i] > maxVal) {
            maxVal = masses[i];
            argmax = i;
          }
        }
        if (opBinsMode === 1) {
          arr[q] = mismatch * 9 + argmax;
        } else {
          let sum = 0;
          for (let i = 0; i < masses.length; i += 1) sum += masses[i];
          let h = 0;
          if (sum > 0) {
            for (let i = 0; i < masses.length; i += 1) {
              const p = masses[i] / sum;
              if (p > 0) h += -p * Math.log(p);
            }
          }
          const hNorm = h / Math.log(9);
          let hBin = 2;
          if (hNorm < 0.33) hBin = 0;
          else if (hNorm < 0.66) hBin = 1;
          arr[q] = mismatch + 3 * (argmax + 9 * hBin);
        }
      }
    }
    classes[iface - 1] = arr;
  }
  return classes;
}

export function mOpStateCount(opBinsMode) {
  if (opBinsMode === 0) return 729;
  if (opBinsMode === 1) return 27;
  return 81;
}

export function coarseEPDecompose(edgeCounts, alpha = 0.5) {
  const perState = new Map();
  const perEdge = new Map();
  let total = 0;
  for (const [key, count] of edgeCounts.entries()) {
    const parts = key.includes("->") ? key.split("->") : key.split("|");
    const i = Number(parts[0]);
    const j = Number(parts[1]);
    if (Number.isNaN(i) || Number.isNaN(j)) continue;
    if (i === j) continue;
    if (i > j) continue;
    const revKey = key.includes("->") ? `${j}->${i}` : `${j}|${i}`;
    const rev = edgeCounts.get(revKey) ?? 0;
    const c1 = count + alpha;
    const c2 = rev + alpha;
    const epPair = (c1 - c2) * Math.log(c1 / c2);
    if (epPair > 0) total += epPair;
    perState.set(String(i), (perState.get(String(i)) ?? 0) + epPair / 2);
    perState.set(String(j), (perState.get(String(j)) ?? 0) + epPair / 2);
    perEdge.set(
      `${i}->${j}`,
      (perEdge.get(`${i}->${j}`) ?? 0) + count * Math.log(c1 / c2),
    );
    perEdge.set(
      `${j}->${i}`,
      (perEdge.get(`${j}->${i}`) ?? 0) + rev * Math.log(c2 / c1),
    );
  }
  return { total, perState, perEdge };
}

export function vocabStats(countMap, topN = 10) {
  const entries = Array.from(countMap.values());
  const total = entries.reduce((acc, v) => acc + v, 0);
  let h = 0;
  if (total > 0) {
    for (const v of entries) {
      const p = v / total;
      if (p > 0) h += -p * Math.log(p);
    }
  }
  const sorted = entries.slice().sort((a, b) => b - a);
  const top = sorted.slice(0, topN).reduce((acc, v) => acc + v, 0);
  return {
    H_vocab: h,
    V_eff: Math.exp(h),
    topMass10: total > 0 ? top / total : 0,
    total,
  };
}

export function asymmetryScore(counts) {
  let numerator = 0;
  let denom = 0;
  for (const [key, c] of counts.entries()) {
    const [from, to] = key.split("|");
    if (from === to) continue;
    const revKey = `${to}|${from}`;
    const rev = counts.get(revKey) ?? 0;
    if (from < to) {
      numerator += Math.abs(c - rev);
    }
    denom += c;
  }
  return denom > 0 ? numerator / denom : 0;
}

export function coarseEPSmoothed(counts, alpha = 0.5) {
  let acc = 0;
  for (const [key, c] of counts.entries()) {
    const [from, to] = key.split("|");
    if (from === to) continue;
    const revKey = `${to}|${from}`;
    const rev = counts.get(revKey) ?? 0;
    if (from < to) {
      const c1 = c + alpha;
      const c2 = rev + alpha;
      acc += (c1 - c2) * Math.log(c1 / c2);
    }
  }
  return 0.5 * acc;
}

export function jsDivergence(countA, countB, eps = 1e-12) {
  const keys = new Set([...countA.keys(), ...countB.keys()]);
  let totalA = 0;
  let totalB = 0;
  for (const v of countA.values()) totalA += v;
  for (const v of countB.values()) totalB += v;
  if (totalA === 0 && totalB === 0) return 0;
  const normA = totalA > 0 ? totalA : 1;
  const normB = totalB > 0 ? totalB : 1;
  let klA = 0;
  let klB = 0;
  for (const key of keys) {
    const p = (countA.get(key) ?? 0) / normA;
    const q = (countB.get(key) ?? 0) / normB;
    const m = 0.5 * (p + q);
    if (p > 0) klA += p * Math.log((p + eps) / (m + eps));
    if (q > 0) klB += q * Math.log((q + eps) / (m + eps));
  }
  return 0.5 * (klA + klB);
}
</file>

<file path="scripts/opk-motif-utils.mjs">
export function motifKeyFromTokens(tokens, idx, rCount) {
  const start = idx * rCount;
  const parts = [];
  for (let i = 0; i < rCount; i += 1) {
    parts.push(tokens[start + i]);
  }
  return parts.join(",");
}

export function motifFeatures(tokensVec, offsets) {
  const total = tokensVec.reduce((acc, v) => acc + v, 0);
  if (total <= 0) {
    return { H: 0, R2: 0, dX: 0, dY: 0, m0: 0 };
  }
  let h = 0;
  let r2 = 0;
  let dx = 0;
  let dy = 0;
  let m0 = 0;
  for (let i = 0; i < tokensVec.length; i += 1) {
    const p = tokensVec[i] / total;
    if (p > 0) h += -p * Math.log(p);
    const [ox, oy] = offsets[i];
    r2 += p * (ox * ox + oy * oy);
    dx += p * ox;
    dy += p * oy;
    if (ox === 0 && oy === 0) m0 = p;
  }
  return { H: h, R2: r2, dX: dx, dY: dy, m0 };
}

export function buildTopVocab(countMap, topN) {
  const entries = Array.from(countMap.entries()).sort((a, b) => b[1] - a[1]);
  const vocabKeys = entries.slice(0, topN).map(([key]) => key);
  const keyToId = new Map();
  vocabKeys.forEach((key, idx) => keyToId.set(key, idx));
  return { vocabKeys, keyToId, OTHER_ID: topN };
}

export function asymmetryScore(transCounts) {
  let num = 0;
  let denom = 0;
  for (const [key, count] of transCounts.entries()) {
    const [from, to] = key.split("|");
    if (from === to) continue;
    const revKey = `${to}|${from}`;
    const revCount = transCounts.get(revKey) ?? 0;
    if (from < to) {
      num += Math.abs(count - revCount);
    }
    denom += count;
  }
  return denom > 0 ? num / denom : 0;
}

export function coarseEPFromCounts(transCounts, eps = 1e-12) {
  let acc = 0;
  const seen = new Set();
  for (const [key, count] of transCounts.entries()) {
    const [from, to] = key.split("|");
    if (from === to) continue;
    const revKey = `${to}|${from}`;
    const pairKey = from < to ? key : revKey;
    if (seen.has(pairKey)) continue;
    seen.add(pairKey);
    const c1 = count;
    const c2 = transCounts.get(revKey) ?? 0;
    const diff = c1 - c2;
    if (diff === 0) continue;
    acc += diff * Math.log((c1 + eps) / (c2 + eps));
  }
  return acc;
}
</file>

<file path="scripts/README.md">
# Scripts

- `scripts/bootstrap.sh`: installs Rust + wasm toolchain, `wasm-pack`, and Node.js LTS (via `nvm`) for WSL/Ubuntu dev environments.

Run from repo root:

```bash
bash scripts/bootstrap.sh
make dev
```
</file>

<file path="scripts/run-deadline-event-stats.mjs">
#!/usr/bin/env node
import fs from "node:fs";
import path from "node:path";
import {
  loadWasm,
  mean,
  std,
  percentile,
  parseSeedList,
  readJson,
  runDeadlineEvents,
} from "./deadline-event-utils.mjs";

const rootDir = path.resolve(path.dirname(new URL(import.meta.url).pathname), "..");
const outDir = path.resolve(rootDir, ".tmp", "clock_code");

function ensureDir(dir) {
  fs.mkdirSync(dir, { recursive: true });
}

function parseArgs(argv) {
  const out = {
    preset: null,
    variant: "drift",
    seeds: [1, 2, 3],
    steps: 2_000_000,
    reportEvery: 5_000,
    eventEvery: 50_000,
    deadline: 25_000,
    region: "quadrant",
    regionIndex: 2,
    gateSpan: null,
    corruptFrac: 0.2,
    errGood: 0.1,
    sdiffGood: 1.0,
    tailWindow: 200_000,
  };
  const args = argv.slice(2);
  for (let i = 0; i < args.length; i += 1) {
    const arg = args[i];
    if (arg === "--preset") out.preset = args[++i];
    else if (arg === "--variant") out.variant = args[++i];
    else if (arg === "--seeds") out.seeds = parseSeedList(args[++i]);
    else if (arg === "--steps") out.steps = Number(args[++i]);
    else if (arg === "--reportEvery") out.reportEvery = Number(args[++i]);
    else if (arg === "--eventEvery") out.eventEvery = Number(args[++i]);
    else if (arg === "--deadline") out.deadline = Number(args[++i]);
    else if (arg === "--region") out.region = args[++i];
    else if (arg === "--regionIndex") out.regionIndex = Number(args[++i]);
    else if (arg === "--gateSpan") out.gateSpan = Number(args[++i]);
    else if (arg === "--corruptFrac") out.corruptFrac = Number(args[++i]);
    else if (arg === "--errGood") out.errGood = Number(args[++i]);
    else if (arg === "--sdiffGood") out.sdiffGood = Number(args[++i]);
    else if (arg === "--tailWindow") out.tailWindow = Number(args[++i]);
  }
  return out;
}

ensureDir(outDir);
const opts = parseArgs(process.argv);
if (!opts.preset) {
  console.error("Missing --preset");
  process.exit(1);
}

await loadWasm();
const presetPath = path.resolve(rootDir, opts.preset);
const presetParams = readJson(presetPath);

const variants = opts.variant === "all" ? ["drift", "random", "static"] : [opts.variant];
const rawPath = path.join(outDir, "deadline_event_stats_raw.jsonl");
const summaryPath = path.join(outDir, "deadline_event_stats_summary.csv");
const rawLines = [];
const summaries = [];

for (const variant of variants) {
  const { runs } = await runDeadlineEvents({
    presetPath,
    presetParams,
    variant,
    seeds: opts.seeds,
    steps: opts.steps,
    reportEvery: opts.reportEvery,
    eventEvery: opts.eventEvery,
    deadline: opts.deadline,
    regionType: opts.region,
    regionIndex: opts.regionIndex,
    gateSpan: opts.gateSpan,
    corruptFrac: opts.corruptFrac,
    errGood: opts.errGood,
    sdiffGood: opts.sdiffGood,
    tailWindow: opts.tailWindow,
  });
  for (const r of runs) {
    rawLines.push(JSON.stringify({ variant, ...r }));
  }

  const missFrac = runs.map((r) => r.missFrac);
  const recP95 = runs.map((r) => r.recoveryP95 ?? 0);
  const uptime = runs.map((r) => r.uptime);
  const errEnd = runs.map((r) => r.errEnd);
  const sdiffEnd = runs.map((r) => r.sdiffEnd);
  const uptimeTail = runs.map((r) => r.uptimeTail);
  const errTail = runs.map((r) => r.errTailMean);
  const sdiffTail = runs.map((r) => r.sdiffTailMean);
  const errP95 = runs.map((r) => r.errP95 ?? 0);
  const epTotal = runs.map((r) => r.epTotalRate);
  const epClock = runs.map((r) => r.epClockRate);
  const epRepair = runs.map((r) => r.epRepairRate);
  const epNoise = runs.map((r) => r.epNoiseRate);

  summaries.push({
    variant,
    missFracMean: mean(missFrac),
    missFracStd: std(missFrac),
    recoveryP95Mean: mean(recP95),
    recoveryP95Std: std(recP95),
    uptimeMean: mean(uptime),
    uptimeStd: std(uptime),
    errEndMean: mean(errEnd),
    errEndStd: std(errEnd),
    sdiffEndMean: mean(sdiffEnd),
    sdiffEndStd: std(sdiffEnd),
    uptimeTailMean: mean(uptimeTail),
    uptimeTailStd: std(uptimeTail),
    errTailMean: mean(errTail),
    errTailStd: std(errTail),
    sdiffTailMean: mean(sdiffTail),
    sdiffTailStd: std(sdiffTail),
    errP95Mean: mean(errP95),
    errP95Std: std(errP95),
    epTotalRateMean: mean(epTotal),
    epClockRateMean: mean(epClock),
    epRepairRateMean: mean(epRepair),
    epNoiseRateMean: mean(epNoise),
  });
}

fs.writeFileSync(rawPath, rawLines.join("\n"));

const header = [
  "variant",
  "missFracMean",
  "missFracStd",
  "recoveryP95Mean",
  "recoveryP95Std",
  "uptimeMean",
  "uptimeStd",
  "errEndMean",
  "errEndStd",
  "sdiffEndMean",
  "sdiffEndStd",
  "uptimeTailMean",
  "uptimeTailStd",
  "errTailMean",
  "errTailStd",
  "sdiffTailMean",
  "sdiffTailStd",
  "errP95Mean",
  "errP95Std",
  "epTotalRateMean",
  "epClockRateMean",
  "epRepairRateMean",
  "epNoiseRateMean",
];
const csv = [
  header.join(","),
  ...summaries.map((summary) =>
    [
      summary.variant,
      summary.missFracMean,
      summary.missFracStd,
      summary.recoveryP95Mean,
      summary.recoveryP95Std,
      summary.uptimeMean,
      summary.uptimeStd,
      summary.errEndMean,
      summary.errEndStd,
      summary.sdiffEndMean,
      summary.sdiffEndStd,
      summary.uptimeTailMean,
      summary.uptimeTailStd,
      summary.errTailMean,
      summary.errTailStd,
      summary.sdiffTailMean,
      summary.sdiffTailStd,
      summary.errP95Mean,
      summary.errP95Std,
      summary.epTotalRateMean,
      summary.epClockRateMean,
      summary.epRepairRateMean,
      summary.epNoiseRateMean,
    ].join(","),
  ),
];
fs.writeFileSync(summaryPath, csv.join("\n"));

console.log("Deadline event stats summary:");
for (const summary of summaries) {
  console.log(
    `${summary.variant} | missFrac ${summary.missFracMean.toFixed(3)} | recP95 ${summary.recoveryP95Mean.toFixed(0)} | uptimeTail ${summary.uptimeTailMean.toFixed(3)} | errTail ${summary.errTailMean.toFixed(3)}`,
  );
}
</file>

<file path="scripts/run-deadline-opk-ci-iso.mjs">
#!/usr/bin/env node
import fs from "node:fs";
import path from "node:path";
import {
  loadWasm,
  mean,
  std,
  readJson,
  runDeadlineEvents,
} from "./deadline-event-utils.mjs";

const rootDir = path.resolve(path.dirname(new URL(import.meta.url).pathname), "..");
const outDir = path.resolve(rootDir, ".tmp", "op_coupling");

function ensureDir(dir) {
  fs.mkdirSync(dir, { recursive: true });
}

class LcgRng {
  constructor(seed) {
    this.state = seed >>> 0;
  }
  next() {
    this.state = (1664525 * this.state + 1013904223) >>> 0;
    return this.state / 0xffffffff;
  }
  int(max) {
    return Math.floor(this.next() * max);
  }
}

function bootstrapMean(values, reps, rng) {
  const n = values.length;
  const draws = [];
  for (let i = 0; i < reps; i += 1) {
    let acc = 0;
    for (let j = 0; j < n; j += 1) {
      acc += values[rng.int(n)];
    }
    draws.push(acc / n);
  }
  draws.sort((a, b) => a - b);
  const lo = draws[Math.floor(0.025 * reps)];
  const hi = draws[Math.floor(0.975 * reps)];
  return { lo, hi };
}

function bootstrapDiff(a, b, reps, rng) {
  const n = a.length;
  const m = b.length;
  const draws = [];
  for (let i = 0; i < reps; i += 1) {
    let accA = 0;
    let accB = 0;
    for (let j = 0; j < n; j += 1) accA += a[rng.int(n)];
    for (let j = 0; j < m; j += 1) accB += b[rng.int(m)];
    draws.push(accA / n - accB / m);
  }
  draws.sort((x, y) => x - y);
  const lo = draws[Math.floor(0.025 * reps)];
  const hi = draws[Math.floor(0.975 * reps)];
  return { lo, hi };
}

ensureDir(outDir);
await loadWasm();

const presetPath = path.resolve(rootDir, "scripts/params/clock_code/deadline_fidelity_found.json");
const fallback = path.resolve(rootDir, "scripts/params/clock_code/code_deadline_gated_clock.json");
const chosenPreset = fs.existsSync(presetPath) ? presetPath : fallback;
const baseParams = readJson(chosenPreset).params ?? readJson(chosenPreset);
const deadline = readJson(chosenPreset).deadline ?? 25_000;

const seeds = Array.from({ length: 50 }, (_, i) => i + 1);
const steps = 500_000;
const reportEvery = 5_000;
const eventEvery = 50_000;
const regionType = "quadrant";
const regionIndex = 2;
const errGood = 0.1;
const sdiffGood = 1.0;
const tailWindow = 200_000;
const reps = 2000;

const modes = [
  {
    id: "A_legacy",
    params: { ...baseParams, opCouplingOn: 0, sCouplingMode: 0, opDriveOnK: 0, pSWrite: 0.65, etaDrive: 0.4 },
  },
  {
    id: "B_dilution_only",
    params: { ...baseParams, opCouplingOn: 1, sCouplingMode: 0, opDriveOnK: 0, pSWrite: 0.2, etaDrive: 0.4 },
  },
  {
    id: "C_op_noKdrive",
    params: { ...baseParams, opCouplingOn: 1, sCouplingMode: 1, opDriveOnK: 0, pSWrite: 0.2, etaDrive: 0.4, opStencil: 1, opBudgetK: 32 },
  },
  {
    id: "D_op_withKdrive",
    params: { ...baseParams, opCouplingOn: 1, sCouplingMode: 1, opDriveOnK: 1, pSWrite: 0.2, etaDrive: 0.8, opStencil: 1, opBudgetK: 16 },
  },
];

const rawRows = [];
const epByMode = new Map();
const missByMode = new Map();
const summaryRows = [];

for (const mode of modes) {
  const result = await runDeadlineEvents({
    presetPath: chosenPreset,
    presetParams: mode.params,
    variant: "drift",
    seeds,
    steps,
    reportEvery,
    eventEvery,
    deadline,
    regionType,
    regionIndex,
    gateSpan: null,
    corruptFrac: 0.2,
    errGood,
    sdiffGood,
    tailWindow,
  });
  for (const run of result.runs) {
    rawRows.push(JSON.stringify({ mode: mode.id, ...run }));
  }
  const ep = result.runs.map((r) => r.epTotalRate);
  const miss = result.runs.map((r) => r.missFrac);
  epByMode.set(mode.id, ep);
  missByMode.set(mode.id, miss);
  const uptime = result.runs.map((r) => r.uptimeTail);
  const err = result.runs.map((r) => r.errTailMean);
  const rng = new LcgRng(123456);
  const epCi = bootstrapMean(ep, reps, rng);
  const missCi = bootstrapMean(miss, reps, rng);
  summaryRows.push({
    mode: mode.id,
    meanEp: mean(ep),
    stdEp: std(ep),
    epCiLow: epCi.lo,
    epCiHigh: epCi.hi,
    meanMiss: mean(miss),
    stdMiss: std(miss),
    missCiLow: missCi.lo,
    missCiHigh: missCi.hi,
    meanUptime: mean(uptime),
    meanErr: mean(err),
  });
}

const modeMap = Object.fromEntries(summaryRows.map((row) => [row.mode, row]));
const rng = new LcgRng(98765);
const epDiffCB = bootstrapDiff(
  epByMode.get("C_op_noKdrive") ?? [],
  epByMode.get("B_dilution_only") ?? [],
  reps,
  rng,
);
const missDiffCB = bootstrapDiff(
  missByMode.get("C_op_noKdrive") ?? [],
  missByMode.get("B_dilution_only") ?? [],
  reps,
  rng,
);

const summaryPath = path.join(outDir, "deadline_iso_ci_summary.csv");
const header = [
  "mode",
  "meanEp",
  "epCiLow",
  "epCiHigh",
  "meanMiss",
  "missCiLow",
  "missCiHigh",
  "meanUptime",
  "meanErr",
  "note",
];
const lines = [
  header.join(","),
  ...summaryRows.map((row) =>
    [
      row.mode,
      row.meanEp,
      row.epCiLow,
      row.epCiHigh,
      row.meanMiss,
      row.missCiLow,
      row.missCiHigh,
      row.meanUptime,
      row.meanErr,
      "",
    ].join(","),
  ),
  [
    "DELTA_EP_C_MINUS_B",
    modeMap.C_op_noKdrive.meanEp - modeMap.B_dilution_only.meanEp,
    epDiffCB.lo,
    epDiffCB.hi,
    "",
    "",
    "",
    "",
    "",
    "",
  ].join(","),
  [
    "DELTA_MISS_C_MINUS_B",
    modeMap.C_op_noKdrive.meanMiss - modeMap.B_dilution_only.meanMiss,
    missDiffCB.lo,
    missDiffCB.hi,
    "",
    "",
    "",
    "",
    "",
    "",
  ].join(","),
].join("\n");

fs.writeFileSync(summaryPath, `${lines}\n`);
fs.writeFileSync(path.join(outDir, "deadline_iso_ci_raw.jsonl"), rawRows.join("\n"));

const excludesZero = epDiffCB.lo > 0 || epDiffCB.hi < 0;
console.log(
  `DELTA_EP_C_MINUS_B_CI: ${epDiffCB.lo.toFixed(6)} ${epDiffCB.hi.toFixed(6)} excludesZero=${excludesZero}`,
);
</file>

<file path="scripts/run-deadline-opk-compare.mjs">
#!/usr/bin/env node
import fs from "node:fs";
import path from "node:path";
import {
  loadWasm,
  mean,
  std,
  readJson,
  runDeadlineEvents,
} from "./deadline-event-utils.mjs";

const rootDir = path.resolve(path.dirname(new URL(import.meta.url).pathname), "..");
const outDir = path.resolve(rootDir, ".tmp", "op_coupling");

function ensureDir(dir) {
  fs.mkdirSync(dir, { recursive: true });
}

function pickPreset() {
  const preferred = path.resolve(rootDir, "scripts/params/clock_code/deadline_fidelity_found.json");
  const fallback = path.resolve(rootDir, "scripts/params/clock_code/code_deadline_gated_clock.json");
  if (fs.existsSync(preferred)) return preferred;
  return fallback;
}

function summarizeRuns(id, runs, meta) {
  const missFrac = runs.map((r) => r.missFrac);
  const uptimeTail = runs.map((r) => r.uptimeTail);
  const errTail = runs.map((r) => r.errTailMean);
  const sdiffTail = runs.map((r) => r.sdiffTailMean);
  const recP95 = runs.map((r) => r.recoveryP95 ?? 0);
  const epTotal = runs.map((r) => r.epTotalRate);
  const epClock = runs.map((r) => r.epClockRate);
  const epRepair = runs.map((r) => r.epRepairRate);
  const epOpK = runs.map((r) => r.epOpKRate ?? 0);

  return {
    id,
    ...meta,
    missFracMean: mean(missFrac),
    missFracStd: std(missFrac),
    uptimeTailMean: mean(uptimeTail),
    uptimeTailStd: std(uptimeTail),
    errTailMean: mean(errTail),
    errTailStd: std(errTail),
    sdiffTailMean: mean(sdiffTail),
    sdiffTailStd: std(sdiffTail),
    recoveryP95Mean: mean(recP95),
    recoveryP95Std: std(recP95),
    epTotalRateMean: mean(epTotal),
    epClockRateMean: mean(epClock),
    epRepairRateMean: mean(epRepair),
    epOpKRateMean: mean(epOpK),
  };
}

function pickBestOp(rows) {
  return rows.reduce((best, row) => {
    if (!best) return row;
    if (row.missFracMean < best.missFracMean - 1e-6) return row;
    if (Math.abs(row.missFracMean - best.missFracMean) <= 1e-6) {
      if (row.uptimeTailMean > best.uptimeTailMean + 1e-6) return row;
      if (Math.abs(row.uptimeTailMean - best.uptimeTailMean) <= 1e-6) {
        if (row.epTotalRateMean < best.epTotalRateMean - 1e-6) return row;
      }
    }
    return best;
  }, null);
}

ensureDir(outDir);
await loadWasm();

const presetPath = pickPreset();
const presetRaw = readJson(presetPath);
const baseParams = presetRaw.params ?? presetRaw;
const baseDeadline = presetRaw.deadline ?? 25_000;
const seeds = Array.from({ length: 10 }, (_, i) => i + 1);

const rawRows = [];
const summaryRows = [];

const legacyParams = { ...baseParams, opCouplingOn: 0, sCouplingMode: 0 };
const legacyRuns = await runDeadlineEvents({
  presetPath,
  presetParams: legacyParams,
  variant: "drift",
  seeds,
  steps: 2_000_000,
  reportEvery: 5_000,
  eventEvery: 50_000,
  deadline: baseDeadline,
  regionType: "quadrant",
  regionIndex: 2,
  gateSpan: null,
  corruptFrac: 0.2,
  errGood: 0.1,
  sdiffGood: 1.0,
  tailWindow: 200_000,
});
for (const r of legacyRuns.runs) rawRows.push(JSON.stringify({ variant: "legacy", ...r }));
summaryRows.push(
  summarizeRuns("legacy", legacyRuns.runs, { opStencil: "", opBudgetK: "" }),
);

const opConfigs = [];
for (const opStencil of [0, 1]) {
  for (const opBudgetK of [8, 16, 32]) {
    opConfigs.push({ opStencil, opBudgetK });
  }
}

const opSummaries = [];
for (const cfg of opConfigs) {
  const params = {
    ...baseParams,
    opCouplingOn: 1,
    sCouplingMode: 1,
    opDriveOnK: 1,
    opStencil: cfg.opStencil,
    opBudgetK: cfg.opBudgetK,
  };
  const runs = await runDeadlineEvents({
    presetPath,
    presetParams: params,
    variant: "drift",
    seeds,
    steps: 2_000_000,
    reportEvery: 5_000,
    eventEvery: 50_000,
    deadline: baseDeadline,
    regionType: "quadrant",
    regionIndex: 2,
    gateSpan: null,
    corruptFrac: 0.2,
    errGood: 0.1,
    sdiffGood: 1.0,
    tailWindow: 200_000,
  });
  for (const r of runs.runs) {
    rawRows.push(JSON.stringify({ variant: "op", opStencil: cfg.opStencil, opBudgetK: cfg.opBudgetK, ...r }));
  }
  const summary = summarizeRuns(
    `op_s${cfg.opStencil}_b${cfg.opBudgetK}`,
    runs.runs,
    cfg,
  );
  opSummaries.push(summary);
  summaryRows.push(summary);
}

const bestOp = pickBestOp(opSummaries);
const legacy = summaryRows.find((r) => r.id === "legacy");
const bestConfig = {
  ...baseParams,
  opCouplingOn: 1,
  sCouplingMode: 1,
  opDriveOnK: 1,
  opStencil: bestOp.opStencil,
  opBudgetK: bestOp.opBudgetK,
  note: "best op config from run-deadline-opk-compare",
};
const bestPath = path.resolve(rootDir, "scripts/params/op_coupling/deadline_opk_best.json");
fs.mkdirSync(path.dirname(bestPath), { recursive: true });
fs.writeFileSync(bestPath, `${JSON.stringify(bestConfig, null, 2)}\n`);

const summaryPath = path.join(outDir, "deadline_opk_summary.csv");
const header = [
  "id",
  "opStencil",
  "opBudgetK",
  "missFracMean",
  "missFracStd",
  "uptimeTailMean",
  "uptimeTailStd",
  "errTailMean",
  "errTailStd",
  "sdiffTailMean",
  "sdiffTailStd",
  "recoveryP95Mean",
  "recoveryP95Std",
  "epTotalRateMean",
  "epClockRateMean",
  "epRepairRateMean",
  "epOpKRateMean",
];
const lines = [
  header.join(","),
  ...summaryRows.map((row) =>
    [
      row.id,
      row.opStencil,
      row.opBudgetK,
      row.missFracMean,
      row.missFracStd,
      row.uptimeTailMean,
      row.uptimeTailStd,
      row.errTailMean,
      row.errTailStd,
      row.sdiffTailMean,
      row.sdiffTailStd,
      row.recoveryP95Mean,
      row.recoveryP95Std,
      row.epTotalRateMean,
      row.epClockRateMean,
      row.epRepairRateMean,
      row.epOpKRateMean,
    ].join(","),
  ),
].join("\n");
fs.writeFileSync(summaryPath, `${lines}\n`);

const rawPath = path.join(outDir, "deadline_opk_raw.jsonl");
fs.writeFileSync(rawPath, rawRows.join("\n"));

console.log(`BEST_OP_CONFIG: stencil=${bestOp.opStencil} budget=${bestOp.opBudgetK} miss=${bestOp.missFracMean.toFixed(3)} uptimeTail=${bestOp.uptimeTailMean.toFixed(3)} epRate=${bestOp.epTotalRateMean.toFixed(4)}`);
const improved =
  bestOp.missFracMean < legacy.missFracMean - 1e-3 ||
  (bestOp.missFracMean <= legacy.missFracMean + 1e-3 &&
    bestOp.uptimeTailMean > legacy.uptimeTailMean + 0.02);
if (!improved) {
  console.log("NO_IMPROVEMENT_OVER_LEGACY");
}
console.log(`deadline opk compare summary written to ${summaryPath}`);
</file>

<file path="scripts/run-deadline-opk-decomposition.mjs">
#!/usr/bin/env node
import fs from "node:fs";
import path from "node:path";
import {
  loadWasm,
  mean,
  std,
  readJson,
  runDeadlineEvents,
} from "./deadline-event-utils.mjs";

const rootDir = path.resolve(path.dirname(new URL(import.meta.url).pathname), "..");
const outDir = path.resolve(rootDir, ".tmp", "op_coupling");

function ensureDir(dir) {
  fs.mkdirSync(dir, { recursive: true });
}

function pickPreset() {
  const preferred = path.resolve(rootDir, "scripts/params/clock_code/deadline_fidelity_found.json");
  const fallback = path.resolve(rootDir, "scripts/params/clock_code/code_deadline_gated_clock.json");
  if (fs.existsSync(preferred)) return preferred;
  return fallback;
}

function summarizeRuns(id, runs, meta) {
  const missFrac = runs.map((r) => r.missFrac);
  const uptimeTail = runs.map((r) => r.uptimeTail);
  const errTail = runs.map((r) => r.errTailMean);
  const sdiffTail = runs.map((r) => r.sdiffTailMean);
  const recMean = runs.map((r) => r.recoveryMean ?? 0);
  const recP95 = runs.map((r) => r.recoveryP95 ?? 0);
  const epTotal = runs.map((r) => r.epTotalRate);
  const epClock = runs.map((r) => r.epClockRate);
  const epRepair = runs.map((r) => r.epRepairRate);
  const epOpK = runs.map((r) => r.epOpKRate ?? 0);
  const p5MetaRecover = runs.map((r) => r.p5MetaToRecoverMean ?? 0);
  const p5MetaRecoverP95 = runs.map((r) => r.p5MetaToRecoverP95 ?? 0);
  const opkRecover = runs.map((r) => r.opkToRecoverMean ?? 0);
  const opkRecoverP95 = runs.map((r) => r.opkToRecoverP95 ?? 0);
  const repairRate = runs.map((r) => r.repairRate ?? 0);
  const opkRate = runs.map((r) => r.opkRate ?? 0);
  const p5MetaSuccessMean = runs.map((r) => r.p5MetaToRecoverSuccessMean ?? 0);
  const p5MetaSuccessP95 = runs.map((r) => r.p5MetaToRecoverSuccessP95 ?? 0);
  const p5MetaMissMean = runs.map((r) => r.p5MetaBeforeMissMean ?? 0);
  const p5MetaMissP95 = runs.map((r) => r.p5MetaBeforeMissP95 ?? 0);
  const opkSuccessMean = runs.map((r) => r.opkToRecoverSuccessMean ?? 0);
  const opkSuccessP95 = runs.map((r) => r.opkToRecoverSuccessP95 ?? 0);
  const opkMissMean = runs.map((r) => r.opkBeforeMissMean ?? 0);
  const opkMissP95 = runs.map((r) => r.opkBeforeMissP95 ?? 0);
  const clockSuccessMean = runs.map((r) => r.clockToRecoverSuccessMean ?? 0);
  const clockSuccessP95 = runs.map((r) => r.clockToRecoverSuccessP95 ?? 0);
  const clockMissMean = runs.map((r) => r.clockBeforeMissMean ?? 0);
  const clockMissP95 = runs.map((r) => r.clockBeforeMissP95 ?? 0);
  const recoveriesCount = runs.map((r) => r.recoveriesCount ?? 0);
  const missesCount = runs.map((r) => r.missesCount ?? 0);
  const repairEfficiencyMean = runs.map((r) => r.repairEfficiencySuccessMean ?? 0);
  const repairEfficiencyMedian = runs.map((r) => r.repairEfficiencySuccessMedian ?? 0);

  return {
    id,
    ...meta,
    missFracMean: mean(missFrac),
    missFracStd: std(missFrac),
    uptimeTailMean: mean(uptimeTail),
    uptimeTailStd: std(uptimeTail),
    errTailMean: mean(errTail),
    errTailStd: std(errTail),
    sdiffTailMean: mean(sdiffTail),
    sdiffTailStd: std(sdiffTail),
    recoveryMean: mean(recMean),
    recoveryP95Mean: mean(recP95),
    epTotalRateMean: mean(epTotal),
    epClockRateMean: mean(epClock),
    epRepairRateMean: mean(epRepair),
    epOpKRateMean: mean(epOpK),
    p5MetaToRecoverMean: mean(p5MetaRecover),
    p5MetaToRecoverP95: mean(p5MetaRecoverP95),
    opkToRecoverMean: mean(opkRecover),
    opkToRecoverP95: mean(opkRecoverP95),
    repairRateMean: mean(repairRate),
    opkRateMean: mean(opkRate),
    p5MetaToRecoverSuccessMean: mean(p5MetaSuccessMean),
    p5MetaToRecoverSuccessP95: mean(p5MetaSuccessP95),
    p5MetaBeforeMissMean: mean(p5MetaMissMean),
    p5MetaBeforeMissP95: mean(p5MetaMissP95),
    opkToRecoverSuccessMean: mean(opkSuccessMean),
    opkToRecoverSuccessP95: mean(opkSuccessP95),
    opkBeforeMissMean: mean(opkMissMean),
    opkBeforeMissP95: mean(opkMissP95),
    clockToRecoverSuccessMean: mean(clockSuccessMean),
    clockToRecoverSuccessP95: mean(clockSuccessP95),
    clockBeforeMissMean: mean(clockMissMean),
    clockBeforeMissP95: mean(clockMissP95),
    recoveriesCountMean: mean(recoveriesCount),
    missesCountMean: mean(missesCount),
    repairEfficiencySuccessMean: mean(repairEfficiencyMean),
    repairEfficiencySuccessMedian: mean(repairEfficiencyMedian),
  };
}

function pickBest(rows) {
  return rows.reduce((best, row) => {
    if (!best) return row;
    if (row.missFracMean < best.missFracMean - 1e-6) return row;
    if (Math.abs(row.missFracMean - best.missFracMean) <= 1e-6) {
      if (row.uptimeTailMean > best.uptimeTailMean + 1e-6) return row;
      if (Math.abs(row.uptimeTailMean - best.uptimeTailMean) <= 1e-6) {
        if (row.epTotalRateMean < best.epTotalRateMean - 1e-6) return row;
      }
    }
    return best;
  }, null);
}

ensureDir(outDir);
await loadWasm();

const presetPath = pickPreset();
const presetRaw = readJson(presetPath);
const baseParams = presetRaw.params ?? presetRaw;
const baseDeadline = presetRaw.deadline ?? 25_000;

const seeds = Array.from({ length: 10 }, (_, i) => i + 1);
const steps = 500_000;
const reportEvery = 5_000;
const eventEvery = 50_000;
const regionType = "quadrant";
const regionIndex = 2;
const errGood = 0.1;
const sdiffGood = 1.0;
const tailWindow = 200_000;

const rawRows = [];
const summaryRows = [];

const conditions = [
  {
    id: "A_legacy",
    params: { ...baseParams, opCouplingOn: 0, sCouplingMode: 0, opDriveOnK: 0 },
  },
  {
    id: "B_dilution_only",
    params: { ...baseParams, opCouplingOn: 1, sCouplingMode: 0, opDriveOnK: 0 },
  },
];

for (const cond of conditions) {
  const { runs } = await runDeadlineEvents({
    presetPath,
    presetParams: cond.params,
    variant: "drift",
    seeds,
    steps,
    reportEvery,
    eventEvery,
    deadline: baseDeadline,
    regionType,
    regionIndex,
    gateSpan: null,
    corruptFrac: 0.2,
    errGood,
    sdiffGood,
    tailWindow,
  });
  for (const r of runs) rawRows.push(JSON.stringify({ condition: cond.id, ...r }));
  summaryRows.push(summarizeRuns(cond.id, runs, { opStencil: "", opBudgetK: "" }));
}

const opConfigs = [];
for (const opStencil of [0, 1]) {
  for (const opBudgetK of [8, 16, 32]) {
    opConfigs.push({ opStencil, opBudgetK });
  }
}

const opNoDriveSummaries = [];
const opDriveSummaries = [];

for (const cfg of opConfigs) {
  const paramsNoDrive = {
    ...baseParams,
    opCouplingOn: 1,
    sCouplingMode: 1,
    opDriveOnK: 0,
    opStencil: cfg.opStencil,
    opBudgetK: cfg.opBudgetK,
  };
  const paramsDrive = {
    ...baseParams,
    opCouplingOn: 1,
    sCouplingMode: 1,
    opDriveOnK: 1,
    opStencil: cfg.opStencil,
    opBudgetK: cfg.opBudgetK,
  };

  const runNoDrive = await runDeadlineEvents({
    presetPath,
    presetParams: paramsNoDrive,
    variant: "drift",
    seeds,
    steps,
    reportEvery,
    eventEvery,
    deadline: baseDeadline,
    regionType,
    regionIndex,
    gateSpan: null,
    corruptFrac: 0.2,
    errGood,
    sdiffGood,
    tailWindow,
  });
  for (const r of runNoDrive.runs) {
    rawRows.push(JSON.stringify({ condition: "C_op_noKdrive", opStencil: cfg.opStencil, opBudgetK: cfg.opBudgetK, ...r }));
  }
  const summaryNoDrive = summarizeRuns(
    `C_op_noKdrive_s${cfg.opStencil}_b${cfg.opBudgetK}`,
    runNoDrive.runs,
    cfg,
  );
  opNoDriveSummaries.push(summaryNoDrive);
  summaryRows.push(summaryNoDrive);

  const runDrive = await runDeadlineEvents({
    presetPath,
    presetParams: paramsDrive,
    variant: "drift",
    seeds,
    steps,
    reportEvery,
    eventEvery,
    deadline: baseDeadline,
    regionType,
    regionIndex,
    gateSpan: null,
    corruptFrac: 0.2,
    errGood,
    sdiffGood,
    tailWindow,
  });
  for (const r of runDrive.runs) {
    rawRows.push(JSON.stringify({ condition: "D_op_withKdrive", opStencil: cfg.opStencil, opBudgetK: cfg.opBudgetK, ...r }));
  }
  const summaryDrive = summarizeRuns(
    `D_op_withKdrive_s${cfg.opStencil}_b${cfg.opBudgetK}`,
    runDrive.runs,
    cfg,
  );
  opDriveSummaries.push(summaryDrive);
  summaryRows.push(summaryDrive);
}

const bestNoDrive = pickBest(opNoDriveSummaries);
const bestDrive = pickBest(opDriveSummaries);
const legacy = summaryRows.find((r) => r.id === "A_legacy");
const dilution = summaryRows.find((r) => r.id === "B_dilution_only");

summaryRows.push({
  id: "BEST_C",
  ...bestNoDrive,
  note: "BEST_C",
});
summaryRows.push({
  id: "BEST_D",
  ...bestDrive,
  note: "BEST_D",
});

const deltaMissLegacyToDilution = dilution.missFracMean - legacy.missFracMean;
const deltaMissLegacyToBestD = bestDrive.missFracMean - legacy.missFracMean;
const attribution =
  deltaMissLegacyToBestD !== 0
    ? deltaMissLegacyToDilution / deltaMissLegacyToBestD
    : 0;
const explained = deltaMissLegacyToBestD !== 0 && attribution >= 0.7;
const line = `DILUTION_ATTRIBUTION: miss(A)->miss(B)=${deltaMissLegacyToDilution.toFixed(4)} miss(A)->miss(bestD)=${deltaMissLegacyToBestD.toFixed(4)} ratio=${attribution.toFixed(2)} explained=${explained}`;
console.log(line);

const summaryPath = path.join(outDir, "deadline_decomp_summary.csv");
const header = [
  "id",
  "opStencil",
  "opBudgetK",
  "missFracMean",
  "missFracStd",
  "deltaMissVsA",
  "uptimeTailMean",
  "uptimeTailStd",
  "errTailMean",
  "errTailStd",
  "sdiffTailMean",
  "sdiffTailStd",
  "recoveryMean",
  "recoveryP95Mean",
  "epTotalRateMean",
  "deltaEpTotalRateVsA",
  "epClockRateMean",
  "epRepairRateMean",
  "deltaRepairRateVsA",
  "epOpKRateMean",
  "p5MetaToRecoverMean",
  "p5MetaToRecoverSuccessMean",
  "p5MetaToRecoverSuccessP95",
  "p5MetaBeforeMissMean",
  "p5MetaBeforeMissP95",
  "deltaP5MetaToRecoverMeanVsA",
  "p5MetaToRecoverP95",
  "opkToRecoverMean",
  "opkToRecoverSuccessMean",
  "opkToRecoverSuccessP95",
  "opkBeforeMissMean",
  "opkBeforeMissP95",
  "opkToRecoverP95",
  "clockToRecoverSuccessMean",
  "clockToRecoverSuccessP95",
  "clockBeforeMissMean",
  "clockBeforeMissP95",
  "recoveriesCountMean",
  "missesCountMean",
  "repairEfficiencySuccessMean",
  "repairEfficiencySuccessMedian",
  "repairRateMean",
  "deltaRepairRateMeanVsA",
  "opkRateMean",
  "note",
];
const deltaFor = (row, key) => {
  if (!legacy) return "";
  const base = legacy[key];
  if (!Number.isFinite(base) || !Number.isFinite(row[key])) return "";
  return row[key] - base;
};
const lines = [
  header.join(","),
  ...summaryRows.map((row) =>
    [
      row.id,
      row.opStencil ?? "",
      row.opBudgetK ?? "",
      row.missFracMean,
      row.missFracStd,
      deltaFor(row, "missFracMean"),
      row.uptimeTailMean,
      row.uptimeTailStd,
      row.errTailMean,
      row.errTailStd,
      row.sdiffTailMean,
      row.sdiffTailStd,
      row.recoveryMean,
      row.recoveryP95Mean,
      row.epTotalRateMean,
      deltaFor(row, "epTotalRateMean"),
      row.epClockRateMean,
      row.epRepairRateMean,
      deltaFor(row, "epRepairRateMean"),
      row.epOpKRateMean,
      row.p5MetaToRecoverMean,
      row.p5MetaToRecoverSuccessMean,
      row.p5MetaToRecoverSuccessP95,
      row.p5MetaBeforeMissMean,
      row.p5MetaBeforeMissP95,
      deltaFor(row, "p5MetaToRecoverMean"),
      row.p5MetaToRecoverP95,
      row.opkToRecoverMean,
      row.opkToRecoverSuccessMean,
      row.opkToRecoverSuccessP95,
      row.opkBeforeMissMean,
      row.opkBeforeMissP95,
      row.opkToRecoverP95,
      row.clockToRecoverSuccessMean,
      row.clockToRecoverSuccessP95,
      row.clockBeforeMissMean,
      row.clockBeforeMissP95,
      row.recoveriesCountMean,
      row.missesCountMean,
      row.repairEfficiencySuccessMean,
      row.repairEfficiencySuccessMedian,
      row.repairRateMean,
      deltaFor(row, "repairRateMean"),
      row.opkRateMean,
      row.note ?? "",
    ].join(","),
  ),
].join("\n");
fs.writeFileSync(summaryPath, `${lines}\n`);

const rawPath = path.join(outDir, "deadline_decomp_raw.jsonl");
fs.writeFileSync(rawPath, rawRows.join("\n"));
</file>

<file path="scripts/run-deadline-opk-frontier.mjs">
#!/usr/bin/env node
import fs from "node:fs";
import path from "node:path";
import {
  loadWasm,
  mean,
  std,
  readJson,
  runDeadlineEvents,
} from "./deadline-event-utils.mjs";

const rootDir = path.resolve(path.dirname(new URL(import.meta.url).pathname), "..");
const outDir = path.resolve(rootDir, ".tmp", "op_coupling");

function ensureDir(dir) {
  fs.mkdirSync(dir, { recursive: true });
}

function pickPreset() {
  const preferred = path.resolve(rootDir, "scripts/params/clock_code/deadline_fidelity_found.json");
  const fallback = path.resolve(rootDir, "scripts/params/clock_code/code_deadline_gated_clock.json");
  if (fs.existsSync(preferred)) return preferred;
  return fallback;
}

function readBestConfigs() {
  const summaryPath = path.resolve(outDir, "deadline_decomp_summary.csv");
  const defaults = {
    bestC: { opStencil: 1, opBudgetK: 32 },
    bestD: { opStencil: 1, opBudgetK: 16 },
  };
  if (!fs.existsSync(summaryPath)) return defaults;
  const lines = fs.readFileSync(summaryPath, "utf8").trim().split("\n");
  const header = lines.shift()?.split(",") ?? [];
  const idx = Object.fromEntries(header.map((k, i) => [k, i]));
  for (const line of lines) {
    const cols = line.split(",");
    const note = cols[idx.note] ?? "";
    if (note === "BEST_C") {
      defaults.bestC = {
        opStencil: Number(cols[idx.opStencil]),
        opBudgetK: Number(cols[idx.opBudgetK]),
      };
    }
    if (note === "BEST_D") {
      defaults.bestD = {
        opStencil: Number(cols[idx.opStencil]),
        opBudgetK: Number(cols[idx.opBudgetK]),
      };
    }
  }
  return defaults;
}

function summarizeRuns(id, runs, meta) {
  const missFrac = runs.map((r) => r.missFrac);
  const uptimeTail = runs.map((r) => r.uptimeTail);
  const errTail = runs.map((r) => r.errTailMean);
  const epTotal = runs.map((r) => r.epTotalRate);
  const epRepair = runs.map((r) => r.epRepairRate);
  const repairRate = runs.map((r) => r.repairRate ?? 0);
  const p5MetaSuccess = runs.map((r) => r.p5MetaToRecoverSuccessMean ?? 0);

  return {
    id,
    ...meta,
    missFracMean: mean(missFrac),
    missFracStd: std(missFrac),
    uptimeTailMean: mean(uptimeTail),
    uptimeTailStd: std(uptimeTail),
    errTailMean: mean(errTail),
    errTailStd: std(errTail),
    epTotalRateMean: mean(epTotal),
    epTotalRateStd: std(epTotal),
    epRepairRateMean: mean(epRepair),
    repairRateMean: mean(repairRate),
    p5MetaToRecoverSuccessMean: mean(p5MetaSuccess),
  };
}

function paretoFront(points) {
  return points.filter((a) => {
    for (const b of points) {
      if (b === a) continue;
      const noWorse = b.missFracMean <= a.missFracMean && b.epTotalRateMean <= a.epTotalRateMean;
      const strictlyBetter = b.missFracMean < a.missFracMean || b.epTotalRateMean < a.epTotalRateMean;
      if (noWorse && strictlyBetter) return false;
    }
    return true;
  });
}

ensureDir(outDir);
await loadWasm();

const presetPath = pickPreset();
const presetRaw = readJson(presetPath);
const baseParams = presetRaw.params ?? presetRaw;
const baseDeadline = presetRaw.deadline ?? 25_000;
const best = readBestConfigs();

const seeds = [1, 2, 3, 4, 5];
const steps = 500_000;
const reportEvery = 5_000;
const eventEvery = 50_000;
const regionType = "quadrant";
const regionIndex = 2;
const errGood = 0.1;
const sdiffGood = 1.0;
const tailWindow = 200_000;

const pSWriteGrid = [0.2, 0.35, 0.5, 0.65, 0.8, 0.95];
const etaDriveGrid = [0.4, 0.6, 0.8, 1.0];

const modes = [
  { id: "A_legacy", params: { ...baseParams, opCouplingOn: 0, sCouplingMode: 0, opDriveOnK: 0 } },
  { id: "B_dilution_only", params: { ...baseParams, opCouplingOn: 1, sCouplingMode: 0, opDriveOnK: 0 } },
  {
    id: "C_op_noKdrive",
    params: {
      ...baseParams,
      opCouplingOn: 1,
      sCouplingMode: 1,
      opDriveOnK: 0,
      opStencil: best.bestC.opStencil,
      opBudgetK: best.bestC.opBudgetK,
    },
  },
  {
    id: "D_op_withKdrive",
    params: {
      ...baseParams,
      opCouplingOn: 1,
      sCouplingMode: 1,
      opDriveOnK: 1,
      opStencil: best.bestD.opStencil,
      opBudgetK: best.bestD.opBudgetK,
    },
  },
];

const rawRows = [];
const summaryRows = [];

for (const mode of modes) {
  for (const pSWrite of pSWriteGrid) {
    for (const etaDrive of etaDriveGrid) {
      const params = { ...mode.params, pSWrite, etaDrive };
      const result = await runDeadlineEvents({
        presetPath,
        presetParams: params,
        variant: "drift",
        seeds,
        steps,
        reportEvery,
        eventEvery,
        deadline: baseDeadline,
        regionType,
        regionIndex,
        gateSpan: null,
        corruptFrac: 0.2,
        errGood,
        sdiffGood,
        tailWindow,
      });
      for (const run of result.runs) {
        rawRows.push(
          JSON.stringify({
            mode: mode.id,
            seed: run.seed,
            pSWrite,
            etaDrive,
            opStencil: params.opStencil ?? "",
            opBudgetK: params.opBudgetK ?? "",
            opDriveOnK: params.opDriveOnK ?? "",
            ...run,
          }),
        );
      }
      summaryRows.push(
        summarizeRuns(
          `${mode.id}_p${pSWrite}_e${etaDrive}`,
          result.runs,
          {
            mode: mode.id,
            pSWrite,
            etaDrive,
            opStencil: params.opStencil ?? "",
            opBudgetK: params.opBudgetK ?? "",
            opDriveOnK: params.opDriveOnK ?? "",
          },
        ),
      );
    }
  }
}

const summaryPath = path.join(outDir, "deadline_frontier_summary.csv");
const header = [
  "id",
  "mode",
  "pSWrite",
  "etaDrive",
  "opStencil",
  "opBudgetK",
  "opDriveOnK",
  "missFracMean",
  "missFracStd",
  "uptimeTailMean",
  "uptimeTailStd",
  "errTailMean",
  "errTailStd",
  "epTotalRateMean",
  "epTotalRateStd",
  "epRepairRateMean",
  "repairRateMean",
  "p5MetaToRecoverSuccessMean",
];
const lines = [
  header.join(","),
  ...summaryRows.map((row) =>
    [
      row.id,
      row.mode,
      row.pSWrite,
      row.etaDrive,
      row.opStencil ?? "",
      row.opBudgetK ?? "",
      row.opDriveOnK ?? "",
      row.missFracMean,
      row.missFracStd,
      row.uptimeTailMean,
      row.uptimeTailStd,
      row.errTailMean,
      row.errTailStd,
      row.epTotalRateMean,
      row.epTotalRateStd,
      row.epRepairRateMean,
      row.repairRateMean,
      row.p5MetaToRecoverSuccessMean,
    ].join(","),
  ),
].join("\n");
fs.writeFileSync(summaryPath, `${lines}\n`);
fs.writeFileSync(path.join(outDir, "deadline_frontier_raw.jsonl"), rawRows.join("\n"));

const frontierRows = [];
for (const mode of modes) {
  const subset = summaryRows.filter((row) => row.mode === mode.id);
  const front = paretoFront(subset);
  frontierRows.push(...front);
}
const frontHeader = [
  "mode",
  "pSWrite",
  "etaDrive",
  "opStencil",
  "opBudgetK",
  "opDriveOnK",
  "missFracMean",
  "epTotalRateMean",
  "uptimeTailMean",
  "errTailMean",
];
const frontLines = [
  frontHeader.join(","),
  ...frontierRows.map((row) =>
    [
      row.mode,
      row.pSWrite,
      row.etaDrive,
      row.opStencil ?? "",
      row.opBudgetK ?? "",
      row.opDriveOnK ?? "",
      row.missFracMean,
      row.epTotalRateMean,
      row.uptimeTailMean,
      row.errTailMean,
    ].join(","),
  ),
].join("\n");
fs.writeFileSync(path.join(outDir, "deadline_frontier_points.csv"), `${frontLines}\n`);

const isoThresholds = [0.05, 0.08, 0.1, 0.12];
const isoRows = [];
for (const mode of modes) {
  for (const tau of isoThresholds) {
    const candidates = summaryRows.filter((row) => row.mode === mode.id && row.missFracMean <= tau);
    if (candidates.length === 0) {
      isoRows.push({ mode: mode.id, tau, epTotalRate: null, pSWrite: null, etaDrive: null });
      continue;
    }
    const best = candidates.reduce(
      (acc, row) => (row.epTotalRateMean < acc.epTotalRateMean ? row : acc),
      candidates[0],
    );
    isoRows.push({
      mode: mode.id,
      tau,
      epTotalRate: best.epTotalRateMean,
      pSWrite: best.pSWrite,
      etaDrive: best.etaDrive,
    });
  }
}
const isoPath = path.join(outDir, "deadline_frontier_iso_miss.csv");
const isoLines = [
  "mode,tau,epTotalRate,pSWrite,etaDrive",
  ...isoRows.map((row) =>
    [
      row.mode,
      row.tau,
      row.epTotalRate ?? "",
      row.pSWrite ?? "",
      row.etaDrive ?? "",
    ].join(","),
  ),
].join("\n");
fs.writeFileSync(isoPath, `${isoLines}\n`);

console.log("ISO_MISS_TABLE");
for (const row of isoRows) {
  const ep = row.epTotalRate === null ? "NA" : row.epTotalRate.toFixed(6);
  console.log(`${row.mode} tau=${row.tau} ep=${ep} pSWrite=${row.pSWrite ?? "NA"} etaDrive=${row.etaDrive ?? "NA"}`);
}
</file>

<file path="scripts/run-deadline-opk-motif-compare.mjs">
#!/usr/bin/env node
import fs from "node:fs";
import path from "node:path";
import { mean, std, readJson, parseSeedList } from "./deadline-event-utils.mjs";
import { runDeadlineOpkMotifEvents } from "./run-deadline-opk-motif-events.mjs";
import { coarseEPSmoothed, asymmetryScore, jsDivergence } from "./opk-motif-basis.mjs";

const rootDir = path.resolve(path.dirname(new URL(import.meta.url).pathname), "..");
const outDirState = path.resolve(rootDir, ".tmp", "motif_pressure_v4");
const outDirMove = path.resolve(rootDir, ".tmp", "motif_pressure_v5");
const outDirP5 = path.resolve(rootDir, ".tmp", "motif_pressure_v6");
let outDir = outDirState;

function ensureDir(dir) {
  fs.mkdirSync(dir, { recursive: true });
}

function parseMoveEdgesCounts(pathname) {
  if (!fs.existsSync(pathname)) return new Map();
  const raw = fs.readFileSync(pathname, "utf8").trim();
  if (!raw) return new Map();
  const lines = raw.split(/\r?\n/);
  const counts = new Map();
  const epSum = new Map();
  const epAbsSum = new Map();
  for (let i = 1; i < lines.length; i += 1) {
    const [fromIdx, toIdx, count, epSumVal, epAbsVal] = lines[i].split(",");
    const key = `${fromIdx}->${toIdx}`;
    counts.set(key, (counts.get(key) ?? 0) + Number(count ?? 0));
    epSum.set(key, (epSum.get(key) ?? 0) + Number(epSumVal ?? 0));
    epAbsSum.set(key, (epAbsSum.get(key) ?? 0) + Number(epAbsVal ?? 0));
  }
  return { counts, epSum, epAbsSum };
}

function totalMovesFromCounts(counts) {
  let total = 0;
  for (const count of counts.values()) total += count;
  return total;
}

function topEdgeMass(counts, topN) {
  const total = totalMovesFromCounts(counts);
  if (total === 0) return 0;
  const values = Array.from(counts.values()).sort((a, b) => b - a);
  const topSum = values.slice(0, topN).reduce((acc, v) => acc + v, 0);
  return topSum / total;
}

function symmetryGap(counts) {
  const pairMap = new Map();
  for (const [key, count] of counts.entries()) {
    const [fromStr, toStr] = key.split("->");
    const from = Number(fromStr);
    const to = Number(toStr);
    if (Number.isNaN(from) || Number.isNaN(to) || from === to) continue;
    const a = Math.min(from, to);
    const b = Math.max(from, to);
    const pairKey = `${a}|${b}`;
    const prev = pairMap.get(pairKey) ?? { ab: 0, ba: 0, a, b };
    if (from === a) prev.ab += count;
    else prev.ba += count;
    pairMap.set(pairKey, prev);
  }
  let num = 0;
  let denom = 0;
  for (const pair of pairMap.values()) {
    num += Math.abs(pair.ab - pair.ba);
    denom += pair.ab + pair.ba;
  }
  return denom > 0 ? num / denom : 0;
}

function coarseEPFromCounts(counts, alpha = 0.5) {
  const pairMap = new Map();
  for (const [key, count] of counts.entries()) {
    const [fromStr, toStr] = key.split("->");
    const from = Number(fromStr);
    const to = Number(toStr);
    if (Number.isNaN(from) || Number.isNaN(to) || from === to) continue;
    const a = Math.min(from, to);
    const b = Math.max(from, to);
    const pairKey = `${a}|${b}`;
    const prev = pairMap.get(pairKey) ?? { ab: 0, ba: 0, a, b };
    if (from === a) prev.ab += count;
    else prev.ba += count;
    pairMap.set(pairKey, prev);
  }
  let total = 0;
  for (const pair of pairMap.values()) {
    const nij = pair.ab;
    const nji = pair.ba;
    const c1 = nij + alpha;
    const c2 = nji + alpha;
    total += (nij - nji) * Math.log(c1 / c2);
  }
  return total;
}

function edgeAggKey(condition, key) {
  return `${condition}|${key}`;
}

function parseP5Counts(pathname) {
  if (!fs.existsSync(pathname)) return { counts: new Map(), epSum: new Map(), epAbsSum: new Map() };
  const raw = fs.readFileSync(pathname, "utf8").trim();
  if (!raw) return { counts: new Map(), epSum: new Map(), epAbsSum: new Map() };
  const lines = raw.split(/\r?\n/);
  const counts = new Map();
  const epSum = new Map();
  const epAbsSum = new Map();
  for (let i = 1; i < lines.length; i += 1) {
    const [motifId, count, epSumVal, epAbsVal] = lines[i].split(",");
    const key = String(motifId);
    counts.set(key, (counts.get(key) ?? 0) + Number(count ?? 0));
    epSum.set(key, (epSum.get(key) ?? 0) + Number(epSumVal ?? 0));
    epAbsSum.set(key, (epAbsSum.get(key) ?? 0) + Number(epAbsVal ?? 0));
  }
  return { counts, epSum, epAbsSum };
}

function parseP5Transitions(pathname) {
  if (!fs.existsSync(pathname)) return new Map();
  const raw = fs.readFileSync(pathname, "utf8").trim();
  if (!raw) return new Map();
  const lines = raw.split(/\r?\n/);
  const counts = new Map();
  for (let i = 1; i < lines.length; i += 1) {
    const [fromMotif, toMotif, count] = lines[i].split(",");
    const key = `${fromMotif}->${toMotif}`;
    counts.set(key, (counts.get(key) ?? 0) + Number(count ?? 0));
  }
  return counts;
}

function entropyFromCountsMap(counts) {
  let total = 0;
  for (const val of counts.values()) total += val;
  if (total === 0) return 0;
  let h = 0;
  for (const val of counts.values()) {
    if (val <= 0) continue;
    const p = val / total;
    h += -p * Math.log(p);
  }
  return h;
}

function topMassFromCounts(counts, topN) {
  const total = totalMovesFromCounts(counts);
  if (total === 0) return 0;
  const values = Array.from(counts.values()).sort((a, b) => b - a);
  const topSum = values.slice(0, topN).reduce((acc, v) => acc + v, 0);
  return topSum / total;
}

async function runMoveEdgesCompare({ presetPath, seeds, eventEvery, deadline }) {
  outDir = outDirMove;
  ensureDir(outDir);

  const conditions = [
    { id: "A", label: "A_legacy", sets: ["opCouplingOn=0", "sCouplingMode=0", "opDriveOnK=0"] },
    { id: "B", label: "B_op_noKdrive", sets: ["opCouplingOn=1", "sCouplingMode=1", "opDriveOnK=0"] },
    { id: "C", label: "C_op_withKdrive", sets: ["opCouplingOn=1", "sCouplingMode=1", "opDriveOnK=1"] },
  ];

  const perCondition = new Map();
  const edgeAgg = new Map();

  for (const cond of conditions) {
    const rows = [];
    for (const seed of seeds) {
      await runDeadlineOpkMotifEvents({
        presetPath,
        seed,
        condition: cond.id,
        outDir,
        motifMode: "move_edges",
        eventEvery,
        deadline,
        sets: cond.sets,
      });

      const seedDir = path.join(outDir, cond.id, `seed_${seed}`);
      const summaryPath = path.join(seedDir, "move_edges_summary.json");
      const summary = readJson(summaryPath);
      const hazCountsPath = path.join(seedDir, "move_edges_counts_hazard.csv");
      const outCountsPath = path.join(seedDir, "move_edges_counts_outside.csv");
      const hazCounts = parseMoveEdgesCounts(hazCountsPath);
      const outCounts = parseMoveEdgesCounts(outCountsPath);

      const totalMovesHazard = summary.totalMovesHazard ?? totalMovesFromCounts(hazCounts.counts);
      const totalMovesOutside = summary.totalMovesOutside ?? totalMovesFromCounts(outCounts.counts);
      const totalEpHazard = summary.totalEpHazard ?? 0;
      const totalEpOutside = summary.totalEpOutside ?? 0;
      const uniqueEdgesHazard = summary.uniqueEdgesHazard ?? hazCounts.counts.size;
      const uniqueEdgesOutside = summary.uniqueEdgesOutside ?? outCounts.counts.size;

      rows.push({
        totalMovesHazard,
        totalMovesOutside,
        totalEpHazard,
        totalEpOutside,
        uniqueEdgesHazard,
        uniqueEdgesOutside,
        epPerMoveHazard: totalMovesHazard > 0 ? totalEpHazard / totalMovesHazard : 0,
        epPerMoveOutside: totalMovesOutside > 0 ? totalEpOutside / totalMovesOutside : 0,
        topEdgeMass10Hazard: topEdgeMass(hazCounts.counts, 10),
        topEdgeMass10Outside: topEdgeMass(outCounts.counts, 10),
        symmetryGapHazard: symmetryGap(hazCounts.counts),
        symmetryGapOutside: symmetryGap(outCounts.counts),
        coarseEPHazard: coarseEPFromCounts(hazCounts.counts, 0.5),
        coarseEPOutside: coarseEPFromCounts(outCounts.counts, 0.5),
      });

      for (const [key, count] of hazCounts.counts.entries()) {
        const aggKey = edgeAggKey(cond.label, key);
        const prev = edgeAgg.get(aggKey) ?? { condition: cond.label, key, countSum: 0, epSum: 0 };
        prev.countSum += count;
        prev.epSum += hazCounts.epSum.get(key) ?? 0;
        edgeAgg.set(aggKey, prev);
      }
    }
    perCondition.set(cond.label, rows);
  }

  const summaryRows = [];
  for (const [condition, rows] of perCondition.entries()) {
    const seedsCount = rows.length;
    const toMetric = (key) => rows.map((r) => r[key]);
    summaryRows.push({
      condition,
      seeds: seedsCount,
      totalMovesHazardMean: mean(toMetric("totalMovesHazard")),
      totalMovesHazardStd: std(toMetric("totalMovesHazard")),
      uniqueEdgesHazardMean: mean(toMetric("uniqueEdgesHazard")),
      uniqueEdgesHazardStd: std(toMetric("uniqueEdgesHazard")),
      totalEpHazardMean: mean(toMetric("totalEpHazard")),
      totalEpHazardStd: std(toMetric("totalEpHazard")),
      epPerMoveHazardMean: mean(toMetric("epPerMoveHazard")),
      topEdgeMass10HazardMean: mean(toMetric("topEdgeMass10Hazard")),
      symmetryGapHazardMean: mean(toMetric("symmetryGapHazard")),
      coarseEPHazardMean: mean(toMetric("coarseEPHazard")),
      totalMovesOutsideMean: mean(toMetric("totalMovesOutside")),
      totalMovesOutsideStd: std(toMetric("totalMovesOutside")),
      uniqueEdgesOutsideMean: mean(toMetric("uniqueEdgesOutside")),
      uniqueEdgesOutsideStd: std(toMetric("uniqueEdgesOutside")),
      totalEpOutsideMean: mean(toMetric("totalEpOutside")),
      totalEpOutsideStd: std(toMetric("totalEpOutside")),
      epPerMoveOutsideMean: mean(toMetric("epPerMoveOutside")),
      topEdgeMass10OutsideMean: mean(toMetric("topEdgeMass10Outside")),
      symmetryGapOutsideMean: mean(toMetric("symmetryGapOutside")),
      coarseEPOutsideMean: mean(toMetric("coarseEPOutside")),
    });
  }

  const summaryHeader = [
    "condition",
    "seeds",
    "totalMovesHazardMean",
    "totalMovesHazardStd",
    "uniqueEdgesHazardMean",
    "uniqueEdgesHazardStd",
    "totalEpHazardMean",
    "totalEpHazardStd",
    "epPerMoveHazardMean",
    "topEdgeMass10HazardMean",
    "symmetryGapHazardMean",
    "coarseEPHazardMean",
    "totalMovesOutsideMean",
    "totalMovesOutsideStd",
    "uniqueEdgesOutsideMean",
    "uniqueEdgesOutsideStd",
    "totalEpOutsideMean",
    "totalEpOutsideStd",
    "epPerMoveOutsideMean",
    "topEdgeMass10OutsideMean",
    "symmetryGapOutsideMean",
    "coarseEPOutsideMean",
  ];
  const summaryLines = [summaryHeader.join(",")];
  for (const row of summaryRows) {
    summaryLines.push(summaryHeader.map((key) => row[key]).join(","));
  }
  fs.writeFileSync(path.join(outDir, "compare_move_edges_summary.csv"), summaryLines.join("\n") + "\n");

  const edgeRows = [];
  for (const entry of edgeAgg.values()) {
    const [fromIdx, toIdx] = entry.key.split("->");
    const seedsCount = perCondition.get(entry.condition)?.length ?? 1;
    edgeRows.push({
      condition: entry.condition,
      fromIdx,
      toIdx,
      countMean: entry.countSum / seedsCount,
      epSumMean: entry.epSum / seedsCount,
    });
  }
  edgeRows.sort((a, b) => b.countMean - a.countMean);
  const topEdges = edgeRows.slice(0, 30);
  const edgeHeader = ["condition", "fromIdx", "toIdx", "countMean", "epSumMean"];
  const edgeLines = [edgeHeader.join(",")];
  for (const row of topEdges) {
    edgeLines.push(edgeHeader.map((key) => row[key]).join(","));
  }
  fs.writeFileSync(path.join(outDir, "top_edges_hazard.csv"), edgeLines.join("\n") + "\n");

  const summaryByCond = new Map(summaryRows.map((row) => [row.condition, row]));
  const bRow = summaryByCond.get("B_op_noKdrive");
  const cRow = summaryByCond.get("C_op_withKdrive");
  const signalOk =
    bRow &&
    cRow &&
    bRow.totalMovesHazardMean >= 5000 &&
    cRow.totalMovesHazardMean >= 5000 &&
    (bRow.symmetryGapHazardMean >= 0.05 ||
      bRow.coarseEPHazardMean >= 50 ||
      cRow.symmetryGapHazardMean >= 0.05 ||
      cRow.coarseEPHazardMean >= 50);

  console.log(signalOk ? "MOVE_EDGE_MOTIF_SIGNAL_OK" : "MOVE_EDGE_MOTIF_SIGNAL_TOO_WEAK");
}

async function runP5ActionsCompare({ presetPath, seeds, eventEvery, deadline }) {
  outDir = outDirP5;
  ensureDir(outDir);

  const conditions = [
    { id: "A", label: "A_legacy", sets: ["opCouplingOn=0", "sCouplingMode=0", "opDriveOnK=0"] },
    { id: "B", label: "B_op_noKdrive", sets: ["opCouplingOn=1", "sCouplingMode=1", "opDriveOnK=0"] },
    { id: "C", label: "C_op_withKdrive", sets: ["opCouplingOn=1", "sCouplingMode=1", "opDriveOnK=1"] },
  ];

  const perCondition = new Map();
  const motifAggHazard = new Map();
  const motifAggOutside = new Map();

  for (const cond of conditions) {
    const rows = [];
    for (const seed of seeds) {
      await runDeadlineOpkMotifEvents({
        presetPath,
        seed,
        condition: cond.id,
        outDir,
        motifMode: "p5_actions",
        eventEvery,
        deadline,
        sets: cond.sets,
      });

      const seedDir = path.join(outDir, cond.id, `seed_${seed}`);
      const summary = readJson(path.join(seedDir, "p5_actions_summary.json"));
      const hazCounts = parseP5Counts(path.join(seedDir, "p5_actions_counts_hazard.csv"));
      const outCounts = parseP5Counts(path.join(seedDir, "p5_actions_counts_outside.csv"));
      const hazTrans = parseP5Transitions(path.join(seedDir, "p5_actions_transitions_hazard.csv"));
      const outTrans = parseP5Transitions(path.join(seedDir, "p5_actions_transitions_outside.csv"));

      const totalMovesHazard = summary.totalMovesHazard ?? totalMovesFromCounts(hazCounts.counts);
      const totalMovesOutside = summary.totalMovesOutside ?? totalMovesFromCounts(outCounts.counts);
      const totalEpHazard = summary.totalEpHazard ?? 0;
      const totalEpOutside = summary.totalEpOutside ?? 0;
      const uniqueMotifsHazard = summary.uniqueMotifsHazard ?? hazCounts.counts.size;
      const uniqueMotifsOutside = summary.uniqueMotifsOutside ?? outCounts.counts.size;
      const epPerMoveHazard = totalMovesHazard > 0 ? totalEpHazard / totalMovesHazard : 0;
      const epPerMoveOutside = totalMovesOutside > 0 ? totalEpOutside / totalMovesOutside : 0;
      const entropyHazard = entropyFromCountsMap(hazCounts.counts);
      const entropyOutside = entropyFromCountsMap(outCounts.counts);
      const topMotifMass10Hazard = topMassFromCounts(hazCounts.counts, 10);
      const topMotifMass10Outside = topMassFromCounts(outCounts.counts, 10);
      const symmetryGapHazard = symmetryGap(hazTrans);
      const symmetryGapOutside = symmetryGap(outTrans);
      const coarseEPHazard = coarseEPFromCounts(hazTrans, 0.5);
      const coarseEPOutside = coarseEPFromCounts(outTrans, 0.5);

      rows.push({
        totalMovesHazard,
        totalMovesOutside,
        totalEpHazard,
        totalEpOutside,
        uniqueMotifsHazard,
        uniqueMotifsOutside,
        epPerMoveHazard,
        epPerMoveOutside,
        entropyHazard,
        entropyOutside,
        topMotifMass10Hazard,
        topMotifMass10Outside,
        symmetryGapHazard,
        symmetryGapOutside,
        coarseEPHazard,
        coarseEPOutside,
      });

      for (const [motifId, count] of hazCounts.counts.entries()) {
        const key = `${cond.label}|${motifId}`;
        const prev = motifAggHazard.get(key) ?? { condition: cond.label, motifId, countSum: 0, epSum: 0 };
        prev.countSum += count;
        prev.epSum += hazCounts.epSum.get(motifId) ?? 0;
        motifAggHazard.set(key, prev);
      }
      for (const [motifId, count] of outCounts.counts.entries()) {
        const key = `${cond.label}|${motifId}`;
        const prev = motifAggOutside.get(key) ?? { condition: cond.label, motifId, countSum: 0, epSum: 0 };
        prev.countSum += count;
        prev.epSum += outCounts.epSum.get(motifId) ?? 0;
        motifAggOutside.set(key, prev);
      }
    }
    perCondition.set(cond.label, rows);
  }

  const summaryRows = [];
  for (const [condition, rows] of perCondition.entries()) {
    const seedsCount = rows.length;
    const toMetric = (key) => rows.map((r) => r[key]);
    summaryRows.push({
      condition,
      seeds: seedsCount,
      totalMovesHazardMean: mean(toMetric("totalMovesHazard")),
      totalMovesHazardStd: std(toMetric("totalMovesHazard")),
      uniqueMotifsHazardMean: mean(toMetric("uniqueMotifsHazard")),
      uniqueMotifsHazardStd: std(toMetric("uniqueMotifsHazard")),
      totalEpHazardMean: mean(toMetric("totalEpHazard")),
      totalEpHazardStd: std(toMetric("totalEpHazard")),
      epPerMoveHazardMean: mean(toMetric("epPerMoveHazard")),
      entropyHazardMean: mean(toMetric("entropyHazard")),
      topMotifMass10HazardMean: mean(toMetric("topMotifMass10Hazard")),
      symmetryGapHazardMean: mean(toMetric("symmetryGapHazard")),
      coarseEPHazardMean: mean(toMetric("coarseEPHazard")),
      totalMovesOutsideMean: mean(toMetric("totalMovesOutside")),
      totalMovesOutsideStd: std(toMetric("totalMovesOutside")),
      uniqueMotifsOutsideMean: mean(toMetric("uniqueMotifsOutside")),
      uniqueMotifsOutsideStd: std(toMetric("uniqueMotifsOutside")),
      totalEpOutsideMean: mean(toMetric("totalEpOutside")),
      totalEpOutsideStd: std(toMetric("totalEpOutside")),
      epPerMoveOutsideMean: mean(toMetric("epPerMoveOutside")),
      entropyOutsideMean: mean(toMetric("entropyOutside")),
      topMotifMass10OutsideMean: mean(toMetric("topMotifMass10Outside")),
      symmetryGapOutsideMean: mean(toMetric("symmetryGapOutside")),
      coarseEPOutsideMean: mean(toMetric("coarseEPOutside")),
    });
  }

  const summaryHeader = [
    "condition",
    "seeds",
    "totalMovesHazardMean",
    "totalMovesHazardStd",
    "uniqueMotifsHazardMean",
    "uniqueMotifsHazardStd",
    "totalEpHazardMean",
    "totalEpHazardStd",
    "epPerMoveHazardMean",
    "entropyHazardMean",
    "topMotifMass10HazardMean",
    "symmetryGapHazardMean",
    "coarseEPHazardMean",
    "totalMovesOutsideMean",
    "totalMovesOutsideStd",
    "uniqueMotifsOutsideMean",
    "uniqueMotifsOutsideStd",
    "totalEpOutsideMean",
    "totalEpOutsideStd",
    "epPerMoveOutsideMean",
    "entropyOutsideMean",
    "topMotifMass10OutsideMean",
    "symmetryGapOutsideMean",
    "coarseEPOutsideMean",
  ];
  const summaryLines = [summaryHeader.join(",")];
  for (const row of summaryRows) {
    summaryLines.push(summaryHeader.map((key) => row[key]).join(","));
  }
  fs.writeFileSync(path.join(outDir, "compare_p5_actions_summary.csv"), summaryLines.join("\n") + "\n");

  const emitTopMotifs = (aggMap, filename) => {
    const rows = [];
    for (const entry of aggMap.values()) {
      const seedsCount = perCondition.get(entry.condition)?.length ?? 1;
      rows.push({
        condition: entry.condition,
        motifId: entry.motifId,
        countMean: entry.countSum / seedsCount,
        epPerMoveMean: entry.countSum > 0 ? entry.epSum / entry.countSum : 0,
      });
    }
    const outRows = [];
    const byCond = new Map();
    for (const row of rows) {
      const list = byCond.get(row.condition) ?? [];
      list.push(row);
      byCond.set(row.condition, list);
    }
    for (const [condition, list] of byCond.entries()) {
      list.sort((a, b) => b.countMean - a.countMean);
      for (const row of list.slice(0, 10)) {
        outRows.push({ ...row, condition });
      }
    }
    const header = ["condition", "motifId", "countMean", "epPerMoveMean"];
    const lines = [header.join(",")];
    for (const row of outRows) {
      lines.push(header.map((key) => row[key]).join(","));
    }
    fs.writeFileSync(path.join(outDir, filename), lines.join("\n") + "\n");
  };

  emitTopMotifs(motifAggHazard, "top_p5_motifs_hazard.csv");
  emitTopMotifs(motifAggOutside, "top_p5_motifs_outside.csv");

  const summaryByCond = new Map(summaryRows.map((row) => [row.condition, row]));
  const bRow = summaryByCond.get("B_op_noKdrive");
  const cRow = summaryByCond.get("C_op_withKdrive");
  let verdict = "P5_ACTION_MOTIF_SIGNAL_TOO_WEAK";
  if (
    !bRow ||
    !cRow ||
    bRow.totalMovesHazardMean < 1000 ||
    cRow.totalMovesHazardMean < 1000
  ) {
    verdict = "P5_ACTIONS_TOO_SPARSE";
  } else if (
    bRow.coarseEPHazardMean >= 50 ||
    bRow.symmetryGapHazardMean >= 0.03 ||
    cRow.coarseEPHazardMean >= 50 ||
    cRow.symmetryGapHazardMean >= 0.03
  ) {
    verdict = "P5_ACTION_MOTIF_SIGNAL_PRESENT";
  }
  console.log(verdict);
}

function pickPreset() {
  const tuned = path.resolve(rootDir, "scripts/params/op_motifs_selection/selection_base_tuned.json");
  const preferred = path.resolve(rootDir, "scripts/params/clock_code/deadline_fidelity_found.json");
  const fallback = path.resolve(rootDir, "scripts/params/clock_code/code_deadline_gated_clock.json");
  if (fs.existsSync(tuned)) return tuned;
  if (fs.existsSync(preferred)) return preferred;
  return fallback;
}

function meanStd(values) {
  const nums = values.filter((v) => Number.isFinite(v));
  return {
    mean: nums.length ? mean(nums) : null,
    std: nums.length ? std(nums) : null,
  };
}

function safeMean(values) {
  const nums = values.filter((v) => Number.isFinite(v));
  return nums.length ? mean(nums) : null;
}

function addPairsToMap(target, pairs) {
  for (const [key, count] of pairs ?? []) {
    target.set(key, (target.get(key) ?? 0) + count);
  }
}

function pairsToMap(pairs) {
  const map = new Map();
  for (const [key, count] of pairs ?? []) {
    map.set(String(key), (map.get(key) ?? 0) + count);
  }
  return map;
}

function totalTransitions(counts) {
  let total = 0;
  for (const [key, count] of counts.entries()) {
    const [from, to] = key.split("|");
    if (from === to) continue;
    total += count;
  }
  return total;
}

function edgesFromCounts(counts, alpha = 0.5) {
  const edges = [];
  for (const [key, c] of counts.entries()) {
    const [i, j] = key.split("|");
    if (i === j) continue;
    const rev = counts.get(`${j}|${i}`) ?? 0;
    const netJ = c - rev;
    const c1 = c + alpha;
    const c2 = rev + alpha;
    const epEdge = (c1 - c2) * Math.log(c1 / c2);
    edges.push({ i, j, count_ij: c, count_ji: rev, netJ, epEdge });
  }
  return edges;
}

function entropyFromCounts(counts) {
  let total = 0;
  for (const v of counts.values()) total += v;
  if (total === 0) return 0;
  let h = 0;
  for (const v of counts.values()) {
    if (v <= 0) continue;
    const p = v / total;
    h += -p * Math.log(p);
  }
  return h;
}

function summarizeTransitions(counts) {
  const totalTrans = totalTransitions(counts);
  const coarseEP = coarseEPSmoothed(counts, 0.5);
  const asym = asymmetryScore(counts);
  const edges = edgesFromCounts(counts, 0.5);
  let topEdgeNetJ = 0;
  let topEdgeEP = 0;
  for (const edge of edges) {
    topEdgeNetJ = Math.max(topEdgeNetJ, Math.abs(edge.netJ));
    topEdgeEP = Math.max(topEdgeEP, Math.abs(edge.epEdge));
  }
  return {
    totalTrans,
    coarseEP,
    coarseEP_perTrans: totalTrans > 0 ? coarseEP / totalTrans : 0,
    asym_perTrans: asym,
    topEdgeNetJ,
    topEdgeEP,
    edges,
  };
}

function parseEdgeCsv(pathname) {
  if (!fs.existsSync(pathname)) return [];
  const raw = fs.readFileSync(pathname, "utf8").trim();
  if (!raw) return [];
  const lines = raw.split(/\r?\n/);
  const header = lines[0].split(",");
  const rows = [];
  for (let i = 1; i < lines.length; i += 1) {
    const cols = lines[i].split(",");
    const row = {};
    header.forEach((key, idx) => {
      row[key] = cols[idx];
    });
    rows.push(row);
  }
  return rows;
}

function aggregateEdgeRows(rows) {
  const agg = new Map();
  for (const row of rows) {
    const key = [row.condition, row.region, row.family, row.from, row.to].join("|");
    const prev = agg.get(key) ?? {
      condition: row.condition,
      region: row.region,
      family: row.family,
      from: row.from,
      to: row.to,
      count: 0,
      countRev: 0,
      epRepairSum: 0,
      epOpKSum: 0,
      epTotalSum: 0,
    };
    prev.count += Number(row.count ?? 0);
    prev.countRev += Number(row.countRev ?? 0);
    prev.epRepairSum += Number(row.epRepairSum ?? 0);
    prev.epOpKSum += Number(row.epOpKSum ?? 0);
    prev.epTotalSum += Number(row.epTotalSum ?? 0);
    agg.set(key, prev);
  }
  const out = [];
  for (const row of agg.values()) {
    out.push({
      ...row,
      epRepairPerTrans: row.count > 0 ? row.epRepairSum / row.count : 0,
      epOpKPerTrans: row.count > 0 ? row.epOpKSum / row.count : 0,
      epTotalPerTrans: row.count > 0 ? row.epTotalSum / row.count : 0,
    });
  }
  return out;
}

async function tuneDeadline({ presetPath, baseParams, baseDeadline, steps, reportEvery, eventEvery, opBinsMode, gateConditioned, gateCheckEvery }) {
  const targetMin = 0.05;
  const targetMax = 0.7;
  const seeds = [1, 2, 3];
  let deadline = baseDeadline;
  let missMean = null;
  let attempts = 0;
  while (attempts < 6) {
    const misses = [];
    for (const seed of seeds) {
      const result = await runDeadlineOpkMotifEvents({
        presetPath,
        seed,
        condition: "A",
        outDir: path.join(outDir, "tune"),
        steps,
        reportEvery,
        eventEvery,
        deadline,
        regionType: baseParams.repairGateMode === 1 ? "stripe" : "quadrant",
        regionIndex: 2,
        gateSpan: baseParams.repairGateSpan ?? 1,
        corruptFrac: 0.2,
        errGood: 0.1,
        sdiffGood: 1.0,
        tailWindow: 200_000,
        opBinsMode,
        gateConditioned,
        gateCheckEvery,
        sets: ["opCouplingOn=0", "sCouplingMode=0"],
      });
      misses.push(result.summary.missFrac);
    }
    missMean = mean(misses);
    if (missMean >= targetMin && missMean <= targetMax) break;
    if (missMean < targetMin) {
      deadline = Math.max(1_000, Math.floor(deadline * 0.7));
    } else if (missMean > targetMax) {
      deadline = Math.ceil(deadline * 1.3);
    }
    attempts += 1;
  }
  return { deadline, missMean, attempts };
}

const motifMode = process.argv.includes("--motifMode")
  ? process.argv[process.argv.indexOf("--motifMode") + 1]
  : "state";

const basis = process.argv.includes("--basis")
  ? process.argv[process.argv.indexOf("--basis") + 1]
  : "v2";
const opBinsMode = process.argv.includes("--opBinsMode")
  ? Number(process.argv[process.argv.indexOf("--opBinsMode") + 1])
  : 2;
const gateConditioned = process.argv.includes("--gateConditioned")
  ? Number(process.argv[process.argv.indexOf("--gateConditioned") + 1])
  : 1;
const gateCheckEvery = process.argv.includes("--gateCheckEvery")
  ? Number(process.argv[process.argv.indexOf("--gateCheckEvery") + 1])
  : 5000;

const presetPath = pickPreset();
if (motifMode === "p5_actions") {
  const seeds = parseSeedList(process.env.SEEDS ?? "1,2,3,4,5,6,7,8,9,10");
  const presetRaw = readJson(presetPath);
  await runP5ActionsCompare({
    presetPath,
    seeds,
    eventEvery: presetRaw.eventEvery ?? 50_000,
    deadline: presetRaw.deadline ?? 25_000,
  });
  process.exit(0);
}
if (motifMode === "move_edges") {
  const seeds = parseSeedList(process.env.SEEDS ?? "1,2,3,4,5,6,7,8,9,10");
  const presetRaw = readJson(presetPath);
  await runMoveEdgesCompare({
    presetPath,
    seeds,
    eventEvery: presetRaw.eventEvery ?? 50_000,
    deadline: presetRaw.deadline ?? 25_000,
  });
  process.exit(0);
}

outDir = outDirState;
ensureDir(outDir);
const presetRaw = readJson(presetPath);
const baseParams = presetRaw.params ?? presetRaw;
const baseDeadline = presetRaw.deadline ?? 25_000;
const steps = presetRaw.steps ?? 2_000_000;
const reportEvery = presetRaw.reportEvery ?? 5_000;
const eventEvery = presetRaw.eventEvery ?? 50_000;

const tuned = await tuneDeadline({
  presetPath,
  baseParams,
  baseDeadline,
  steps,
  reportEvery,
  eventEvery,
  opBinsMode,
  gateConditioned,
  gateCheckEvery,
});

const tunedPreset = {
  params: baseParams,
  deadline: tuned.deadline,
  sourcePreset: presetPath,
  notes: `auto-tuned deadline for missFrac band (attempts=${tuned.attempts}, missMean=${tuned.missMean})`,
};
const tunedDir = path.resolve(rootDir, "scripts/params/op_motifs_selection");
ensureDir(tunedDir);
const tunedPath = path.join(tunedDir, "selection_base_tuned.json");
fs.writeFileSync(tunedPath, JSON.stringify(tunedPreset, null, 2));

const seeds = parseSeedList(process.env.SEEDS ?? "1,2,3,4,5,6,7,8,9,10");
const conditions = [
  { id: "A_legacy", condition: "A", overrides: { opCouplingOn: 0, sCouplingMode: 0 } },
  { id: "B_op_noKdrive", condition: "B", overrides: { opCouplingOn: 1, sCouplingMode: 1, opDriveOnK: 0 } },
  { id: "C_op_withKdrive", condition: "C", overrides: { opCouplingOn: 1, sCouplingMode: 1, opDriveOnK: 1 } },
];

const condSummaries = [];
const eventConditionedRows = [];
const edgeRowsBase = [];
const edgeRowsOp = [];

for (const cond of conditions) {
  const runSummaries = [];
  const allEvents = [];
  const transBaseHaz = new Map();
  const transBaseOut = new Map();
  const transOpHaz = new Map();
  const transOpOut = new Map();

  for (const seed of seeds) {
    const result = await runDeadlineOpkMotifEvents({
      presetPath: tunedPath,
      seed,
      condition: cond.condition,
      outDir,
      steps,
      reportEvery,
      eventEvery,
      deadline: tuned.deadline,
      regionType: baseParams.repairGateMode === 1 ? "stripe" : "quadrant",
      regionIndex: 2,
      gateSpan: baseParams.repairGateSpan ?? 1,
      corruptFrac: 0.2,
      errGood: 0.1,
      sdiffGood: 1.0,
      tailWindow: 200_000,
      basis,
      opBinsMode,
      gateConditioned,
      gateCheckEvery,
      sets: Object.entries(cond.overrides).map(([k, v]) => `${k}=${v}`),
    });
    runSummaries.push(result.summary);
    allEvents.push(...result.events);
    addPairsToMap(transBaseHaz, result.summary.transitionsBaseHazard ?? []);
    addPairsToMap(transBaseOut, result.summary.transitionsBaseOutside ?? []);
    addPairsToMap(transOpHaz, result.summary.transitionsOpHazard ?? []);
    addPairsToMap(transOpOut, result.summary.transitionsOpOutside ?? []);

    const baseEdges = parseEdgeCsv(path.join(result.outDir, `transition_edges_Mbase_seed_${seed}.csv`));
    const opEdges = parseEdgeCsv(path.join(result.outDir, `transition_edges_Mop_seed_${seed}.csv`));
    edgeRowsBase.push(...baseEdges);
    edgeRowsOp.push(...opEdges);
  }

  const missFrac = meanStd(runSummaries.map((s) => s.missFrac));
  const uptimeTail = meanStd(runSummaries.map((s) => s.uptimeTail));
  const errTail = meanStd(runSummaries.map((s) => s.errTailMean));
  const epTotalRate = meanStd(runSummaries.map((s) => s.epTotalRate));
  const epRepairRate = meanStd(runSummaries.map((s) => s.epRepairRate));
  const epOpKRate = meanStd(runSummaries.map((s) => s.epOpKRate));
  const epClockRate = meanStd(runSummaries.map((s) => s.epClockRate));
  const repairRate = meanStd(runSummaries.map((s) => s.repairRate));
  const opKRate = meanStd(runSummaries.map((s) => s.opKRate));
  const hazardGateSamples = meanStd(runSummaries.map((s) => s.hazardGateSampleCount));
  const recoverySampleCounts = meanStd(runSummaries.map((s) => s.eventRecoveryWindowSampleCountMean));

  const hazardOpChangeFrac = meanStd(runSummaries.map((s) => s.hazardOpChangeFracMean));
  const hazardOpUnique = meanStd(runSummaries.map((s) => s.hazardOpUniqueStatesVisited));

  const hazardBaseH = meanStd(runSummaries.map((s) => s.hazardBase_H?.mean));
  const hazardBaseV = meanStd(runSummaries.map((s) => s.hazardBase_Veff?.mean));
  const hazardBaseTop = meanStd(runSummaries.map((s) => s.hazardBase_topMass10?.mean));
  const hazardBaseChange = meanStd(runSummaries.map((s) => s.hazardBase_changeFrac?.mean));
  const hazardBaseUnique = meanStd(runSummaries.map((s) => s.uniqueBaseHazard));

  const hazardOpH = meanStd(runSummaries.map((s) => s.hazardOp_H?.mean));
  const hazardOpV = meanStd(runSummaries.map((s) => s.hazardOp_Veff?.mean));
  const hazardOpTop = meanStd(runSummaries.map((s) => s.hazardOp_topMass10?.mean));
  const hazardOpChange = meanStd(runSummaries.map((s) => s.hazardOp_changeFrac?.mean));

  const baseHazTrans = summarizeTransitions(transBaseHaz);
  const baseOutTrans = summarizeTransitions(transBaseOut);
  const opHazTrans = summarizeTransitions(transOpHaz);
  const opOutTrans = summarizeTransitions(transOpOut);

  condSummaries.push({
    condition: cond.id,
    opBinsMode,
    gateConditioned,
    gateCheckEvery,
    seeds: seeds.length,
    missFracMean: missFrac.mean,
    missFracStd: missFrac.std,
    uptimeTailMean: uptimeTail.mean,
    uptimeTailStd: uptimeTail.std,
    errTailMean: errTail.mean,
    errTailStd: errTail.std,
    epTotalRateMean: epTotalRate.mean,
    epTotalRateStd: epTotalRate.std,
    epRepairRateMean: epRepairRate.mean,
    epRepairRateStd: epRepairRate.std,
    epOpKRateMean: epOpKRate.mean,
    epOpKRateStd: epOpKRate.std,
    epClockRateMean: epClockRate.mean,
    epClockRateStd: epClockRate.std,
    repairRateMean: repairRate.mean,
    repairRateStd: repairRate.std,
    opKRateMean: opKRate.mean,
    opKRateStd: opKRate.std,
    hazardGateSampleCountMean: hazardGateSamples.mean,
    hazardGateSampleCountStd: hazardGateSamples.std,
    eventRecoveryWindowSampleCountMean: recoverySampleCounts.mean,
    eventRecoveryWindowSampleCountStd: recoverySampleCounts.std,
    hazardOpUniqueStatesMean: hazardOpUnique.mean,
    hazardOpUniqueStatesStd: hazardOpUnique.std,
    hazardOpChangeFracMean: hazardOpChangeFrac.mean,
    hazardOpChangeFracStd: hazardOpChangeFrac.std,
    hazardBase_HMean: hazardBaseH.mean,
    hazardBase_HStd: hazardBaseH.std,
    hazardBase_VeffMean: hazardBaseV.mean,
    hazardBase_VeffStd: hazardBaseV.std,
    hazardBase_topMass10Mean: hazardBaseTop.mean,
    hazardBase_topMass10Std: hazardBaseTop.std,
    hazardBase_changeFracMean: hazardBaseChange.mean,
    hazardBase_changeFracStd: hazardBaseChange.std,
    hazardBase_uniqueStatesMean: hazardBaseUnique.mean,
    hazardBase_uniqueStatesStd: hazardBaseUnique.std,
    hazardOp_HMean: hazardOpH.mean,
    hazardOp_HStd: hazardOpH.std,
    hazardOp_VeffMean: hazardOpV.mean,
    hazardOp_VeffStd: hazardOpV.std,
    hazardOp_topMass10Mean: hazardOpTop.mean,
    hazardOp_topMass10Std: hazardOpTop.std,
    hazardOp_changeFracSampleMean: hazardOpChange.mean,
    hazardOp_changeFracSampleStd: hazardOpChange.std,
    hazardBase_totalTrans: baseHazTrans.totalTrans,
    hazardBase_coarseEP: baseHazTrans.coarseEP,
    hazardBase_coarseEP_perTrans: baseHazTrans.coarseEP_perTrans,
    hazardBase_asym_perTrans: baseHazTrans.asym_perTrans,
    hazardBase_topEdgeNetJ: baseHazTrans.topEdgeNetJ,
    hazardBase_topEdgeEP: baseHazTrans.topEdgeEP,
    outsideBase_totalTrans: baseOutTrans.totalTrans,
    outsideBase_coarseEP: baseOutTrans.coarseEP,
    outsideBase_coarseEP_perTrans: baseOutTrans.coarseEP_perTrans,
    outsideBase_asym_perTrans: baseOutTrans.asym_perTrans,
    hazardOp_totalTrans: opHazTrans.totalTrans,
    hazardOp_coarseEP: opHazTrans.coarseEP,
    hazardOp_coarseEP_perTrans: opHazTrans.coarseEP_perTrans,
    hazardOp_asym_perTrans: opHazTrans.asym_perTrans,
    hazardOp_topEdgeNetJ: opHazTrans.topEdgeNetJ,
    hazardOp_topEdgeEP: opHazTrans.topEdgeEP,
    outsideOp_totalTrans: opOutTrans.totalTrans,
    outsideOp_coarseEP: opOutTrans.coarseEP,
    outsideOp_coarseEP_perTrans: opOutTrans.coarseEP_perTrans,
    outsideOp_asym_perTrans: opOutTrans.asym_perTrans,
  });

  const windows = [
    { key: "pre", label: "pre" },
    { key: "rec", label: "recovery" },
    { key: "tail", label: "tail" },
  ];

  for (const win of windows) {
    const baseSuccCounts = new Map();
    const baseFailCounts = new Map();
    const opSuccCounts = new Map();
    const opFailCounts = new Map();
    const baseSuccStats = [];
    const baseFailStats = [];
    const opSuccStats = [];
    const opFailStats = [];
    const baseSuccChanges = [];
    const baseFailChanges = [];
    const opSuccChanges = [];
    const opFailChanges = [];
    const baseSuccUnique = [];
    const baseFailUnique = [];
    const opSuccUnique = [];
    const opFailUnique = [];
    const baseSuccCoarse = [];
    const baseFailCoarse = [];
    const opSuccCoarse = [];
    const opFailCoarse = [];
    const baseSuccCoarsePer = [];
    const baseFailCoarsePer = [];
    const opSuccCoarsePer = [];
    const opFailCoarsePer = [];
    const baseSuccAsymPer = [];
    const baseFailAsymPer = [];
    const opSuccAsymPer = [];
    const opFailAsymPer = [];
    const baseSuccEpTotal = [];
    const baseFailEpTotal = [];
    const baseSuccEpRepair = [];
    const baseFailEpRepair = [];
    const opSuccEpOpK = [];
    const opFailEpOpK = [];
    const sampleCounts = [];

    for (const event of allEvents) {
      const baseCountsKey = `${win.key}BaseHaz_counts`;
      const opCountsKey = `${win.key}OpHaz_counts`;
      const baseTransKey = `${win.key}BaseHaz_trans`;
      const opTransKey = `${win.key}OpHaz_trans`;
      const baseCountPairs = event[baseCountsKey] ?? [];
      const opCountPairs = event[opCountsKey] ?? [];
      const baseTransPairs = event[baseTransKey] ?? [];
      const opTransPairs = event[opTransKey] ?? [];
      const baseCounts = pairsToMap(baseCountPairs);
      const opCounts = pairsToMap(opCountPairs);
      const baseTrans = pairsToMap(baseTransPairs);
      const opTrans = pairsToMap(opTransPairs);
      const baseH = entropyFromCounts(baseCounts);
      const opH = entropyFromCounts(opCounts);
      const baseTransTotal = totalTransitions(baseTrans);
      const opTransTotal = totalTransitions(opTrans);
      const baseCoarse = coarseEPSmoothed(baseTrans, 0.5);
      const opCoarse = coarseEPSmoothed(opTrans, 0.5);
      const baseAsym = asymmetryScore(baseTrans);
      const opAsym = asymmetryScore(opTrans);
      const baseCoarsePer = baseTransTotal > 0 ? baseCoarse / baseTransTotal : 0;
      const opCoarsePer = opTransTotal > 0 ? opCoarse / opTransTotal : 0;

      const baseChangeCount = event[`${win.key}_baseChangeCount`] ?? 0;
      const baseChangeTotal = event[`${win.key}_baseChangeTotal`] ?? 0;
      const opChangeCount = event[`${win.key}_opChangeCount`] ?? 0;
      const opChangeTotal = event[`${win.key}_opChangeTotal`] ?? 0;
      const baseChangeFrac = baseChangeTotal > 0 ? baseChangeCount / baseChangeTotal : 0;
      const opChangeFrac = opChangeTotal > 0 ? opChangeCount / opChangeTotal : 0;

      const epTotal = event[`${win.key}_epTotal`] ?? 0;
      const epRepair = event[`${win.key}_epRepair`] ?? 0;
      const epOpK = event[`${win.key}_epOpK`] ?? 0;

      const baseEpTotalPer = baseChangeCount > 0 ? epTotal / baseChangeCount : 0;
      const baseEpRepairPer = baseChangeCount > 0 ? epRepair / baseChangeCount : 0;
      const opEpOpKPer = opChangeCount > 0 ? epOpK / opChangeCount : 0;

      const targetBaseCounts = event.success ? baseSuccCounts : baseFailCounts;
      const targetOpCounts = event.success ? opSuccCounts : opFailCounts;
      for (const [key, count] of baseCounts.entries()) {
        targetBaseCounts.set(key, (targetBaseCounts.get(key) ?? 0) + count);
      }
      for (const [key, count] of opCounts.entries()) {
        targetOpCounts.set(key, (targetOpCounts.get(key) ?? 0) + count);
      }

      if (event.success) {
        baseSuccStats.push(baseH);
        opSuccStats.push(opH);
        baseSuccChanges.push(baseChangeFrac);
        opSuccChanges.push(opChangeFrac);
        baseSuccUnique.push(baseCounts.size);
        opSuccUnique.push(opCounts.size);
        baseSuccCoarse.push(baseCoarse);
        opSuccCoarse.push(opCoarse);
        baseSuccCoarsePer.push(baseCoarsePer);
        opSuccCoarsePer.push(opCoarsePer);
        baseSuccAsymPer.push(baseAsym);
        opSuccAsymPer.push(opAsym);
        baseSuccEpTotal.push(baseEpTotalPer);
        baseSuccEpRepair.push(baseEpRepairPer);
        opSuccEpOpK.push(opEpOpKPer);
      } else {
        baseFailStats.push(baseH);
        opFailStats.push(opH);
        baseFailChanges.push(baseChangeFrac);
        opFailChanges.push(opChangeFrac);
        baseFailUnique.push(baseCounts.size);
        opFailUnique.push(opCounts.size);
        baseFailCoarse.push(baseCoarse);
        opFailCoarse.push(opCoarse);
        baseFailCoarsePer.push(baseCoarsePer);
        opFailCoarsePer.push(opCoarsePer);
        baseFailAsymPer.push(baseAsym);
        opFailAsymPer.push(opAsym);
        baseFailEpTotal.push(baseEpTotalPer);
        baseFailEpRepair.push(baseEpRepairPer);
        opFailEpOpK.push(opEpOpKPer);
      }

      if (win.key === "rec") {
        sampleCounts.push(event.rec_samples ?? 0);
      }
    }

    const baseJs = jsDivergence(baseSuccCounts, baseFailCounts);
    const opJs = jsDivergence(opSuccCounts, opFailCounts);
    const recoverySampleMean = sampleCounts.length ? mean(sampleCounts) : 0;

    eventConditionedRows.push({
      condition: cond.id,
      opBinsMode,
      region: "hazard",
      family: "M_base",
      window: win.label,
      H_succ: safeMean(baseSuccStats),
      H_fail: safeMean(baseFailStats),
      H_delta: safeMean(baseSuccStats) - safeMean(baseFailStats),
      js_divergence: baseJs,
      coarseEP_succ: safeMean(baseSuccCoarse),
      coarseEP_fail: safeMean(baseFailCoarse),
      coarseEP_perTrans_succ: safeMean(baseSuccCoarsePer),
      coarseEP_perTrans_fail: safeMean(baseFailCoarsePer),
      asym_perTrans_succ: safeMean(baseSuccAsymPer),
      asym_perTrans_fail: safeMean(baseFailAsymPer),
      changeFrac_succ: safeMean(baseSuccChanges),
      changeFrac_fail: safeMean(baseFailChanges),
      uniqueStatesVisited_succ: safeMean(baseSuccUnique),
      uniqueStatesVisited_fail: safeMean(baseFailUnique),
      epTotalPerChange_succ: safeMean(baseSuccEpTotal),
      epTotalPerChange_fail: safeMean(baseFailEpTotal),
      epRepairPerChange_succ: safeMean(baseSuccEpRepair),
      epRepairPerChange_fail: safeMean(baseFailEpRepair),
      epOpKPerChange_succ: null,
      epOpKPerChange_fail: null,
      recoverySampleCountMean: recoverySampleMean,
    });

    eventConditionedRows.push({
      condition: cond.id,
      opBinsMode,
      region: "hazard",
      family: "M_op",
      window: win.label,
      H_succ: safeMean(opSuccStats),
      H_fail: safeMean(opFailStats),
      H_delta: safeMean(opSuccStats) - safeMean(opFailStats),
      js_divergence: opJs,
      coarseEP_succ: safeMean(opSuccCoarse),
      coarseEP_fail: safeMean(opFailCoarse),
      coarseEP_perTrans_succ: safeMean(opSuccCoarsePer),
      coarseEP_perTrans_fail: safeMean(opFailCoarsePer),
      asym_perTrans_succ: safeMean(opSuccAsymPer),
      asym_perTrans_fail: safeMean(opFailAsymPer),
      changeFrac_succ: safeMean(opSuccChanges),
      changeFrac_fail: safeMean(opFailChanges),
      uniqueStatesVisited_succ: safeMean(opSuccUnique),
      uniqueStatesVisited_fail: safeMean(opFailUnique),
      epTotalPerChange_succ: null,
      epTotalPerChange_fail: null,
      epRepairPerChange_succ: null,
      epRepairPerChange_fail: null,
      epOpKPerChange_succ: safeMean(opSuccEpOpK),
      epOpKPerChange_fail: safeMean(opFailEpOpK),
      recoverySampleCountMean: recoverySampleMean,
    });
  }
}

ensureDir(outDir);
const condHeader = Object.keys(condSummaries[0] ?? {}).join(",");
const condLines = [condHeader];
for (const row of condSummaries) {
  condLines.push(Object.values(row).join(","));
}
fs.writeFileSync(path.join(outDir, "compare_summary.csv"), condLines.join("\n") + "\n");

const eventHeader = Object.keys(eventConditionedRows[0] ?? {}).join(",");
const eventLines = [eventHeader];
for (const row of eventConditionedRows) {
  eventLines.push(Object.values(row).join(","));
}
fs.writeFileSync(path.join(outDir, "event_conditioned_summary.csv"), eventLines.join("\n") + "\n");

const edgeHeader =
  "condition,seed,region,family,from,to,count,countRev,epRepairSum,epOpKSum,epTotalSum,epRepairPerTrans,epOpKPerTrans,epTotalPerTrans";
const baseAllLines = [edgeHeader];
for (const row of edgeRowsBase) {
  baseAllLines.push(
    [
      row.condition,
      row.seed,
      row.region,
      row.family,
      row.from,
      row.to,
      row.count,
      row.countRev,
      row.epRepairSum,
      row.epOpKSum,
      row.epTotalSum,
      row.epRepairPerTrans,
      row.epOpKPerTrans,
      row.epTotalPerTrans,
    ].join(","),
  );
}
fs.writeFileSync(path.join(outDir, "transition_edges_Mbase_all.csv"), baseAllLines.join("\n") + "\n");

const opAllLines = [edgeHeader];
for (const row of edgeRowsOp) {
  opAllLines.push(
    [
      row.condition,
      row.seed,
      row.region,
      row.family,
      row.from,
      row.to,
      row.count,
      row.countRev,
      row.epRepairSum,
      row.epOpKSum,
      row.epTotalSum,
      row.epRepairPerTrans,
      row.epOpKPerTrans,
      row.epTotalPerTrans,
    ].join(","),
  );
}
fs.writeFileSync(path.join(outDir, "transition_edges_Mop_all.csv"), opAllLines.join("\n") + "\n");

const baseAgg = aggregateEdgeRows(edgeRowsBase);
const opAgg = aggregateEdgeRows(edgeRowsOp);
const baseTop = baseAgg.slice().sort((a, b) => b.count - a.count).slice(0, 50);
const opTop = opAgg.slice().sort((a, b) => b.count - a.count).slice(0, 50);
const topHeader =
  "condition,region,family,from,to,count,countRev,epRepairSum,epOpKSum,epTotalSum,epRepairPerTrans,epOpKPerTrans,epTotalPerTrans";
const baseTopLines = [topHeader];
for (const row of baseTop) {
  baseTopLines.push(
    [
      row.condition,
      row.region,
      row.family,
      row.from,
      row.to,
      row.count,
      row.countRev,
      row.epRepairSum,
      row.epOpKSum,
      row.epTotalSum,
      row.epRepairPerTrans,
      row.epOpKPerTrans,
      row.epTotalPerTrans,
    ].join(","),
  );
}
fs.writeFileSync(path.join(outDir, "transition_edges_Mbase_top.csv"), baseTopLines.join("\n") + "\n");

const opTopLines = [topHeader];
for (const row of opTop) {
  opTopLines.push(
    [
      row.condition,
      row.region,
      row.family,
      row.from,
      row.to,
      row.count,
      row.countRev,
      row.epRepairSum,
      row.epOpKSum,
      row.epTotalSum,
      row.epRepairPerTrans,
      row.epOpKPerTrans,
      row.epTotalPerTrans,
    ].join(","),
  );
}
fs.writeFileSync(path.join(outDir, "transition_edges_Mop_top.csv"), opTopLines.join("\n") + "\n");

console.log(`tuned deadline: ${tuned.deadline} (missFracMean=${tuned.missMean})`);
console.log(`compare summary written: ${path.join(outDir, "compare_summary.csv")}`);
console.log(`event-conditioned written: ${path.join(outDir, "event_conditioned_summary.csv")}`);

const rowA = condSummaries.find((row) => row.condition === "A_legacy");
const rowB = condSummaries.find((row) => row.condition === "B_op_noKdrive");
const rowC = condSummaries.find((row) => row.condition === "C_op_withKdrive");
const ok =
  (rowB?.hazardOpUniqueStatesMean ?? 0) >= 10 &&
  (rowB?.eventRecoveryWindowSampleCountMean ?? 0) >= 1 &&
  (rowB?.hazardOpChangeFracMean ?? 0) >= 0.01 &&
  (rowC?.hazardOpUniqueStatesMean ?? 0) >= 10 &&
  (rowC?.eventRecoveryWindowSampleCountMean ?? 0) >= 1 &&
  (rowC?.hazardOpChangeFracMean ?? 0) >= 0.01;

if (ok) console.log("MOTIF_SIGNAL_PLAUSIBLE");
else console.log("MOTIF_INSTRUMENTATION_TOO_SPARSE");
</file>

<file path="scripts/run-deadline-opk-motif-events.mjs">
#!/usr/bin/env node
import fs from "node:fs";
import path from "node:path";
import {
  loadWasm,
  mean,
  std,
  percentile,
  readJson,
  errRegionBits,
  meanAbsDiffRegion,
} from "./deadline-event-utils.mjs";
import { parseOpOffsets } from "./opk-metrics.mjs";
import {
  computeMBaseClasses,
  computeMOpClasses,
  vocabStats,
  asymmetryScore,
  coarseEPSmoothed,
  edgeFamily,
  edgeKey,
  offsetsToDxDy,
} from "./opk-motif-basis.mjs";

const rootDir = path.resolve(path.dirname(new URL(import.meta.url).pathname), "..");

function ensureDir(dir) {
  fs.mkdirSync(dir, { recursive: true });
}

function parseArgs(argv) {
  const out = {
    presetPath: null,
    seed: 1,
    condition: "A",
    outDir: path.resolve(rootDir, ".tmp", "motif_pressure_v4"),
    steps: null,
    reportEvery: null,
    eventEvery: null,
    deadline: null,
    regionType: null,
    regionIndex: null,
    gateSpan: null,
    corruptFrac: null,
    errGood: 0.1,
    sdiffGood: 1.0,
    tailWindow: 200_000,
    sampleEvery: null,
    burnIn: 200_000,
    opBinsMode: 2,
    gateConditioned: 1,
    gateCheckEvery: 5000,
    motifMode: "state",
    sets: [],
  };
  for (let i = 2; i < argv.length; i += 1) {
    const arg = argv[i];
    if (arg === "--preset") out.presetPath = argv[++i];
    else if (arg === "--seed") out.seed = Number(argv[++i]);
    else if (arg === "--condition") out.condition = argv[++i];
    else if (arg === "--outDir") out.outDir = path.resolve(argv[++i]);
    else if (arg === "--steps") out.steps = Number(argv[++i]);
    else if (arg === "--reportEvery") out.reportEvery = Number(argv[++i]);
    else if (arg === "--eventEvery") out.eventEvery = Number(argv[++i]);
    else if (arg === "--deadline") out.deadline = Number(argv[++i]);
    else if (arg === "--regionType") out.regionType = argv[++i];
    else if (arg === "--regionIndex") out.regionIndex = Number(argv[++i]);
    else if (arg === "--gateSpan") out.gateSpan = Number(argv[++i]);
    else if (arg === "--corruptFrac") out.corruptFrac = Number(argv[++i]);
    else if (arg === "--errGood") out.errGood = Number(argv[++i]);
    else if (arg === "--sdiffGood") out.sdiffGood = Number(argv[++i]);
    else if (arg === "--tailWindow") out.tailWindow = Number(argv[++i]);
    else if (arg === "--sampleEvery") out.sampleEvery = Number(argv[++i]);
    else if (arg === "--burnIn") out.burnIn = Number(argv[++i]);
    else if (arg === "--opBinsMode") out.opBinsMode = Number(argv[++i]);
    else if (arg === "--gateConditioned") out.gateConditioned = Number(argv[++i]);
    else if (arg === "--gateCheckEvery") out.gateCheckEvery = Number(argv[++i]);
    else if (arg === "--motifMode") out.motifMode = argv[++i];
    else if (arg === "--set") out.sets.push(argv[++i] ?? "");
  }
  return out;
}

function applySets(target, sets) {
  for (const item of sets) {
    if (!item) continue;
    const [key, raw] = item.split("=");
    if (!key) continue;
    const val = Number(raw);
    if (!Number.isFinite(val)) continue;
    target[key] = val;
  }
}

function quadrantIndex(idx, g) {
  const x = idx % g;
  const y = Math.floor(idx / g);
  const qx = x < g / 2 ? 0 : 1;
  const qy = y < g / 2 ? 0 : 1;
  return qy * 2 + qx;
}

function stripeIndex(idx, g, bins) {
  const x = idx % g;
  const fx = x / g;
  return Math.min(bins - 1, Math.floor(fx * bins));
}

function inWrapSpan(active, center, span, mod) {
  const s = Math.max(0, span);
  for (let offset = -s; offset <= s; offset += 1) {
    const idx = ((center + offset) % mod + mod) % mod;
    if (idx === active) return true;
  }
  return false;
}

function hazardGateActive(clockState, params, regionIndex, gateSpan) {
  const mode = params.repairGateMode ?? 0;
  if (mode === 0) return true;
  if (mode === 1) {
    const k = params.clockK ?? 8;
    const active = ((clockState % k) + k) % k;
    return inWrapSpan(active, regionIndex, gateSpan, k);
  }
  if (mode === 2) {
    const k = 4;
    const active = ((clockState % k) + k) % k;
    return inWrapSpan(active, regionIndex, gateSpan, k);
  }
  return true;
}

function buildRegionMask(g, regionType, regionIndex, span, bins) {
  const cells = g * g;
  const mask = new Array(cells);
  if (regionType === "stripe") {
    const s = Math.max(1, span);
    for (let i = 0; i < cells; i += 1) {
      const stripe = stripeIndex(i, g, bins);
      let ok = false;
      for (let k = 0; k < s; k += 1) {
        if ((regionIndex + k) % bins === stripe) {
          ok = true;
          break;
        }
      }
      mask[i] = ok;
    }
  } else {
    for (let i = 0; i < cells; i += 1) {
      mask[i] = quadrantIndex(i, g) === regionIndex;
    }
  }
  return mask;
}

function shuffleInPlace(arr, seed) {
  let x = seed >>> 0;
  if (x === 0) x = 1;
  for (let i = arr.length - 1; i > 0; i -= 1) {
    x ^= x << 13;
    x ^= x >>> 17;
    x ^= x << 5;
    const j = x % (i + 1);
    [arr[i], arr[j]] = [arr[j], arr[i]];
  }
}

function buildMatchedOutsideMask(regionMask, seed) {
  const outside = [];
  const regionCount = regionMask.filter(Boolean).length;
  for (let i = 0; i < regionMask.length; i += 1) {
    if (!regionMask[i]) outside.push(i);
  }
  if (outside.length === 0) return regionMask.map(() => false);
  shuffleInPlace(outside, seed);
  const target = Math.min(regionCount, outside.length);
  const mask = new Array(regionMask.length).fill(false);
  for (let i = 0; i < target; i += 1) {
    mask[outside[i]] = true;
  }
  return mask;
}

function updateCount(map, key, delta = 1) {
  map.set(key, (map.get(key) ?? 0) + delta);
}

function updateTransition(map, fromKey, toKey, delta = 1) {
  const key = `${fromKey}|${toKey}`;
  map.set(key, (map.get(key) ?? 0) + delta);
}

function mapToPairs(map) {
  return Array.from(map.entries()).map(([key, count]) => [key, count]);
}

function addEdgeStats(counts, epSum, epAbsSum, key, epDelta) {
  counts.set(key, (counts.get(key) ?? 0) + 1);
  epSum.set(key, (epSum.get(key) ?? 0) + epDelta);
  epAbsSum.set(key, (epAbsSum.get(key) ?? 0) + Math.abs(epDelta));
}

function summarizeValues(values) {
  const nums = values.filter((v) => Number.isFinite(v));
  return {
    mean: nums.length ? mean(nums) : null,
    std: nums.length ? std(nums) : null,
  };
}

function computeCounts(classesByIface, mask) {
  const counts = new Map();
  let total = 0;
  for (const iface of classesByIface) {
    for (let q = 0; q < iface.length; q += 1) {
      if (!mask[q]) continue;
      updateCount(counts, String(iface[q]));
      total += 1;
    }
  }
  return { counts, total };
}

function computeChange(prevByIface, nextByIface, mask) {
  let changed = 0;
  let total = 0;
  for (let iface = 0; iface < nextByIface.length; iface += 1) {
    const prev = prevByIface[iface];
    const next = nextByIface[iface];
    for (let q = 0; q < next.length; q += 1) {
      if (!mask[q]) continue;
      total += 1;
      if (prev[q] !== next[q]) changed += 1;
    }
  }
  return { changed, total, frac: total > 0 ? changed / total : 0 };
}

function accumulateTransitions(prevByIface, nextByIface, mask, targetMap) {
  let changed = 0;
  for (let iface = 0; iface < nextByIface.length; iface += 1) {
    const prev = prevByIface[iface];
    const next = nextByIface[iface];
    for (let q = 0; q < next.length; q += 1) {
      if (!mask[q]) continue;
      const fromKey = String(prev[q]);
      const toKey = String(next[q]);
      if (fromKey === toKey) continue;
      updateTransition(targetMap, fromKey, toKey);
      changed += 1;
    }
  }
  return changed;
}

function emptyWindowStats() {
  return {
    countsBaseHazard: new Map(),
    countsBaseOutside: new Map(),
    countsOpHazard: new Map(),
    countsOpOutside: new Map(),
    transBaseHazard: new Map(),
    transBaseOutside: new Map(),
    transOpHazard: new Map(),
    transOpOutside: new Map(),
    changeBaseHazardCount: 0,
    changeBaseHazardTotal: 0,
    changeBaseOutsideCount: 0,
    changeBaseOutsideTotal: 0,
    changeOpHazardCount: 0,
    changeOpHazardTotal: 0,
    changeOpOutsideCount: 0,
    changeOpOutsideTotal: 0,
    changeBaseHazard: 0,
    changeBaseOutside: 0,
    changeOpHazard: 0,
    changeOpOutside: 0,
    epTotal: 0,
    epRepair: 0,
    epOpK: 0,
    samples: 0,
  };
}

function eventWindowForTime(event, t, wPre, tailWindow, deadline) {
  if (t >= event.tEvent - wPre && t < event.tEvent) return "pre";
  if (t >= event.tEvent && t <= event.tEvent + deadline) return "recovery";
  if (t > event.tEvent + deadline && t <= event.tEvent + deadline + tailWindow) return "tail";
  return null;
}

function totalTransitions(counts) {
  let total = 0;
  for (const [key, count] of counts.entries()) {
    const [from, to] = key.split("|");
    if (from === to) continue;
    total += count;
  }
  return total;
}

function buildEventRow(event) {
  const row = {
    seed: event.seed,
    condition: event.condition,
    opBinsMode: event.opBinsMode,
    eventIdx: event.idx,
    tEvent: event.tEvent,
    success: !!event.recovered,
    miss: !!event.miss,
    recovery: event.recoveryTime ?? null,
    stepsToOutcome: event.stepsToOutcome ?? null,
    recoverySamples: event.recoverySamples ?? 0,
    pre_samples: event.pre.samples,
    rec_samples: event.recovery.samples,
    tail_samples: event.tail.samples,
    preBaseHaz_counts: mapToPairs(event.pre.countsBaseHazard),
    recBaseHaz_counts: mapToPairs(event.recovery.countsBaseHazard),
    tailBaseHaz_counts: mapToPairs(event.tail.countsBaseHazard),
    preBaseOut_counts: mapToPairs(event.pre.countsBaseOutside),
    recBaseOut_counts: mapToPairs(event.recovery.countsBaseOutside),
    tailBaseOut_counts: mapToPairs(event.tail.countsBaseOutside),
    preOpHaz_counts: mapToPairs(event.pre.countsOpHazard),
    recOpHaz_counts: mapToPairs(event.recovery.countsOpHazard),
    tailOpHaz_counts: mapToPairs(event.tail.countsOpHazard),
    preOpOut_counts: mapToPairs(event.pre.countsOpOutside),
    recOpOut_counts: mapToPairs(event.recovery.countsOpOutside),
    tailOpOut_counts: mapToPairs(event.tail.countsOpOutside),
    preBaseHaz_trans: mapToPairs(event.pre.transBaseHazard),
    recBaseHaz_trans: mapToPairs(event.recovery.transBaseHazard),
    tailBaseHaz_trans: mapToPairs(event.tail.transBaseHazard),
    preBaseOut_trans: mapToPairs(event.pre.transBaseOutside),
    recBaseOut_trans: mapToPairs(event.recovery.transBaseOutside),
    tailBaseOut_trans: mapToPairs(event.tail.transBaseOutside),
    preOpHaz_trans: mapToPairs(event.pre.transOpHazard),
    recOpHaz_trans: mapToPairs(event.recovery.transOpHazard),
    tailOpHaz_trans: mapToPairs(event.tail.transOpHazard),
    preOpOut_trans: mapToPairs(event.pre.transOpOutside),
    recOpOut_trans: mapToPairs(event.recovery.transOpOutside),
    tailOpOut_trans: mapToPairs(event.tail.transOpOutside),
    pre_baseChange: event.pre.changeBaseHazard,
    rec_baseChange: event.recovery.changeBaseHazard,
    tail_baseChange: event.tail.changeBaseHazard,
    pre_baseChangeOutside: event.pre.changeBaseOutside,
    rec_baseChangeOutside: event.recovery.changeBaseOutside,
    tail_baseChangeOutside: event.tail.changeBaseOutside,
    pre_baseChangeCount: event.pre.changeBaseHazardCount,
    rec_baseChangeCount: event.recovery.changeBaseHazardCount,
    tail_baseChangeCount: event.tail.changeBaseHazardCount,
    pre_baseChangeTotal: event.pre.changeBaseHazardTotal,
    rec_baseChangeTotal: event.recovery.changeBaseHazardTotal,
    tail_baseChangeTotal: event.tail.changeBaseHazardTotal,
    pre_baseChangeOutsideCount: event.pre.changeBaseOutsideCount,
    rec_baseChangeOutsideCount: event.recovery.changeBaseOutsideCount,
    tail_baseChangeOutsideCount: event.tail.changeBaseOutsideCount,
    pre_baseChangeOutsideTotal: event.pre.changeBaseOutsideTotal,
    rec_baseChangeOutsideTotal: event.recovery.changeBaseOutsideTotal,
    tail_baseChangeOutsideTotal: event.tail.changeBaseOutsideTotal,
    pre_opChange: event.pre.changeOpHazard,
    rec_opChange: event.recovery.changeOpHazard,
    tail_opChange: event.tail.changeOpHazard,
    pre_opChangeOutside: event.pre.changeOpOutside,
    rec_opChangeOutside: event.recovery.changeOpOutside,
    tail_opChangeOutside: event.tail.changeOpOutside,
    pre_opChangeCount: event.pre.changeOpHazardCount,
    rec_opChangeCount: event.recovery.changeOpHazardCount,
    tail_opChangeCount: event.tail.changeOpHazardCount,
    pre_opChangeTotal: event.pre.changeOpHazardTotal,
    rec_opChangeTotal: event.recovery.changeOpHazardTotal,
    tail_opChangeTotal: event.tail.changeOpHazardTotal,
    pre_opChangeOutsideCount: event.pre.changeOpOutsideCount,
    rec_opChangeOutsideCount: event.recovery.changeOpOutsideCount,
    tail_opChangeOutsideCount: event.tail.changeOpOutsideCount,
    pre_opChangeOutsideTotal: event.pre.changeOpOutsideTotal,
    rec_opChangeOutsideTotal: event.recovery.changeOpOutsideTotal,
    tail_opChangeOutsideTotal: event.tail.changeOpOutsideTotal,
    pre_epTotal: event.pre.epTotal,
    rec_epTotal: event.recovery.epTotal,
    tail_epTotal: event.tail.epTotal,
    pre_epRepair: event.pre.epRepair,
    rec_epRepair: event.recovery.epRepair,
    tail_epRepair: event.tail.epRepair,
    pre_epOpK: event.pre.epOpK,
    rec_epOpK: event.recovery.epOpK,
    tail_epOpK: event.tail.epOpK,
  };
  return row;
}

async function runOnceMoveEdges(args) {
  const mod = await loadWasm();
  const presetRaw = readJson(args.presetPath);
  const baseParams = presetRaw.params ?? presetRaw;
  const params = { ...baseParams };
  applySets(params, args.sets);

  if (args.condition === "A") {
    params.opCouplingOn = 0;
    params.sCouplingMode = 0;
    params.opDriveOnK = 0;
  } else if (args.condition === "B") {
    params.opCouplingOn = 1;
    params.sCouplingMode = 1;
    params.opDriveOnK = 0;
  } else if (args.condition === "C") {
    params.opCouplingOn = 1;
    params.sCouplingMode = 1;
    params.opDriveOnK = 1;
  }

  const steps = args.steps ?? presetRaw.steps ?? 2_000_000;
  const eventEvery = args.eventEvery ?? presetRaw.eventEvery ?? 50_000;
  const deadline = args.deadline ?? presetRaw.deadline ?? 25_000;
  const regionType = args.regionType ?? (params.repairGateMode === 1 ? "stripe" : "quadrant");
  const regionIndex = args.regionIndex ?? 2;
  const gateSpan = args.gateSpan ?? params.repairGateSpan ?? 1;
  const corruptFrac = args.corruptFrac ?? 0.2;

  const gridSize = params.gridSize ?? 32;
  const bins = params.clockK ?? 8;
  const regionMask = buildRegionMask(gridSize, regionType, regionIndex, gateSpan, bins);
  const outsideMask = buildMatchedOutsideMask(regionMask, args.seed + 999);

  const sim = new mod.Sim(50, args.seed);
  sim.set_params({ ...params, epDebug: 1 });
  if (args.burnIn > 0) sim.step(args.burnIn);

  const moveLabels = sim.ep_move_labels ? Array.from(sim.ep_move_labels()) : [];
  const opkLabelIdx = moveLabels.indexOf("OpK");
  const opkMoveId = 9;
  if (opkLabelIdx >= 0 && opkLabelIdx !== opkMoveId) {
    console.warn(`OpK move label index ${opkLabelIdx} != ${opkMoveId}; using ${opkMoveId} for acceptLogMask`);
  }
  sim.set_params({
    acceptLogOn: 1,
    acceptLogMask: 1 << opkMoveId,
    acceptLogCap: 200000,
  });

  const opOffsets = sim.op_offsets ? sim.op_offsets() : new Int8Array();
  const { dx, dy } = offsetsToDxDy(opOffsets);
  const rCount = dx.length;

  const edgeCountHazard = new Map();
  const edgeEpSumHazard = new Map();
  const edgeEpAbsSumHazard = new Map();
  const edgeCountOutside = new Map();
  const edgeEpSumOutside = new Map();
  const edgeEpAbsSumOutside = new Map();
  const edgeFamCountHazard = new Map();
  const edgeFamCountOutside = new Map();

  let totalMovesHazard = 0;
  let totalMovesOutside = 0;
  let totalEpHazard = 0;
  let totalEpOutside = 0;
  let acceptLogOverflowed = false;

  const maxEventStart = steps - deadline;
  let eventIdx = 0;
  let t = 0;
  const chunkSteps = 10000;

  const applyEventAt = (tEvent) => {
    const perturb = {
      target: "metaS",
      layer: 0,
      frac: corruptFrac,
      mode: "randomize",
    };
    if (regionType === "stripe") {
      perturb.region = "stripe";
      perturb.bins = bins;
      perturb.span = gateSpan;
      perturb.bin = regionIndex;
    } else {
      perturb.region = "quadrant";
      perturb.quadrant = regionIndex;
    }
    perturb.seed = args.seed * 1000 + tEvent;
    sim.apply_perturbation(perturb);
  };

  const processLogEntries = (u32, ep) => {
    const entries = Math.min(ep.length, Math.floor(u32.length / 3));
    for (let i = 0; i < entries; i += 1) {
      const tEntry = u32[i * 3];
      const q = u32[i * 3 + 1];
      const meta = u32[i * 3 + 2];
      const moveId = meta & 0xff;
      if (moveId !== opkMoveId) continue;
      if (tEntry < eventEvery) continue;
      const t0 = Math.floor(tEntry / eventEvery) * eventEvery;
      if (t0 < eventEvery || t0 > maxEventStart) continue;
      if (tEntry - t0 >= deadline) continue;

      const qIdx = Number(q);
      const inHazard = regionMask[qIdx];
      const inOutside = outsideMask[qIdx];
      if (!inHazard && !inOutside) continue;

      const fromIdx = (meta >> 16) & 0xff;
      const toIdx = (meta >> 24) & 0xff;
      if (fromIdx >= rCount || toIdx >= rCount) continue;
      const key = edgeKey(fromIdx, toIdx);
      const epDelta = ep[i] ?? 0;

      if (inHazard) {
        addEdgeStats(edgeCountHazard, edgeEpSumHazard, edgeEpAbsSumHazard, key, epDelta);
        totalMovesHazard += 1;
        totalEpHazard += epDelta;
        if (dx.length > 0 && dy.length > 0) {
          const fam = edgeFamily(fromIdx, toIdx, dx, dy);
          edgeFamCountHazard.set(fam, (edgeFamCountHazard.get(fam) ?? 0) + 1);
        }
      } else if (inOutside) {
        addEdgeStats(edgeCountOutside, edgeEpSumOutside, edgeEpAbsSumOutside, key, epDelta);
        totalMovesOutside += 1;
        totalEpOutside += epDelta;
        if (dx.length > 0 && dy.length > 0) {
          const fam = edgeFamily(fromIdx, toIdx, dx, dy);
          edgeFamCountOutside.set(fam, (edgeFamCountOutside.get(fam) ?? 0) + 1);
        }
      }
    }
  };

  while (t < steps) {
    const nextEventTime = (eventIdx + 1) * eventEvery;
    const next = Math.min(t + chunkSteps, nextEventTime, steps);
    if (next > t) {
      sim.step(next - t);
      t = next;
    }
    if (nextEventTime <= maxEventStart && t === nextEventTime) {
      applyEventAt(nextEventTime);
      eventIdx += 1;
    }

    if (sim.accept_log_len) {
      const len = sim.accept_log_len();
      if (len > 0) {
        const u32 = sim.accept_log_u32();
        const ep = sim.accept_log_ep();
        processLogEntries(u32, ep);
      }
      acceptLogOverflowed = sim.accept_log_overflowed();
      sim.accept_log_clear();
      if (acceptLogOverflowed) {
        throw new Error("ACCEPT_LOG_OVERFLOW: reduce chunkSteps or raise acceptLogCap");
      }
    }
  }

  const topEdges = (counts, epSum, limit) => {
    const rows = [];
    for (const [key, count] of counts.entries()) {
      const [fromIdx, toIdx] = key.split("->");
      rows.push({
        fromIdx: Number(fromIdx),
        toIdx: Number(toIdx),
        count,
        epSum: epSum.get(key) ?? 0,
      });
    }
    rows.sort((a, b) => b.count - a.count);
    return rows.slice(0, limit);
  };

  const summary = {
    seed: args.seed,
    condition: args.condition,
    steps,
    eventEvery,
    deadline,
    gridSize,
    metaLayers: params.metaLayers ?? 0,
    opStencil: params.opStencil ?? 0,
    opBudgetK: params.opBudgetK ?? 0,
    totalMovesHazard,
    totalMovesOutside,
    totalEpHazard,
    totalEpOutside,
    uniqueEdgesHazard: edgeCountHazard.size,
    uniqueEdgesOutside: edgeCountOutside.size,
    topEdgesHazard: topEdges(edgeCountHazard, edgeEpSumHazard, 20),
    topEdgesOutside: topEdges(edgeCountOutside, edgeEpSumOutside, 20),
    acceptLogOverflowed,
  };

  return {
    summary,
    edgeCountHazard,
    edgeEpSumHazard,
    edgeEpAbsSumHazard,
    edgeCountOutside,
    edgeEpSumOutside,
    edgeEpAbsSumOutside,
    edgeFamCountHazard,
    edgeFamCountOutside,
  };
}

async function runOnceP5Actions(args) {
  const mod = await loadWasm();
  const presetRaw = readJson(args.presetPath);
  const baseParams = presetRaw.params ?? presetRaw;
  const params = { ...baseParams };
  applySets(params, args.sets);

  if (args.condition === "A") {
    params.opCouplingOn = 0;
    params.sCouplingMode = 0;
    params.opDriveOnK = 0;
  } else if (args.condition === "B") {
    params.opCouplingOn = 1;
    params.sCouplingMode = 1;
    params.opDriveOnK = 0;
  } else if (args.condition === "C") {
    params.opCouplingOn = 1;
    params.sCouplingMode = 1;
    params.opDriveOnK = 1;
  }

  const steps = args.steps ?? presetRaw.steps ?? 2_000_000;
  const eventEvery = args.eventEvery ?? presetRaw.eventEvery ?? 50_000;
  const deadline = args.deadline ?? presetRaw.deadline ?? 25_000;
  const regionType = args.regionType ?? (params.repairGateMode === 1 ? "stripe" : "quadrant");
  const regionIndex = args.regionIndex ?? 2;
  const gateSpan = args.gateSpan ?? params.repairGateSpan ?? 1;
  const corruptFrac = args.corruptFrac ?? 0.2;

  const gridSize = params.gridSize ?? 32;
  const bins = params.clockK ?? 8;
  const regionMask = buildRegionMask(gridSize, regionType, regionIndex, gateSpan, bins);
  const outsideMask = buildMatchedOutsideMask(regionMask, args.seed + 999);

  const sim = new mod.Sim(50, args.seed);
  sim.set_params({ ...params, epDebug: 1 });
  if (args.burnIn > 0) sim.step(args.burnIn);

  const moveLabels = sim.ep_move_labels ? Array.from(sim.ep_move_labels()) : [];
  const idxP5Base = moveLabels.indexOf("P5Base");
  const idxP5Meta = moveLabels.indexOf("P5Meta");
  if (idxP5Base < 0 || idxP5Meta < 0) {
    throw new Error(`MISSING_P5_MOVE_LABEL: ${JSON.stringify(moveLabels)}`);
  }
  if (idxP5Base > 31 || idxP5Meta > 31) {
    throw new Error("ACCEPT_LOG_MASK_OVERFLOW");
  }
  sim.set_params({
    acceptLogOn: 1,
    acceptLogMask: (1 << idxP5Base) | (1 << idxP5Meta),
    acceptLogCap: 200000,
  });
  if (sim.accept_log_clear) sim.accept_log_clear();

  const rCount = sim.op_r_count ? sim.op_r_count() : 0;
  const effR = Math.max(1, rCount);
  const metaLayers = params.metaLayers ?? 0;

  const motifCountsHazard = new Map();
  const motifEpSumHazard = new Map();
  const motifEpAbsSumHazard = new Map();
  const motifCountsOutside = new Map();
  const motifEpSumOutside = new Map();
  const motifEpAbsSumOutside = new Map();
  const transHazard = new Map();
  const transOutside = new Map();

  let totalMovesHazard = 0;
  let totalMovesOutside = 0;
  let totalEpHazard = 0;
  let totalEpOutside = 0;
  let acceptLogOverflowed = false;

  let prevHazardMotif = null;
  let prevOutsideMotif = null;

  const maxEventStart = steps - eventEvery;
  let eventIdx = 0;
  let t = 0;
  const chunkSteps = 10000;

  const applyEventAt = (tEvent) => {
    const perturb = {
      target: "metaS",
      layer: 0,
      frac: corruptFrac,
      mode: "randomize",
    };
    if (regionType === "stripe") {
      perturb.region = "stripe";
      perturb.bins = bins;
      perturb.span = gateSpan;
      perturb.bin = regionIndex;
    } else {
      perturb.region = "quadrant";
      perturb.quadrant = regionIndex;
    }
    perturb.seed = args.seed * 1000 + tEvent;
    sim.apply_perturbation(perturb);
  };

  const addMotif = (region, motifId, epDelta) => {
    if (region === "hazard") {
      motifCountsHazard.set(motifId, (motifCountsHazard.get(motifId) ?? 0) + 1);
      motifEpSumHazard.set(motifId, (motifEpSumHazard.get(motifId) ?? 0) + epDelta);
      motifEpAbsSumHazard.set(
        motifId,
        (motifEpAbsSumHazard.get(motifId) ?? 0) + Math.abs(epDelta),
      );
      totalMovesHazard += 1;
      totalEpHazard += epDelta;
    } else {
      motifCountsOutside.set(motifId, (motifCountsOutside.get(motifId) ?? 0) + 1);
      motifEpSumOutside.set(motifId, (motifEpSumOutside.get(motifId) ?? 0) + epDelta);
      motifEpAbsSumOutside.set(
        motifId,
        (motifEpAbsSumOutside.get(motifId) ?? 0) + Math.abs(epDelta),
      );
      totalMovesOutside += 1;
      totalEpOutside += epDelta;
    }
  };

  const updateTransitionEdge = (map, from, to) => {
    const key = `${from}->${to}`;
    map.set(key, (map.get(key) ?? 0) + 1);
  };

  const processLogEntries = (u32, ep) => {
    const entries = Math.min(ep.length, Math.floor(u32.length / 3));
    for (let i = 0; i < entries; i += 1) {
      const tEntry = u32[i * 3];
      const q = u32[i * 3 + 1];
      const meta = u32[i * 3 + 2];
      const moveId = meta & 0xff;
      if (moveId !== idxP5Base && moveId !== idxP5Meta) continue;
      if (tEntry < eventEvery) continue;
      const t0 = Math.floor(tEntry / eventEvery) * eventEvery;
      if (t0 < eventEvery) continue;
      if (t0 > maxEventStart) continue;
      if (tEntry - t0 >= deadline) continue;

      const qIdx = Number(q);
      const inHazard = regionMask[qIdx];
      const inOutside = outsideMask[qIdx];
      if (!inHazard && !inOutside) continue;

      const layerByte = (meta >>> 8) & 0xff;
      const mismatchBin = (meta >>> 16) & 0xff;
      const kDir = (meta >>> 24) & 0xff;

      let motifId;
      if (moveId === idxP5Base) {
        motifId = (kDir % effR) * 3 + mismatchBin;
      } else {
        const layer = layerByte;
        if (layer >= metaLayers) continue;
        motifId = effR * 3 + layer * (effR * 3) + (kDir % effR) * 3 + mismatchBin;
      }

      const epDelta = ep[i] ?? 0;
      if (inHazard) {
        if (prevHazardMotif !== null) {
          updateTransitionEdge(transHazard, prevHazardMotif, motifId);
        }
        prevHazardMotif = motifId;
        addMotif("hazard", motifId, epDelta);
      } else if (inOutside) {
        if (prevOutsideMotif !== null) {
          updateTransitionEdge(transOutside, prevOutsideMotif, motifId);
        }
        prevOutsideMotif = motifId;
        addMotif("outside", motifId, epDelta);
      }
    }
  };

  while (t < steps) {
    const nextEventTime = (eventIdx + 1) * eventEvery;
    const next = Math.min(t + chunkSteps, nextEventTime, steps);
    if (next > t) {
      sim.step(next - t);
      t = next;
    }
    if (nextEventTime <= maxEventStart && t === nextEventTime) {
      applyEventAt(nextEventTime);
      eventIdx += 1;
    }

    if (sim.accept_log_len) {
      const len = sim.accept_log_len();
      if (len > 0) {
        const u32 = sim.accept_log_u32();
        const ep = sim.accept_log_ep();
        processLogEntries(u32, ep);
      }
      acceptLogOverflowed = sim.accept_log_overflowed();
      sim.accept_log_clear();
      if (acceptLogOverflowed) {
        throw new Error("ACCEPT_LOG_OVERFLOW: reduce chunkSteps or raise acceptLogCap");
      }
    }
  }

  const summary = {
    seed: args.seed,
    condition: args.condition,
    steps,
    eventEvery,
    deadline,
    rCount,
    metaLayers,
    totalMovesHazard,
    totalMovesOutside,
    totalEpHazard,
    totalEpOutside,
    uniqueMotifsHazard: motifCountsHazard.size,
    uniqueMotifsOutside: motifCountsOutside.size,
    acceptLogOverflowed,
  };

  return {
    summary,
    motifCountsHazard,
    motifEpSumHazard,
    motifEpAbsSumHazard,
    motifCountsOutside,
    motifEpSumOutside,
    motifEpAbsSumOutside,
    transHazard,
    transOutside,
  };
}

async function runOnce(args) {
  const mod = await loadWasm();
  const presetRaw = readJson(args.presetPath);
  const baseParams = presetRaw.params ?? presetRaw;
  const params = { ...baseParams };
  applySets(params, args.sets);

  if (args.condition === "A") {
    params.opCouplingOn = 0;
    params.sCouplingMode = 0;
    params.opDriveOnK = 0;
  } else if (args.condition === "B") {
    params.opCouplingOn = 1;
    params.sCouplingMode = 1;
    params.opDriveOnK = 0;
  } else if (args.condition === "C") {
    params.opCouplingOn = 1;
    params.sCouplingMode = 1;
    params.opDriveOnK = 1;
  } else if (args.condition === "D") {
    params.opCouplingOn = 1;
    params.sCouplingMode = 1;
    params.opDriveOnK = 1;
  }

  const steps = args.steps ?? presetRaw.steps ?? 2_000_000;
  const reportEvery = args.reportEvery ?? presetRaw.reportEvery ?? 5_000;
  const eventEvery = args.eventEvery ?? presetRaw.eventEvery ?? 50_000;
  const deadline = args.deadline ?? presetRaw.deadline ?? 25_000;
  const regionType = args.regionType ?? (params.repairGateMode === 1 ? "stripe" : "quadrant");
  const regionIndex = args.regionIndex ?? 2;
  const gateSpan = args.gateSpan ?? params.repairGateSpan ?? 1;
  const corruptFrac = args.corruptFrac ?? 0.2;
  const tailWindow = args.tailWindow ?? 200_000;
  const gateConditioned = Number(args.gateConditioned ?? 1) === 1;
  const gateCheckEvery = Math.max(1, Math.floor(args.gateCheckEvery ?? 5_000));
  const sampleEvery = gateCheckEvery;
  const wPre = Math.min(50_000, deadline);

  const bins = params.clockK ?? 8;
  const gridSize = params.gridSize ?? 32;
  const cells = gridSize * gridSize;
  const regionMask = buildRegionMask(gridSize, regionType, regionIndex, gateSpan, bins);
  const outsideMask = buildMatchedOutsideMask(regionMask, args.seed + 999);

  const sim = new mod.Sim(50, args.seed);
  sim.set_params({ ...params, epDebug: 1 });
  if (args.burnIn > 0) sim.step(args.burnIn);

  const moveLabels = sim.ep_move_labels ? Array.from(sim.ep_move_labels()) : [];
  const idxP5Base = moveLabels.indexOf("P5Base");
  const idxP5Meta = moveLabels.indexOf("P5Meta");
  const idxOpK = moveLabels.indexOf("OpK");
  const idxClock = moveLabels.indexOf("Clock");
  if ([idxP5Base, idxP5Meta, idxOpK, idxClock].some((idx) => idx < 0)) {
    throw new Error(`ep_move_labels missing expected labels: ${JSON.stringify(moveLabels)}`);
  }

  const rCount = sim.op_r_count ? sim.op_r_count() : 0;
  const offsets = rCount > 0 ? parseOpOffsets(sim.op_offsets()) : [];
  const opBudgetK = params.opBudgetK ?? 1;
  const lS = params.lS ?? 1;

  const events = [];
  for (let t = eventEvery; t + deadline <= steps; t += eventEvery) {
    events.push({
      idx: events.length,
      tEvent: t,
      recovered: false,
      miss: false,
      recoveryTime: null,
      stepsToOutcome: null,
      startEp: null,
      deltaEp: null,
      opBinsMode: args.opBinsMode,
      seed: args.seed,
      condition: args.condition,
      pre: emptyWindowStats(),
      recovery: emptyWindowStats(),
      tail: emptyWindowStats(),
    });
  }

  let nextReport = reportEvery;
  let nextSample = sampleEvery;
  let eventIdx = 0;
  let lastEventTime = null;
  const graceWindow = Math.max(0, Math.floor(0.2 * deadline));
  const reportRecords = [];

  let prevBase = null;
  let prevOp = null;
  let prevEpByMove = null;
  let prevEpTotal = null;
  let hazGateOpen = false;
  let hazardGateSampleCount = 0;
  if ((params.metaLayers ?? 0) > 0) {
    const baseS = sim.base_s_field();
    const metaS = sim.meta_field();
    const tokens = sim.op_k_tokens();
    prevBase = computeMBaseClasses({ baseS, metaS, gridSize, lS, metaLayers: params.metaLayers ?? 0 });
    prevOp = computeMOpClasses({
      baseS,
      metaS,
      gridSize,
      lS,
      metaLayers: params.metaLayers ?? 0,
      tokens,
      rCount,
      offsets,
      opBudgetK,
      opBinsMode: args.opBinsMode,
    });
  }

  const hazardBaseSamples = [];
  const outsideBaseSamples = [];
  const hazardOpSamples = [];
  const outsideOpSamples = [];
  const changeBaseHazard = [];
  const changeBaseOutside = [];
  const changeOpHazard = [];
  const changeOpOutside = [];

  const runCountsBaseHazard = new Map();
  const runCountsBaseOutside = new Map();
  const runCountsOpHazard = new Map();
  const runCountsOpOutside = new Map();
  const runTransBaseHazard = new Map();
  const runTransBaseOutside = new Map();
  const runTransOpHazard = new Map();
  const runTransOpOutside = new Map();

  const edgeEpRepairBase = new Map();
  const edgeEpOpKBase = new Map();
  const edgeEpTotalBase = new Map();
  const edgeEpRepairOp = new Map();
  const edgeEpOpKOp = new Map();
  const edgeEpTotalOp = new Map();

  const epPerChangeBase = [];
  const epPerChangeOp = [];

  for (let t = 0; t < steps; ) {
    const nextEventTime = eventIdx < events.length ? events[eventIdx].tEvent : Number.POSITIVE_INFINITY;
    const next = Math.min(nextReport, nextSample, nextEventTime, steps);
    if (next > t) {
      sim.step(next - t);
      t = next;
    }

    if (eventIdx < events.length && t >= events[eventIdx].tEvent) {
      for (; eventIdx < events.length && events[eventIdx].tEvent <= t; eventIdx += 1) {
        const event = events[eventIdx];
        const perturb = {
          target: "metaS",
          layer: 0,
          frac: corruptFrac,
          mode: "randomize",
        };
        if (regionType === "stripe") {
          perturb.region = "stripe";
          perturb.bins = bins;
          perturb.span = gateSpan;
          perturb.bin = regionIndex;
        } else {
          perturb.region = "quadrant";
          perturb.quadrant = regionIndex;
        }
        perturb.seed = args.seed * 1000 + event.tEvent;
        sim.apply_perturbation(perturb);
        lastEventTime = event.tEvent;
        const epByMove = sim.ep_exact_by_move ? sim.ep_exact_by_move() : [];
        event.startEp = {
          total: sim.ep_exact_total ? sim.ep_exact_total() : 0,
          repair: (epByMove[idxP5Base] ?? 0) + (epByMove[idxP5Meta] ?? 0),
          opK: epByMove[idxOpK] ?? 0,
          clock: epByMove[idxClock] ?? 0,
        };
      }
    }

    if (t === nextSample && (params.metaLayers ?? 0) > 0) {
      const gateOpenNow = gateConditioned
        ? hazardGateActive(sim.clock_state(), params, regionIndex, gateSpan)
        : true;
      if (gateConditioned && !gateOpenNow) {
        hazGateOpen = false;
        prevBase = null;
        prevOp = null;
        prevEpByMove = null;
        prevEpTotal = null;
        nextSample += sampleEvery;
        continue;
      }

      const baseS = sim.base_s_field();
      const metaS = sim.meta_field();
      const tokens = sim.op_k_tokens();
      const currentBase = computeMBaseClasses({
        baseS,
        metaS,
        gridSize,
        lS,
        metaLayers: params.metaLayers ?? 0,
      });
      const currentOp = computeMOpClasses({
        baseS,
        metaS,
        gridSize,
        lS,
        metaLayers: params.metaLayers ?? 0,
        tokens,
        rCount,
        offsets,
        opBudgetK,
        opBinsMode: args.opBinsMode,
      });

      hazardGateSampleCount += 1;

      const baseHazCounts = computeCounts(currentBase, regionMask).counts;
      const baseOutCounts = computeCounts(currentBase, outsideMask).counts;
      const opHazCounts = computeCounts(currentOp, regionMask).counts;
      const opOutCounts = computeCounts(currentOp, outsideMask).counts;

      hazardBaseSamples.push(vocabStats(baseHazCounts));
      outsideBaseSamples.push(vocabStats(baseOutCounts));
      hazardOpSamples.push(vocabStats(opHazCounts));
      outsideOpSamples.push(vocabStats(opOutCounts));

      for (const [key, value] of baseHazCounts.entries()) updateCount(runCountsBaseHazard, key, value);
      for (const [key, value] of baseOutCounts.entries()) updateCount(runCountsBaseOutside, key, value);
      for (const [key, value] of opHazCounts.entries()) updateCount(runCountsOpHazard, key, value);
      for (const [key, value] of opOutCounts.entries()) updateCount(runCountsOpOutside, key, value);

      const epByMove = sim.ep_exact_by_move ? Array.from(sim.ep_exact_by_move()) : [];
      const epTotal = sim.ep_exact_total ? sim.ep_exact_total() : 0;

      const transBaseHaz = new Map();
      const transBaseOut = new Map();
      const transOpHaz = new Map();
      const transOpOut = new Map();
      let changeBaseHaz = { changed: 0, total: 0, frac: 0 };
      let changeBaseOut = { changed: 0, total: 0, frac: 0 };
      let changeOpHaz = { changed: 0, total: 0, frac: 0 };
      let changeOpOut = { changed: 0, total: 0, frac: 0 };
      let deltaTotal = 0;
      let deltaRepair = 0;
      let deltaOpK = 0;

      if (prevBase && prevOp && prevEpByMove && hazGateOpen) {
        changeBaseHaz = computeChange(prevBase, currentBase, regionMask);
        changeBaseOut = computeChange(prevBase, currentBase, outsideMask);
        changeOpHaz = computeChange(prevOp, currentOp, regionMask);
        changeOpOut = computeChange(prevOp, currentOp, outsideMask);

        changeBaseHazard.push(changeBaseHaz.frac);
        changeBaseOutside.push(changeBaseOut.frac);
        changeOpHazard.push(changeOpHaz.frac);
        changeOpOutside.push(changeOpOut.frac);

        const baseHazChanged = accumulateTransitions(prevBase, currentBase, regionMask, transBaseHaz);
        const baseOutChanged = accumulateTransitions(prevBase, currentBase, outsideMask, transBaseOut);
        const opHazChanged = accumulateTransitions(prevOp, currentOp, regionMask, transOpHaz);
        const opOutChanged = accumulateTransitions(prevOp, currentOp, outsideMask, transOpOut);

        for (const [key, value] of transBaseHaz.entries()) updateCount(runTransBaseHazard, key, value);
        for (const [key, value] of transBaseOut.entries()) updateCount(runTransBaseOutside, key, value);
        for (const [key, value] of transOpHaz.entries()) updateCount(runTransOpHazard, key, value);
        for (const [key, value] of transOpOut.entries()) updateCount(runTransOpOutside, key, value);

        deltaTotal = epTotal - (prevEpTotal ?? 0);
        const prevRepair = (prevEpByMove[idxP5Base] ?? 0) + (prevEpByMove[idxP5Meta] ?? 0);
        const currRepair = (epByMove[idxP5Base] ?? 0) + (epByMove[idxP5Meta] ?? 0);
        deltaRepair = currRepair - prevRepair;
        deltaOpK = (epByMove[idxOpK] ?? 0) - (prevEpByMove[idxOpK] ?? 0);

        epPerChangeBase.push(baseHazChanged > 0 ? deltaTotal / baseHazChanged : 0);
        epPerChangeOp.push(opHazChanged > 0 ? deltaOpK / opHazChanged : 0);

        const baseHazTotal = totalTransitions(transBaseHaz);
        if (baseHazTotal > 0) {
          for (const [key, count] of transBaseHaz.entries()) {
            const frac = count / baseHazTotal;
            updateCount(edgeEpRepairBase, key, frac * deltaRepair);
            updateCount(edgeEpOpKBase, key, frac * deltaOpK);
            updateCount(edgeEpTotalBase, key, frac * deltaTotal);
          }
        }

        const opHazTotal = totalTransitions(transOpHaz);
        if (opHazTotal > 0) {
          for (const [key, count] of transOpHaz.entries()) {
            const frac = count / opHazTotal;
            updateCount(edgeEpRepairOp, key, frac * deltaRepair);
            updateCount(edgeEpOpKOp, key, frac * deltaOpK);
            updateCount(edgeEpTotalOp, key, frac * deltaTotal);
          }
        }
      }

      for (const event of events) {
        const window = eventWindowForTime(event, t, wPre, tailWindow, deadline);
        if (!window) continue;
        const slot = event[window];
        for (const [key, value] of baseHazCounts.entries()) updateCount(slot.countsBaseHazard, key, value);
        for (const [key, value] of baseOutCounts.entries()) updateCount(slot.countsBaseOutside, key, value);
        for (const [key, value] of opHazCounts.entries()) updateCount(slot.countsOpHazard, key, value);
        for (const [key, value] of opOutCounts.entries()) updateCount(slot.countsOpOutside, key, value);
        for (const [key, value] of transBaseHaz.entries()) updateCount(slot.transBaseHazard, key, value);
        for (const [key, value] of transBaseOut.entries()) updateCount(slot.transBaseOutside, key, value);
        for (const [key, value] of transOpHaz.entries()) updateCount(slot.transOpHazard, key, value);
        for (const [key, value] of transOpOut.entries()) updateCount(slot.transOpOutside, key, value);
        slot.changeBaseHazard += changeBaseHaz.frac;
        slot.changeBaseOutside += changeBaseOut.frac;
        slot.changeOpHazard += changeOpHaz.frac;
        slot.changeOpOutside += changeOpOut.frac;
        slot.changeBaseHazardCount += changeBaseHaz.changed;
        slot.changeBaseHazardTotal += changeBaseHaz.total;
        slot.changeBaseOutsideCount += changeBaseOut.changed;
        slot.changeBaseOutsideTotal += changeBaseOut.total;
        slot.changeOpHazardCount += changeOpHaz.changed;
        slot.changeOpHazardTotal += changeOpHaz.total;
        slot.changeOpOutsideCount += changeOpOut.changed;
        slot.changeOpOutsideTotal += changeOpOut.total;
        slot.epTotal += deltaTotal;
        slot.epRepair += deltaRepair;
        slot.epOpK += deltaOpK;
        slot.samples += 1;
      }

      prevBase = currentBase;
      prevOp = currentOp;
      prevEpByMove = epByMove;
      prevEpTotal = epTotal;
      hazGateOpen = true;
      nextSample += sampleEvery;
    }

    if (t === nextReport) {
      const baseS = sim.base_s_field();
      const metaS = sim.meta_field();
      const meta0 = metaS.subarray(0, cells);
      const sdiff = meanAbsDiffRegion(baseS, meta0, regionMask);
      const errSample = errRegionBits(baseS, meta0, gridSize, lS, regionMask);
      const errAdj = errSample;
      const good = sdiff <= args.sdiffGood && errAdj <= args.errGood;
      const sinceEvent = lastEventTime === null ? Number.POSITIVE_INFINITY : t - lastEventTime;
      reportRecords.push({ t, err: errAdj, sdiff, good, sinceEvent });

      for (const event of events) {
        if (event.recovered || event.miss) continue;
        if (t < event.tEvent) continue;
        const elapsed = t - event.tEvent;
        if (elapsed > deadline) {
          event.miss = true;
          event.stepsToOutcome = elapsed;
        } else if (good) {
          event.recovered = true;
          event.recoveryTime = elapsed;
          event.stepsToOutcome = elapsed;
        }
      }

      nextReport += reportEvery;
    }
  }

  const epByMove = sim.ep_exact_by_move ? sim.ep_exact_by_move() : [];
  const endEp = {
    total: sim.ep_exact_total ? sim.ep_exact_total() : 0,
    repair: (epByMove[idxP5Base] ?? 0) + (epByMove[idxP5Meta] ?? 0),
    opK: epByMove[idxOpK] ?? 0,
    clock: epByMove[idxClock] ?? 0,
  };

  for (const event of events) {
    if (!event.recovered && !event.miss) {
      event.miss = true;
      event.stepsToOutcome = deadline;
    }
    if (event.startEp) {
      event.deltaEp = {
        total: endEp.total - event.startEp.total,
        repair: endEp.repair - event.startEp.repair,
        opK: endEp.opK - event.startEp.opK,
        clock: endEp.clock - event.startEp.clock,
      };
    }
    event.recoverySamples = event.recovery.samples;
  }

  const missCount = events.filter((e) => e.miss).length;
  const recoveries = events.filter((e) => e.recovered).map((e) => e.recoveryTime ?? 0);
  const tailStart = Math.max(0, steps - tailWindow);
  const tailSamples = reportRecords.filter(
    (s) => s.t >= tailStart && s.sinceEvent >= graceWindow,
  );
  const tailUptime = tailSamples.length ? tailSamples.filter((s) => s.good).length / tailSamples.length : 0;
  const tailErrMean = tailSamples.length ? mean(tailSamples.map((s) => s.err)) : 0;
  const tailSdiffMean = tailSamples.length ? mean(tailSamples.map((s) => s.sdiff)) : 0;

  const qCounts = sim.ep_q_stats().count ?? [];
  const repairRate = steps > 0 ? (qCounts[idxP5Meta] ?? 0) / steps : 0;
  const opkRate = steps > 0 ? (qCounts[idxOpK] ?? 0) / steps : 0;
  const eventRecoveryWindowSampleCountMean = events.length
    ? mean(events.map((e) => e.recovery.samples))
    : 0;
  const hazardOpChangeFracMean = changeOpHazard.length ? mean(changeOpHazard) : 0;

  const summary = {
    seed: args.seed,
    condition: args.condition,
    opBinsMode: args.opBinsMode,
    gateConditioned: gateConditioned ? 1 : 0,
    gateCheckEvery,
    steps,
    reportEvery,
    eventEvery,
    deadline,
    regionType,
    regionIndex,
    gateSpan,
    sampleEvery,
    burnIn: args.burnIn,
    missFrac: events.length > 0 ? missCount / events.length : 0,
    recoveryMean: recoveries.length ? mean(recoveries) : null,
    recoveryP95: percentile(recoveries, 0.95),
    uptimeTail: tailUptime,
    errTailMean: tailErrMean,
    sdiffTailMean: tailSdiffMean,
    epTotalRate: steps > 0 ? endEp.total / steps : 0,
    epRepairRate: steps > 0 ? endEp.repair / steps : 0,
    epOpKRate: steps > 0 ? endEp.opK / steps : 0,
    epClockRate: steps > 0 ? endEp.clock / steps : 0,
    repairRate,
    opKRate: opkRate,
    hazardGateSampleCount,
    hazardOpUniqueStatesVisited: runCountsOpHazard.size,
    hazardOpChangeFracMean,
    eventRecoveryWindowSampleCountMean,
    hazardBase_H: summarizeValues(hazardBaseSamples.map((s) => s.H_vocab)),
    hazardBase_Veff: summarizeValues(hazardBaseSamples.map((s) => s.V_eff)),
    hazardBase_topMass10: summarizeValues(hazardBaseSamples.map((s) => s.topMass10)),
    hazardBase_changeFrac: summarizeValues(changeBaseHazard),
    outsideBase_H: summarizeValues(outsideBaseSamples.map((s) => s.H_vocab)),
    outsideBase_Veff: summarizeValues(outsideBaseSamples.map((s) => s.V_eff)),
    outsideBase_topMass10: summarizeValues(outsideBaseSamples.map((s) => s.topMass10)),
    outsideBase_changeFrac: summarizeValues(changeBaseOutside),
    hazardOp_H: summarizeValues(hazardOpSamples.map((s) => s.H_vocab)),
    hazardOp_Veff: summarizeValues(hazardOpSamples.map((s) => s.V_eff)),
    hazardOp_topMass10: summarizeValues(hazardOpSamples.map((s) => s.topMass10)),
    hazardOp_changeFrac: summarizeValues(changeOpHazard),
    outsideOp_H: summarizeValues(outsideOpSamples.map((s) => s.H_vocab)),
    outsideOp_Veff: summarizeValues(outsideOpSamples.map((s) => s.V_eff)),
    outsideOp_topMass10: summarizeValues(outsideOpSamples.map((s) => s.topMass10)),
    outsideOp_changeFrac: summarizeValues(changeOpOutside),
    asymmetryBase: { mean: asymmetryScore(runTransBaseHazard), std: null },
    asymmetryOp: { mean: asymmetryScore(runTransOpHazard), std: null },
    coarseEPBase: { mean: coarseEPSmoothed(runTransBaseHazard, 0.5), std: null },
    coarseEPOp: { mean: coarseEPSmoothed(runTransOpHazard, 0.5), std: null },
    epPerChangeBase: summarizeValues(epPerChangeBase),
    epPerChangeOp: summarizeValues(epPerChangeOp),
    uniqueBaseHazard: runCountsBaseHazard.size,
    uniqueBaseOutside: runCountsBaseOutside.size,
    uniqueOpHazard: runCountsOpHazard.size,
    uniqueOpOutside: runCountsOpOutside.size,
    transitionsBaseHazard: mapToPairs(runTransBaseHazard),
    transitionsBaseOutside: mapToPairs(runTransBaseOutside),
    transitionsOpHazard: mapToPairs(runTransOpHazard),
    transitionsOpOutside: mapToPairs(runTransOpOutside),
    countsBaseHazard: mapToPairs(runCountsBaseHazard),
    countsBaseOutside: mapToPairs(runCountsBaseOutside),
    countsOpHazard: mapToPairs(runCountsOpHazard),
    countsOpOutside: mapToPairs(runCountsOpOutside),
    edgeEpRepairBase: mapToPairs(edgeEpRepairBase),
    edgeEpOpKBase: mapToPairs(edgeEpOpKBase),
    edgeEpTotalBase: mapToPairs(edgeEpTotalBase),
    edgeEpRepairOp: mapToPairs(edgeEpRepairOp),
    edgeEpOpKOp: mapToPairs(edgeEpOpKOp),
    edgeEpTotalOp: mapToPairs(edgeEpTotalOp),
  };

  const eventRows = events.map((event) => buildEventRow(event));

  return { summary, events: eventRows };
}

export async function runDeadlineOpkMotifEvents(options) {
  const args = options ?? parseArgs(process.argv);
  if (!args.presetPath) throw new Error("--preset is required");
  if (args.motifMode === "move_edges") {
    const result = await runOnceMoveEdges(args);
    ensureDir(args.outDir);
    const condDir = path.join(args.outDir, args.condition, `seed_${args.seed}`);
    ensureDir(condDir);
    const summaryPath = path.join(condDir, "move_edges_summary.json");
    fs.writeFileSync(summaryPath, JSON.stringify(result.summary, null, 2));

    const writeCountsCsv = (filename, counts, epSum, epAbsSum) => {
      const header = "fromIdx,toIdx,count,epSum,epAbsSum";
      const lines = [header];
      for (const [key, count] of counts.entries()) {
        const [fromIdx, toIdx] = key.split("->");
        lines.push(
          [fromIdx, toIdx, count, epSum.get(key) ?? 0, epAbsSum.get(key) ?? 0].join(","),
        );
      }
      fs.writeFileSync(path.join(condDir, filename), lines.join("\n") + "\n");
    };

    writeCountsCsv(
      "move_edges_counts_hazard.csv",
      result.edgeCountHazard,
      result.edgeEpSumHazard,
      result.edgeEpAbsSumHazard,
    );
    writeCountsCsv(
      "move_edges_counts_outside.csv",
      result.edgeCountOutside,
      result.edgeEpSumOutside,
      result.edgeEpAbsSumOutside,
    );

    const famLines = ["region,family,count"];
    if (result.edgeFamCountHazard && result.edgeFamCountHazard.size > 0) {
      for (const [family, count] of result.edgeFamCountHazard.entries()) {
        famLines.push(["hazard", family, count].join(","));
      }
    }
    if (result.edgeFamCountOutside && result.edgeFamCountOutside.size > 0) {
      for (const [family, count] of result.edgeFamCountOutside.entries()) {
        famLines.push(["outside", family, count].join(","));
      }
    }
    if (famLines.length > 1) {
      fs.writeFileSync(path.join(condDir, "move_edges_families.csv"), famLines.join("\n") + "\n");
    }
    return { summary: result.summary, events: [], outDir: condDir };
  }
  if (args.motifMode === "p5_actions") {
    const result = await runOnceP5Actions(args);
    ensureDir(args.outDir);
    const condDir = path.join(args.outDir, args.condition, `seed_${args.seed}`);
    ensureDir(condDir);
    const summaryPath = path.join(condDir, "p5_actions_summary.json");
    fs.writeFileSync(summaryPath, JSON.stringify(result.summary, null, 2));

    const writeCountsCsv = (filename, counts, epSum, epAbsSum) => {
      const header = "motifId,count,epSum,epAbsSum";
      const lines = [header];
      for (const [motifId, count] of counts.entries()) {
        lines.push(
          [motifId, count, epSum.get(motifId) ?? 0, epAbsSum.get(motifId) ?? 0].join(","),
        );
      }
      fs.writeFileSync(path.join(condDir, filename), lines.join("\n") + "\n");
    };

    const writeTransitionsCsv = (filename, transCounts) => {
      const header = "fromMotif,toMotif,count,countRev";
      const lines = [header];
      for (const [key, count] of transCounts.entries()) {
        const [fromMotif, toMotif] = key.split("->");
        const revKey = `${toMotif}->${fromMotif}`;
        const countRev = transCounts.get(revKey) ?? 0;
        lines.push([fromMotif, toMotif, count, countRev].join(","));
      }
      fs.writeFileSync(path.join(condDir, filename), lines.join("\n") + "\n");
    };

    writeCountsCsv(
      "p5_actions_counts_hazard.csv",
      result.motifCountsHazard,
      result.motifEpSumHazard,
      result.motifEpAbsSumHazard,
    );
    writeCountsCsv(
      "p5_actions_counts_outside.csv",
      result.motifCountsOutside,
      result.motifEpSumOutside,
      result.motifEpAbsSumOutside,
    );
    writeTransitionsCsv("p5_actions_transitions_hazard.csv", result.transHazard);
    writeTransitionsCsv("p5_actions_transitions_outside.csv", result.transOutside);

    return { summary: result.summary, events: [], outDir: condDir };
  }

  let lastResult = null;
  lastResult = await runOnce(args);

  ensureDir(args.outDir);
  const condDir = path.join(args.outDir, args.condition);
  ensureDir(condDir);
  const eventsPath = path.join(condDir, `seed_${args.seed}_events.jsonl`);
  const summaryPath = path.join(condDir, `seed_${args.seed}_run_summary.json`);

  fs.writeFileSync(eventsPath, lastResult.events.map((row) => JSON.stringify(row)).join("\n") + "\n");
  fs.writeFileSync(summaryPath, JSON.stringify(lastResult.summary, null, 2));

  const writeEdgeCsv = (filename, family, transPairs, epRepairPairs, epOpKPairs, epTotalPairs) => {
    const counts = new Map(transPairs ?? []);
    const epRepair = new Map(epRepairPairs ?? []);
    const epOpK = new Map(epOpKPairs ?? []);
    const epTotal = new Map(epTotalPairs ?? []);
    const rows = [];
    for (const [key, count] of counts.entries()) {
      const [from, to] = String(key).split("|");
      if (from === to) continue;
      const revKey = `${to}|${from}`;
      const countRev = counts.get(revKey) ?? 0;
      const epRepairSum = epRepair.get(key) ?? 0;
      const epOpKSum = epOpK.get(key) ?? 0;
      const epTotalSum = epTotal.get(key) ?? 0;
      rows.push({
        condition: args.condition,
        seed: args.seed,
        region: "hazard",
        family,
        from,
        to,
        count,
        countRev,
        epRepairSum,
        epOpKSum,
        epTotalSum,
        epRepairPerTrans: count > 0 ? epRepairSum / count : 0,
        epOpKPerTrans: count > 0 ? epOpKSum / count : 0,
        epTotalPerTrans: count > 0 ? epTotalSum / count : 0,
      });
    }
    const header =
      "condition,seed,region,family,from,to,count,countRev,epRepairSum,epOpKSum,epTotalSum,epRepairPerTrans,epOpKPerTrans,epTotalPerTrans";
    const lines = [header];
    for (const row of rows) {
      lines.push(
        [
          row.condition,
          row.seed,
          row.region,
          row.family,
          row.from,
          row.to,
          row.count,
          row.countRev,
          row.epRepairSum,
          row.epOpKSum,
          row.epTotalSum,
          row.epRepairPerTrans,
          row.epOpKPerTrans,
          row.epTotalPerTrans,
        ].join(","),
      );
    }
    fs.writeFileSync(path.join(condDir, filename), lines.join("\n") + "\n");
  };

  writeEdgeCsv(
    `transition_edges_Mbase_seed_${args.seed}.csv`,
    "M_base",
    lastResult.summary.transitionsBaseHazard,
    lastResult.summary.edgeEpRepairBase,
    lastResult.summary.edgeEpOpKBase,
    lastResult.summary.edgeEpTotalBase,
  );
  writeEdgeCsv(
    `transition_edges_Mop_seed_${args.seed}.csv`,
    "M_op",
    lastResult.summary.transitionsOpHazard,
    lastResult.summary.edgeEpRepairOp,
    lastResult.summary.edgeEpOpKOp,
    lastResult.summary.edgeEpTotalOp,
  );

  if (lastResult.summary.hazardGateSampleCount < 10) {
    console.log("MOTIF_INSTRUMENTATION_TOO_SPARSE_GATE_SAMPLES");
  }
  if (lastResult.summary.eventRecoveryWindowSampleCountMean === 0) {
    console.log("RECOVERY_WINDOW_UNOBSERVED");
  }
  if (lastResult.summary.hazardOpUniqueStatesVisited < 10) {
    console.log("MOP_COLLAPSED");
  }

  return { summary: lastResult.summary, events: lastResult.events, outDir: condDir };
}

if (import.meta.url === `file://${process.argv[1]}`) {
  runDeadlineOpkMotifEvents().catch((err) => {
    console.error(err);
    process.exit(1);
  });
}
</file>

<file path="scripts/run-deadline-opk-motif-p2-ablation.mjs">
#!/usr/bin/env node
import fs from "node:fs";
import path from "node:path";
import { mean, std, readJson, parseSeedList } from "./deadline-event-utils.mjs";
import { runDeadlineOpkMotifEvents } from "./run-deadline-opk-motif-events.mjs";
import { coarseEPSmoothed, jsDivergence } from "./opk-motif-basis.mjs";

const rootDir = path.resolve(path.dirname(new URL(import.meta.url).pathname), "..");
const outDir = path.resolve(rootDir, ".tmp", "motif_pressure_v3");

function ensureDir(dir) {
  fs.mkdirSync(dir, { recursive: true });
}

function pickPreset() {
  const preferred = path.resolve(rootDir, "scripts/params/op_motifs_selection/selection_base_tuned.json");
  const fallback = path.resolve(rootDir, "scripts/params/clock_code/code_deadline_gated_clock.json");
  if (fs.existsSync(preferred)) return preferred;
  return fallback;
}

function parseCsv(pathname) {
  if (!fs.existsSync(pathname)) return null;
  const raw = fs.readFileSync(pathname, "utf8").trim();
  if (!raw) return null;
  const lines = raw.split(/\r?\n/);
  const header = lines[0].split(",");
  const rows = [];
  for (let i = 1; i < lines.length; i += 1) {
    if (!lines[i]) continue;
    const cols = lines[i].split(",");
    const row = {};
    header.forEach((key, idx) => {
      row[key] = cols[idx];
    });
    rows.push(row);
  }
  return rows;
}

function meanStd(values) {
  const nums = values.filter((v) => Number.isFinite(v));
  return {
    mean: nums.length ? mean(nums) : null,
    std: nums.length ? std(nums) : null,
  };
}

function pairsToMap(pairs) {
  const map = new Map();
  for (const [key, count] of pairs ?? []) {
    map.set(String(key), (map.get(String(key)) ?? 0) + count);
  }
  return map;
}

function totalTransitions(counts) {
  let total = 0;
  for (const [key, count] of counts.entries()) {
    const [from, to] = key.split("|");
    if (from === to) continue;
    total += count;
  }
  return total;
}

function pickBestOpBinsMode() {
  const summaryPath = path.join(outDir, "compare_summary.csv");
  const rows = parseCsv(summaryPath);
  if (!rows) return 1;
  let best = { mode: 1, score: -Infinity };
  for (const row of rows) {
    if (!row.opBinsMode) continue;
    if (row.condition !== "B_op_noKdrive" && row.condition !== "C_op_withKdrive") continue;
    const score = Number(row.hazardOp_uniqueStatesMean ?? 0);
    if (Number.isFinite(score) && score > best.score) {
      best = { mode: Number(row.opBinsMode), score };
    }
  }
  return best.mode;
}

function eventRecoveryStats(events, family) {
  const countsSucc = new Map();
  const countsFail = new Map();
  const js = (a, b) => jsDivergence(a, b);
  const epTotalSucc = [];
  const epTotalFail = [];
  const epRepairSucc = [];
  const epRepairFail = [];
  const epOpSucc = [];
  const epOpFail = [];
  const changeSucc = [];
  const changeFail = [];

  for (const event of events) {
    const countsKey = family === "M_base" ? "recBaseHaz_counts" : "recOpHaz_counts";
    const pairs = event[countsKey] ?? [];
    const counts = pairsToMap(pairs);
    const target = event.success ? countsSucc : countsFail;
    for (const [key, val] of counts.entries()) {
      target.set(key, (target.get(key) ?? 0) + val);
    }

    if (family === "M_base") {
      const baseChange = event.rec_baseChangeCount ?? 0;
      const epTotal = event.rec_epTotal ?? 0;
      const epRepair = event.rec_epRepair ?? 0;
      const epTotalPer = baseChange > 0 ? epTotal / baseChange : 0;
      const epRepairPer = baseChange > 0 ? epRepair / baseChange : 0;
      if (event.success) {
        epTotalSucc.push(epTotalPer);
        epRepairSucc.push(epRepairPer);
      } else {
        epTotalFail.push(epTotalPer);
        epRepairFail.push(epRepairPer);
      }
    } else {
      const opChange = event.rec_opChangeCount ?? 0;
      const epOp = event.rec_epOpK ?? 0;
      const epOpPer = opChange > 0 ? epOp / opChange : 0;
      if (event.success) epOpSucc.push(epOpPer);
      else epOpFail.push(epOpPer);
    }

    const changeKey = family === "M_base" ? "rec_baseChange" : "rec_opChange";
    const change = event[changeKey] ?? 0;
    if (event.success) changeSucc.push(change);
    else changeFail.push(change);
  }

  return {
    js_divergence: js(countsSucc, countsFail),
    epTotalPerChange_succ: epTotalSucc.length ? mean(epTotalSucc) : null,
    epTotalPerChange_fail: epTotalFail.length ? mean(epTotalFail) : null,
    epRepairPerChange_succ: epRepairSucc.length ? mean(epRepairSucc) : null,
    epRepairPerChange_fail: epRepairFail.length ? mean(epRepairFail) : null,
    epOpKPerChange_succ: epOpSucc.length ? mean(epOpSucc) : null,
    epOpKPerChange_fail: epOpFail.length ? mean(epOpFail) : null,
    changeFrac_succ: changeSucc.length ? mean(changeSucc) : null,
    changeFrac_fail: changeFail.length ? mean(changeFail) : null,
  };
}

ensureDir(outDir);
const presetPath = pickPreset();
const presetRaw = readJson(presetPath);
const baseParams = presetRaw.params ?? presetRaw;
const deadline = presetRaw.deadline ?? 25_000;
const steps = presetRaw.steps ?? 2_000_000;
const reportEvery = presetRaw.reportEvery ?? 5_000;
const eventEvery = presetRaw.eventEvery ?? 50_000;
const seeds = parseSeedList(process.env.SEEDS ?? "1,2,3,4,5,6,7,8,9,10");

const opBinsMode = pickBestOpBinsMode();

const conditions = [
  { id: "A_legacy", condition: "A", overrides: { opCouplingOn: 0, sCouplingMode: 0 } },
  { id: "B_op_noKdrive", condition: "B", overrides: { opCouplingOn: 1, sCouplingMode: 1, opDriveOnK: 0 } },
  { id: "C_op_withKdrive", condition: "C", overrides: { opCouplingOn: 1, sCouplingMode: 1, opDriveOnK: 1 } },
];

const p2Modes = [
  { id: "p2_off", params: { pAWrite: 0, pNWrite: 0 } },
  { id: "p2_on", params: { pAWrite: 0.03, pNWrite: 0.03 } },
];

const rows = [];
for (const p2 of p2Modes) {
  for (const cond of conditions) {
    const runSummaries = [];
    const allEvents = [];
    const transBaseHaz = new Map();
    const transOpHaz = new Map();

    for (const seed of seeds) {
      const result = await runDeadlineOpkMotifEvents({
        presetPath,
        seed,
        condition: cond.condition,
        outDir,
        steps,
        reportEvery,
        eventEvery,
        deadline,
        regionType: baseParams.repairGateMode === 1 ? "stripe" : "quadrant",
        regionIndex: 2,
        gateSpan: baseParams.repairGateSpan ?? 1,
        corruptFrac: 0.2,
        errGood: 0.1,
        sdiffGood: 1.0,
        tailWindow: 200_000,
        opBinsMode,
        sets: [
          ...Object.entries(cond.overrides).map(([k, v]) => `${k}=${v}`),
          `pAWrite=${p2.params.pAWrite}`,
          `pNWrite=${p2.params.pNWrite}`,
        ],
      });
      runSummaries.push(result.summary);
      allEvents.push(...result.events);
      for (const [key, count] of result.summary.transitionsBaseHazard ?? []) {
        transBaseHaz.set(key, (transBaseHaz.get(key) ?? 0) + count);
      }
      for (const [key, count] of result.summary.transitionsOpHazard ?? []) {
        transOpHaz.set(key, (transOpHaz.get(key) ?? 0) + count);
      }
    }

    const missFrac = meanStd(runSummaries.map((s) => s.missFrac));
    const uptimeTail = meanStd(runSummaries.map((s) => s.uptimeTail));
    const errTail = meanStd(runSummaries.map((s) => s.errTailMean));

    const hazardBaseV = meanStd(runSummaries.map((s) => s.hazardBase_Veff?.mean));
    const hazardOpV = meanStd(runSummaries.map((s) => s.hazardOp_Veff?.mean));
    const hazardOpUnique = meanStd(runSummaries.map((s) => s.uniqueOpHazard));

    const baseTransTotal = totalTransitions(transBaseHaz);
    const opTransTotal = totalTransitions(transOpHaz);
    const baseCoarse = coarseEPSmoothed(transBaseHaz, 0.5);
    const opCoarse = coarseEPSmoothed(transOpHaz, 0.5);

    const basePerTrans = baseTransTotal > 0 ? baseCoarse / baseTransTotal : 0;
    const opPerTrans = opTransTotal > 0 ? opCoarse / opTransTotal : 0;

    const baseStats = eventRecoveryStats(allEvents, "M_base");
    const opStats = eventRecoveryStats(allEvents, "M_op");

    rows.push({
      p2Mode: p2.id,
      condition: cond.id,
      opBinsMode,
      seeds: seeds.length,
      missFracMean: missFrac.mean,
      missFracStd: missFrac.std,
      uptimeTailMean: uptimeTail.mean,
      uptimeTailStd: uptimeTail.std,
      errTailMean: errTail.mean,
      errTailStd: errTail.std,
      hazardBaseVeffMean: hazardBaseV.mean,
      hazardBaseVeffStd: hazardBaseV.std,
      hazardOpVeffMean: hazardOpV.mean,
      hazardOpVeffStd: hazardOpV.std,
      hazardOpUniqueMean: hazardOpUnique.mean,
      hazardOpUniqueStd: hazardOpUnique.std,
      hazardBase_coarseEP_perTrans: basePerTrans,
      hazardOp_coarseEP_perTrans: opPerTrans,
      hazardBase_js_divergence: baseStats.js_divergence,
      hazardOp_js_divergence: opStats.js_divergence,
      hazardBase_epTotalPerChange_succ: baseStats.epTotalPerChange_succ,
      hazardBase_epTotalPerChange_fail: baseStats.epTotalPerChange_fail,
      hazardBase_epRepairPerChange_succ: baseStats.epRepairPerChange_succ,
      hazardBase_epRepairPerChange_fail: baseStats.epRepairPerChange_fail,
      hazardOp_epOpKPerChange_succ: opStats.epOpKPerChange_succ,
      hazardOp_epOpKPerChange_fail: opStats.epOpKPerChange_fail,
      hazardBase_changeFrac_succ: baseStats.changeFrac_succ,
      hazardBase_changeFrac_fail: baseStats.changeFrac_fail,
      hazardOp_changeFrac_succ: opStats.changeFrac_succ,
      hazardOp_changeFrac_fail: opStats.changeFrac_fail,
    });
  }
}

const header = Object.keys(rows[0] ?? {}).join(",");
const lines = [header];
for (const row of rows) {
  lines.push(Object.values(row).join(","));
}
fs.writeFileSync(path.join(outDir, "p2_ablation_summary.csv"), lines.join("\n") + "\n");

console.log(`p2 ablation summary written: ${path.join(outDir, "p2_ablation_summary.csv")}`);
console.log(`opBinsMode used: ${opBinsMode}`);
</file>

<file path="scripts/run-deadline-opk-repair-budget-curves.mjs">
#!/usr/bin/env node
import fs from "node:fs";
import path from "node:path";
import {
  loadWasm,
  mean,
  readJson,
  runDeadlineEvents,
} from "./deadline-event-utils.mjs";

const rootDir = path.resolve(path.dirname(new URL(import.meta.url).pathname), "..");
const outDir = path.resolve(rootDir, ".tmp", "op_coupling");

function ensureDir(dir) {
  fs.mkdirSync(dir, { recursive: true });
}

function pickPreset() {
  const preferred = path.resolve(rootDir, "scripts/params/clock_code/deadline_fidelity_found.json");
  const fallback = path.resolve(rootDir, "scripts/params/clock_code/code_deadline_gated_clock.json");
  if (fs.existsSync(preferred)) return preferred;
  return fallback;
}

ensureDir(outDir);
await loadWasm();

const presetPath = pickPreset();
const presetRaw = readJson(presetPath);
const baseParams = presetRaw.params ?? presetRaw;
const baseDeadline = presetRaw.deadline ?? 25_000;

const paramsPath = path.join(outDir, "deadline_rate_matched_v4_best_params.json");
if (!fs.existsSync(paramsPath)) {
  throw new Error("Missing deadline_rate_matched_v4_best_params.json; run throughput-matched v4 first.");
}
const bestParams = readJson(paramsPath);

const seeds = Array.from({ length: 10 }, (_, i) => i + 1);
const steps = 500_000;
const reportEvery = 5_000;
const eventEvery = 50_000;
const regionType = "quadrant";
const regionIndex = 2;
const errGood = 0.1;
const sdiffGood = 1.0;
const tailWindow = 200_000;

const budgets = [5, 10, 15, 20, 30, 40, 60, 80];

const modes = [
  { id: "A_legacy", params: { ...baseParams, opCouplingOn: 0, sCouplingMode: 0, opDriveOnK: 0 } },
  { id: "B_dilution_only", params: { ...baseParams, opCouplingOn: 1, sCouplingMode: 0, opDriveOnK: 0 } },
  { id: "C_op_noKdrive", params: { ...baseParams, opCouplingOn: 1, sCouplingMode: 1, opDriveOnK: 0 } },
  { id: "D_op_withKdrive", params: { ...baseParams, opCouplingOn: 1, sCouplingMode: 1, opDriveOnK: 1 } },
];

const rawRows = [];
const pointsRows = [];
const summaryRows = [];
const repairsByMode = new Map();
const eventsByMode = new Map();

for (const mode of modes) {
  const modeParams = bestParams[mode.id] ?? {};
  const params = {
    ...mode.params,
    opKTargetWeight: modeParams.opKTargetWeightMean ?? 1.0,
    pSWrite: modeParams.pSWriteMean ?? baseParams.pSWrite ?? 0.1,
    opStencil: modeParams.opStencil ?? mode.params.opStencil,
    opBudgetK: modeParams.opBudgetK ?? mode.params.opBudgetK,
    opDriveOnK: modeParams.opDriveOnK ?? mode.params.opDriveOnK,
  };

  const result = await runDeadlineEvents({
    presetPath,
    presetParams: params,
    variant: "drift",
    seeds,
    steps,
    reportEvery,
    eventEvery,
    deadline: baseDeadline,
    regionType,
    regionIndex,
    gateSpan: null,
    corruptFrac: 0.2,
    errGood,
    sdiffGood,
    tailWindow,
    includeEvents: true,
  });

  const repairs = [];
  for (const run of result.runs) {
    rawRows.push(JSON.stringify({ mode: mode.id, seed: run.seed, ...run }));
    if (!run.eventOutcomes) continue;
    for (const evt of run.eventOutcomes) {
      const val = evt.repairsUsed;
      repairs.push(val);
    }
  }
  repairsByMode.set(mode.id, repairs);
  eventsByMode.set(mode.id, result.runs.reduce((acc, r) => acc + (r.events ?? 0), 0));

  const successRepairs = repairs.filter((v) => Number.isFinite(v));
  const meanSuccess = successRepairs.length ? mean(successRepairs) : null;
  const sortedSuccess = successRepairs.slice().sort((a, b) => a - b);
  const medianSuccess = sortedSuccess.length
    ? sortedSuccess[Math.floor(sortedSuccess.length / 2)]
    : null;

  summaryRows.push({
    mode: mode.id,
    meanRepairsSuccess: meanSuccess,
    medianRepairsSuccess: medianSuccess,
    totalEvents: repairs.length,
  });
}

function bootstrapCI(values, budget, reps) {
  const n = values.length;
  if (n === 0) return { lo: null, hi: null };
  const draws = [];
  const rng = new (class {
    constructor(seed) {
      this.state = seed >>> 0;
    }
    next() {
      this.state = (1664525 * this.state + 1013904223) >>> 0;
      return this.state / 0xffffffff;
    }
    int(max) {
      return Math.floor(this.next() * max);
    }
  })(budget * 997 + n);
  for (let i = 0; i < reps; i += 1) {
    let succ = 0;
    for (let j = 0; j < n; j += 1) {
      if (values[rng.int(n)] <= budget) succ += 1;
    }
    draws.push(succ / n);
  }
  draws.sort((a, b) => a - b);
  const lo = draws[Math.floor(0.025 * reps)];
  const hi = draws[Math.floor(0.975 * reps)];
  return { lo, hi };
}

const pointsPath = path.join(outDir, "repair_budget_curves_points.csv");
const pointsLines = [
  "mode,budget,pSucc,pSuccCiLow,pSuccCiHigh,events",
];
for (const mode of modes) {
  const repairs = repairsByMode.get(mode.id) ?? [];
  const total = repairs.length;
  for (const budget of budgets) {
    const succ = repairs.filter((v) => v <= budget).length;
    const psucc = total > 0 ? succ / total : 0;
    const ci = bootstrapCI(repairs, budget, 2000);
    pointsLines.push(
      [
        mode.id,
        budget,
        psucc,
        ci.lo ?? "",
        ci.hi ?? "",
        total,
      ].join(","),
    );
    pointsRows.push({
      mode: mode.id,
      budget,
      pSucc: psucc,
      pSuccCiLow: ci.lo,
      pSuccCiHigh: ci.hi,
      events: total,
    });
  }
}
fs.writeFileSync(pointsPath, `${pointsLines.join("\n")}\n`);

const summaryPath = path.join(outDir, "repair_budget_curves_summary.csv");
const summaryLines = [
  "mode,meanRepairsSuccess,medianRepairsSuccess,totalEvents",
  ...summaryRows.map((row) =>
    [row.mode, row.meanRepairsSuccess ?? "", row.medianRepairsSuccess ?? "", row.totalEvents].join(","),
  ),
].join("\n");
fs.writeFileSync(summaryPath, `${summaryLines}\n`);

fs.writeFileSync(path.join(outDir, "repair_budget_curves_raw.jsonl"), rawRows.join("\n"));

const modeToBudgets = new Map();
for (const row of pointsRows) {
  const key = row.mode;
  if (!modeToBudgets.has(key)) modeToBudgets.set(key, new Map());
  modeToBudgets.get(key).set(row.budget, row.pSucc);
}

const checkDominance = (a, b) => {
  for (const budget of budgets) {
    const pa = modeToBudgets.get(a)?.get(budget);
    const pb = modeToBudgets.get(b)?.get(budget);
    if (pa == null || pb == null) continue;
    if (pa < pb) return { ok: false, budget };
  }
  return { ok: true, budget: null };
};

const domCB = checkDominance("C_op_noKdrive", "B_dilution_only");
const domCA = checkDominance("C_op_noKdrive", "A_legacy");
console.log(
  `DOMINANCE_CHECK C>=B: ${domCB.ok}${domCB.ok ? "" : ` firstFailBudget=${domCB.budget}`}`,
);
console.log(
  `DOMINANCE_CHECK C>=A: ${domCA.ok}${domCA.ok ? "" : ` firstFailBudget=${domCA.budget}`}`,
);
</file>

<file path="scripts/run-deadline-opk-throughput-matched-v4.mjs">
#!/usr/bin/env node
import fs from "node:fs";
import path from "node:path";
import {
  loadWasm,
  mean,
  std,
  readJson,
  runDeadlineEvents,
} from "./deadline-event-utils.mjs";

const rootDir = path.resolve(path.dirname(new URL(import.meta.url).pathname), "..");
const outDir = path.resolve(rootDir, ".tmp", "op_coupling");

function ensureDir(dir) {
  fs.mkdirSync(dir, { recursive: true });
}

function pickPreset() {
  const preferred = path.resolve(rootDir, "scripts/params/clock_code/deadline_fidelity_found.json");
  const fallback = path.resolve(rootDir, "scripts/params/clock_code/code_deadline_gated_clock.json");
  if (fs.existsSync(preferred)) return preferred;
  return fallback;
}

function summarizeRuns(id, runs, meta) {
  const missFrac = runs.map((r) => r.missFrac);
  const uptimeTail = runs.map((r) => r.uptimeTail);
  const errTail = runs.map((r) => r.errTailMean);
  const sdiffTail = runs.map((r) => r.sdiffTailMean);
  const epTotal = runs.map((r) => r.epTotalRate);
  const epRepair = runs.map((r) => r.epRepairRate);
  const epOpK = runs.map((r) => r.epOpKRate ?? 0);
  const epClock = runs.map((r) => r.epClockRate ?? 0);
  const repairRate = runs.map((r) => r.repairRate ?? 0);
  const opkRate = runs.map((r) => r.opkRate ?? 0);
  const p5MetaSuccess = runs.map((r) => r.p5MetaToRecoverSuccessMean ?? 0);
  const repairEfficiency = runs.map((r) => r.repairEfficiencySuccessMean ?? 0);
  const weight = runs.map((r) => r.opKTargetWeight ?? 0);
  const pSWrite = runs.map((r) => r.pSWrite ?? 0);
  const okCount = runs.filter((r) => r.calibrationOk).length;

  return {
    id,
    ...meta,
    missFracMean: mean(missFrac),
    missFracStd: std(missFrac),
    uptimeTailMean: mean(uptimeTail),
    uptimeTailStd: std(uptimeTail),
    errTailMean: mean(errTail),
    errTailStd: std(errTail),
    sdiffTailMean: mean(sdiffTail),
    sdiffTailStd: std(sdiffTail),
    epTotalRateMean: mean(epTotal),
    epRepairRateMean: mean(epRepair),
    epOpKRateMean: mean(epOpK),
    epClockRateMean: mean(epClock),
    repairRateMean: mean(repairRate),
    opkRateMean: mean(opkRate),
    p5MetaToRecoverSuccessMean: mean(p5MetaSuccess),
    repairEfficiencySuccessMean: mean(repairEfficiency),
    opKTargetWeightMean: mean(weight),
    opKTargetWeightStd: std(weight),
    pSWriteMean: mean(pSWrite),
    pSWriteStd: std(pSWrite),
    calibrationOkCount: okCount,
  };
}

function applyVariant(params, variant) {
  const next = { ...params };
  if (variant === "drift") {
    next.clockOn = 1;
    next.clockUsesP6 = 1;
  } else if (variant === "random") {
    next.clockOn = 1;
    next.clockUsesP6 = 0;
  } else {
    next.clockOn = 0;
    next.clockUsesP6 = 1;
  }
  return next;
}

ensureDir(outDir);
const mod = await loadWasm();

const presetPath = pickPreset();
const presetRaw = readJson(presetPath);
const baseParams = presetRaw.params ?? presetRaw;
const baseDeadline = presetRaw.deadline ?? 25_000;
const basePSWrite = baseParams.pSWrite ?? 1.0;

const seeds = Array.from({ length: 10 }, (_, i) => i + 1);
const steps = 500_000;
const reportEvery = 5_000;
const eventEvery = 50_000;
const regionType = "quadrant";
const regionIndex = 2;
const errGood = 0.1;
const sdiffGood = 1.0;
const tailWindow = 200_000;

const MOVE_P5_META = 8;
const calibSteps = 200_000;
const tol = 0.05;

function measureRepairRate(params, seed, variant) {
  const sim = new mod.Sim(50, seed);
  sim.set_params({ ...applyVariant(params, variant), epDebug: 1 });
  sim.step(calibSteps);
  const counts = sim.ep_q_stats().count;
  return (counts[MOVE_P5_META] ?? 0) / calibSteps;
}

function calibrateWeight(params, targetRate, seed) {
  let lo = 0.0;
  let hi = 1.0;
  let best = null;
  for (let i = 0; i < 10; i += 1) {
    const mid = 0.5 * (lo + hi);
    const rate = measureRepairRate({ ...params, opKTargetWeight: mid }, seed, "drift");
    const diff = Math.abs(rate - targetRate);
    if (!best || diff < best.diff) {
      best = { weight: mid, rate, diff };
    }
    if (targetRate > 0 && diff / targetRate <= tol) {
      return { ...best, ok: true };
    }
    if (rate < targetRate) {
      hi = mid;
    } else {
      lo = mid;
    }
  }
  const ok = targetRate > 0 && best && best.diff / targetRate <= tol;
  return { ...best, ok };
}

function calibratePSWrite(params, targetRate, seed) {
  let lo = params.pSWrite ?? 0.1;
  let hi = 1.0;
  let best = null;
  for (let i = 0; i < 10; i += 1) {
    const mid = 0.5 * (lo + hi);
    const rate = measureRepairRate({ ...params, pSWrite: mid, opKTargetWeight: 0.0 }, seed, "drift");
    const diff = Math.abs(rate - targetRate);
    if (!best || diff < best.diff) {
      best = { pSWrite: mid, rate, diff };
    }
    if (targetRate > 0 && diff / targetRate <= tol) {
      return { ...best, ok: true };
    }
    if (rate < targetRate) {
      lo = mid;
    } else {
      hi = mid;
    }
  }
  const ok = targetRate > 0 && best && best.diff / targetRate <= tol;
  return { ...best, ok };
}

const baseLegacy = { ...baseParams, opCouplingOn: 0, sCouplingMode: 0, opDriveOnK: 0, pSWrite: basePSWrite };

const modes = [
  { id: "A_legacy", params: baseLegacy },
  { id: "B_dilution_only", params: { ...baseParams, opCouplingOn: 1, sCouplingMode: 0, opDriveOnK: 0, pSWrite: basePSWrite } },
  { id: "C_op_noKdrive", params: { ...baseParams, opCouplingOn: 1, sCouplingMode: 1, opDriveOnK: 0, pSWrite: basePSWrite } },
  { id: "D_op_withKdrive", params: { ...baseParams, opCouplingOn: 1, sCouplingMode: 1, opDriveOnK: 1, pSWrite: basePSWrite } },
];

const baselineRates = {};
for (const seed of seeds) {
  baselineRates[seed] = measureRepairRate(baseLegacy, seed, "drift");
}

const rawRows = [];
const summaryRows = [];
const paramsSummary = {};

for (const mode of modes) {
  const runs = [];
  for (const seed of seeds) {
    const targetRate = baselineRates[seed];
    let weight = 1.0;
    let pSWrite = basePSWrite;
    let ok = true;
    if (mode.id !== "A_legacy") {
      const calib = calibrateWeight(mode.params, targetRate, seed);
      weight = calib.weight;
      ok = calib.ok;
      if (!ok && weight <= 1e-6) {
        if (basePSWrite < 1.0) {
          const pCalib = calibratePSWrite(mode.params, targetRate, seed);
          pSWrite = pCalib.pSWrite;
          ok = pCalib.ok;
        } else {
          ok = false;
        }
      }
    }
    const params = { ...mode.params, opKTargetWeight: weight, pSWrite };
    const result = await runDeadlineEvents({
      presetPath,
      presetParams: params,
      variant: "drift",
      seeds: [seed],
      steps,
      reportEvery,
      eventEvery,
      deadline: baseDeadline,
      regionType,
      regionIndex,
      gateSpan: null,
      corruptFrac: 0.2,
      errGood,
      sdiffGood,
      tailWindow,
    });
    const run = result.runs[0];
    const row = {
      mode: mode.id,
      seed,
      ...run,
      opKTargetWeight: weight,
      pSWrite,
      targetRepairRate: targetRate,
      achievedRepairRate: run.repairRate ?? 0,
      calibrationOk: ok,
    };
    rawRows.push(JSON.stringify(row));
    runs.push(row);
  }
  const summary = summarizeRuns(mode.id, runs, {
    opStencil: mode.params.opStencil ?? "",
    opBudgetK: mode.params.opBudgetK ?? "",
    opDriveOnK: mode.params.opDriveOnK ?? "",
  });
  summaryRows.push(summary);
  paramsSummary[mode.id] = {
    opKTargetWeightMean: summary.opKTargetWeightMean,
    pSWriteMean: summary.pSWriteMean,
    opStencil: mode.params.opStencil ?? null,
    opBudgetK: mode.params.opBudgetK ?? null,
    opDriveOnK: mode.params.opDriveOnK ?? null,
  };
}

const summaryPath = path.join(outDir, "deadline_rate_matched_v4_summary.csv");
const header = [
  "id",
  "opStencil",
  "opBudgetK",
  "opDriveOnK",
  "missFracMean",
  "missFracStd",
  "uptimeTailMean",
  "uptimeTailStd",
  "errTailMean",
  "errTailStd",
  "sdiffTailMean",
  "sdiffTailStd",
  "epTotalRateMean",
  "epRepairRateMean",
  "epOpKRateMean",
  "epClockRateMean",
  "repairRateMean",
  "opkRateMean",
  "p5MetaToRecoverSuccessMean",
  "repairEfficiencySuccessMean",
  "opKTargetWeightMean",
  "opKTargetWeightStd",
  "pSWriteMean",
  "pSWriteStd",
  "calibrationOkCount",
];
const lines = [
  header.join(","),
  ...summaryRows.map((row) =>
    [
      row.id,
      row.opStencil ?? "",
      row.opBudgetK ?? "",
      row.opDriveOnK ?? "",
      row.missFracMean,
      row.missFracStd,
      row.uptimeTailMean,
      row.uptimeTailStd,
      row.errTailMean,
      row.errTailStd,
      row.sdiffTailMean,
      row.sdiffTailStd,
      row.epTotalRateMean,
      row.epRepairRateMean,
      row.epOpKRateMean,
      row.epClockRateMean,
      row.repairRateMean,
      row.opkRateMean,
      row.p5MetaToRecoverSuccessMean,
      row.repairEfficiencySuccessMean,
      row.opKTargetWeightMean,
      row.opKTargetWeightStd,
      row.pSWriteMean,
      row.pSWriteStd,
      row.calibrationOkCount,
    ].join(","),
  ),
].join("\n");

fs.writeFileSync(summaryPath, `${lines}\n`);
fs.writeFileSync(path.join(outDir, "deadline_rate_matched_v4_raw.jsonl"), rawRows.join("\n"));
fs.writeFileSync(
  path.join(outDir, "deadline_rate_matched_v4_best_params.json"),
  JSON.stringify(paramsSummary, null, 2),
);

const targetMean = mean(Object.values(baselineRates));
const okMean = summaryRows.every((row) => {
  const diff = Math.abs(row.repairRateMean - targetMean);
  return targetMean === 0 ? diff === 0 : diff / targetMean <= tol;
});
const okCounts = summaryRows.map((row) => `${row.id}:${row.calibrationOkCount}/${seeds.length}`);

console.log(`RATE_MATCH_OK=${okMean}`);
console.log(`repairRateTargetMean=${targetMean.toFixed(6)}`);
console.log(`calibrationOkCounts=${okCounts.join(" ")}`);
for (const row of summaryRows) {
  console.log(
    `${row.id} repairRateMean=${row.repairRateMean.toFixed(6)} opKTargetWeightMean=${row.opKTargetWeightMean.toFixed(3)} pSWriteMean=${row.pSWriteMean.toFixed(3)}`,
  );
}
</file>

<file path="scripts/run-deadline-opk-throughput-matched.mjs">
#!/usr/bin/env node
import fs from "node:fs";
import path from "node:path";
import {
  loadWasm,
  mean,
  std,
  readJson,
  runDeadlineEvents,
} from "./deadline-event-utils.mjs";

const rootDir = path.resolve(path.dirname(new URL(import.meta.url).pathname), "..");
const outDir = path.resolve(rootDir, ".tmp", "op_coupling");

function ensureDir(dir) {
  fs.mkdirSync(dir, { recursive: true });
}

function pickPreset() {
  const preferred = path.resolve(rootDir, "scripts/params/clock_code/deadline_fidelity_found.json");
  const fallback = path.resolve(rootDir, "scripts/params/clock_code/code_deadline_gated_clock.json");
  if (fs.existsSync(preferred)) return preferred;
  return fallback;
}

function readBestConfigs() {
  const summaryPath = path.resolve(outDir, "deadline_decomp_summary.csv");
  const defaults = {
    bestC: { opStencil: 1, opBudgetK: 32 },
    bestD: { opStencil: 1, opBudgetK: 16 },
  };
  if (!fs.existsSync(summaryPath)) return defaults;
  const lines = fs.readFileSync(summaryPath, "utf8").trim().split("\n");
  const header = lines.shift()?.split(",") ?? [];
  const idx = Object.fromEntries(header.map((k, i) => [k, i]));
  for (const line of lines) {
    const cols = line.split(",");
    const note = cols[idx.note] ?? "";
    if (note === "BEST_C") {
      defaults.bestC = {
        opStencil: Number(cols[idx.opStencil]),
        opBudgetK: Number(cols[idx.opBudgetK]),
      };
    }
    if (note === "BEST_D") {
      defaults.bestD = {
        opStencil: Number(cols[idx.opStencil]),
        opBudgetK: Number(cols[idx.opBudgetK]),
      };
    }
  }
  return defaults;
}

function summarizeRuns(id, runs, meta) {
  const missFrac = runs.map((r) => r.missFrac);
  const uptimeTail = runs.map((r) => r.uptimeTail);
  const errTail = runs.map((r) => r.errTailMean);
  const sdiffTail = runs.map((r) => r.sdiffTailMean);
  const epTotal = runs.map((r) => r.epTotalRate);
  const epRepair = runs.map((r) => r.epRepairRate);
  const epOpK = runs.map((r) => r.epOpKRate ?? 0);
  const repairRate = runs.map((r) => r.repairRate ?? 0);
  const p5MetaSuccess = runs.map((r) => r.p5MetaToRecoverSuccessMean ?? 0);
  const repairEfficiency = runs.map((r) => r.repairEfficiencySuccessMean ?? 0);
  const pSWrite = runs.map((r) => r.pSWrite ?? 0);
  const targetRate = runs.map((r) => r.targetRepairRate ?? 0);
  const achievedRate = runs.map((r) => r.repairRate ?? 0);
  const okCount = runs.filter((r) => r.calibrationOk).length;

  return {
    id,
    ...meta,
    missFracMean: mean(missFrac),
    missFracStd: std(missFrac),
    uptimeTailMean: mean(uptimeTail),
    uptimeTailStd: std(uptimeTail),
    errTailMean: mean(errTail),
    errTailStd: std(errTail),
    sdiffTailMean: mean(sdiffTail),
    sdiffTailStd: std(sdiffTail),
    epTotalRateMean: mean(epTotal),
    epRepairRateMean: mean(epRepair),
    epOpKRateMean: mean(epOpK),
    repairRateMean: mean(repairRate),
    p5MetaToRecoverSuccessMean: mean(p5MetaSuccess),
    repairEfficiencySuccessMean: mean(repairEfficiency),
    pSWriteMean: mean(pSWrite),
    pSWriteStd: std(pSWrite),
    targetRepairRateMean: mean(targetRate),
    achievedRepairRateMean: mean(achievedRate),
    calibrationOkCount: okCount,
  };
}

ensureDir(outDir);
const mod = await loadWasm();

const presetPath = pickPreset();
const presetRaw = readJson(presetPath);
const baseParams = presetRaw.params ?? presetRaw;
const baseDeadline = presetRaw.deadline ?? 25_000;

const best = readBestConfigs();

const seeds = Array.from({ length: 10 }, (_, i) => i + 1);
const steps = 500_000;
const reportEvery = 5_000;
const eventEvery = 50_000;
const regionType = "quadrant";
const regionIndex = 2;
const errGood = 0.1;
const sdiffGood = 1.0;
const tailWindow = 200_000;

const MOVE_P5_META = 8;
const calibSteps = 200_000;
const calibIterMax = 8;
const tol = 0.05;

function applyVariant(params, variant) {
  const next = { ...params };
  if (variant === "drift") {
    next.clockOn = 1;
    next.clockUsesP6 = 1;
  } else if (variant === "random") {
    next.clockOn = 1;
    next.clockUsesP6 = 0;
  } else {
    next.clockOn = 0;
    next.clockUsesP6 = 1;
  }
  return next;
}

function measureRepairRate(params, seed, variant) {
  const sim = new mod.Sim(50, seed);
  sim.set_params({ ...applyVariant(params, variant), epDebug: 1 });
  sim.step(calibSteps);
  const counts = sim.ep_q_stats().count;
  return (counts[MOVE_P5_META] ?? 0) / calibSteps;
}

function calibratePSWrite(params, targetRate, seed, variant) {
  let lo = 0.1;
  let hi = 1.0;
  let best = null;
  for (let i = 0; i < calibIterMax; i += 1) {
    const mid = 0.5 * (lo + hi);
    const rate = measureRepairRate({ ...params, pSWrite: mid }, seed, variant);
    const diff = Math.abs(rate - targetRate);
    if (!best || diff < best.diff) {
      best = { pSWrite: mid, rate, diff };
    }
    if (targetRate === 0) break;
    if (diff / targetRate <= tol) {
      return { ...best, ok: true };
    }
    if (rate < targetRate) {
      lo = mid;
    } else {
      hi = mid;
    }
  }
  const ok = targetRate > 0 && best && best.diff / targetRate <= tol;
  return { ...best, ok };
}

const baseLegacy = { ...baseParams, opCouplingOn: 0, sCouplingMode: 0, opDriveOnK: 0 };

const modes = [
  { id: "A_legacy", params: baseLegacy },
  { id: "B_dilution_only", params: { ...baseParams, opCouplingOn: 1, sCouplingMode: 0, opDriveOnK: 0 } },
  {
    id: "C_candidate",
    params: {
      ...baseParams,
      opCouplingOn: 1,
      sCouplingMode: 1,
      opDriveOnK: 0,
      opStencil: best.bestC.opStencil,
      opBudgetK: best.bestC.opBudgetK,
    },
  },
  {
    id: "D_candidate",
    params: {
      ...baseParams,
      opCouplingOn: 1,
      sCouplingMode: 1,
      opDriveOnK: 1,
      opStencil: best.bestD.opStencil,
      opBudgetK: best.bestD.opBudgetK,
    },
  },
];

const rawRows = [];
const summaryRows = [];

const baselineRates = {};
for (const seed of seeds) {
  const baseRate = measureRepairRate(baseLegacy, seed, "drift");
  baselineRates[seed] = baseRate;
}

for (const mode of modes) {
  const runs = [];
  for (const seed of seeds) {
    const targetRate = baselineRates[seed];
    let calibration = { pSWrite: baseParams.pSWrite ?? 0.1, rate: targetRate, ok: true };
    if (mode.id !== "A_legacy") {
      calibration = calibratePSWrite(mode.params, targetRate, seed, "drift");
    }
    const params = { ...mode.params, pSWrite: calibration.pSWrite };
    const result = await runDeadlineEvents({
      presetPath,
      presetParams: params,
      variant: "drift",
      seeds: [seed],
      steps,
      reportEvery,
      eventEvery,
      deadline: baseDeadline,
      regionType,
      regionIndex,
      gateSpan: null,
      corruptFrac: 0.2,
      errGood,
      sdiffGood,
      tailWindow,
    });
    const run = result.runs[0];
    const row = {
      mode: mode.id,
      seed,
      ...run,
      pSWrite: calibration.pSWrite,
      targetRepairRate: targetRate,
      achievedRepairRate: run.repairRate ?? 0,
      calibrationOk: calibration.ok,
      calibrationRate: calibration.rate,
    };
    rawRows.push(JSON.stringify(row));
    runs.push(row);
  }

  summaryRows.push(
    summarizeRuns(mode.id, runs, {
      opStencil: mode.params.opStencil ?? "",
      opBudgetK: mode.params.opBudgetK ?? "",
      opDriveOnK: mode.params.opDriveOnK ?? "",
    }),
  );
}

const summaryPath = path.join(outDir, "deadline_rate_matched_summary.csv");
const header = [
  "id",
  "opStencil",
  "opBudgetK",
  "opDriveOnK",
  "missFracMean",
  "missFracStd",
  "uptimeTailMean",
  "uptimeTailStd",
  "errTailMean",
  "errTailStd",
  "sdiffTailMean",
  "sdiffTailStd",
  "epTotalRateMean",
  "epRepairRateMean",
  "epOpKRateMean",
  "repairRateMean",
  "p5MetaToRecoverSuccessMean",
  "repairEfficiencySuccessMean",
  "pSWriteMean",
  "pSWriteStd",
  "targetRepairRateMean",
  "achievedRepairRateMean",
  "calibrationOkCount",
];

const lines = [
  header.join(","),
  ...summaryRows.map((row) =>
    [
      row.id,
      row.opStencil ?? "",
      row.opBudgetK ?? "",
      row.opDriveOnK ?? "",
      row.missFracMean,
      row.missFracStd,
      row.uptimeTailMean,
      row.uptimeTailStd,
      row.errTailMean,
      row.errTailStd,
      row.sdiffTailMean,
      row.sdiffTailStd,
      row.epTotalRateMean,
      row.epRepairRateMean,
      row.epOpKRateMean,
      row.repairRateMean,
      row.p5MetaToRecoverSuccessMean,
      row.repairEfficiencySuccessMean,
      row.pSWriteMean,
      row.pSWriteStd,
      row.targetRepairRateMean,
      row.achievedRepairRateMean,
      row.calibrationOkCount,
    ].join(","),
  ),
].join("\n");

fs.writeFileSync(summaryPath, `${lines}\n`);
fs.writeFileSync(path.join(outDir, "deadline_rate_matched_raw.jsonl"), rawRows.join("\n"));

const baselineMean = mean(Object.values(baselineRates));
const rateOk = summaryRows.every((row) => {
  const diff = Math.abs(row.repairRateMean - baselineMean);
  return baselineMean === 0 ? diff === 0 : diff / baselineMean <= tol;
});
const okCounts = summaryRows.map((row) => `${row.id}:${row.calibrationOkCount}/${seeds.length}`);

console.log(`RATE_MATCH_OK=${rateOk}`);
console.log(`baselineRepairRateMean=${baselineMean.toFixed(6)}`);
console.log(`calibrationOkCounts=${okCounts.join(" ")}`);
for (const row of summaryRows) {
  console.log(`${row.id} repairRateMean=${row.repairRateMean.toFixed(6)} pSWriteMean=${row.pSWriteMean.toFixed(3)}`);
}
</file>

<file path="scripts/run-deadline-phase-diagram.mjs">
#!/usr/bin/env node
import fs from "node:fs";
import path from "node:path";
import { calibrateGateGaps, loadWasm, mean, runDeadlineEvents } from "./deadline-event-utils.mjs";

const rootDir = path.resolve(path.dirname(new URL(import.meta.url).pathname), "..");
const outDir = path.resolve(rootDir, ".tmp", "clock_code");

function ensureDir(dir) {
  fs.mkdirSync(dir, { recursive: true });
}

ensureDir(outDir);
await loadWasm();

const gridSizes = [48];
const gateSpans = [2, 3];
const noiseRates = [0.005, 0.008, 0.01, 0.012, 0.02];
const mus = [1.8, 2.0, 2.2];
const etaDrives = [0.4, 0.45, 0.5, 0.55, 0.6, 0.65, 0.7, 0.75, 0.8, 0.85, 0.9, 0.95, 1.0];
const extraGridSizes = [64];
const extraNoiseRates = [0.008, 0.01, 0.012];
const extraEtaDrives = [0.4, 0.45, 0.5, 0.55, 0.6, 0.65, 0.7, 0.75, 0.8, 0.85, 0.9, 0.95, 1.0];

const steps = 1_500_000;
const reportEvery = 1_000;
const eventEvery = 50_000;
const seeds = [1, 2, 3];
const corruptFrac = 0.1;
const tailWindow = 200_000;
const deadlineScale = 1.2;

const rows = [];
let strongCount = 0;

async function evaluateGrid({
  gridSizeList,
  gateSpanList,
  noiseRateList,
  muList,
  etaDriveList,
}) {
  for (const gridSize of gridSizeList) {
    for (const gateSpan of gateSpanList) {
      for (const codeNoiseRate of noiseRateList) {
        for (const mu of muList) {
          for (const etaDrive of etaDriveList) {
          const clockK = gridSize;
          const regionIndex = Math.floor(clockK / 2);
          const baseParams = {
            beta: 2.0,
            stepSize: 0.01,
            p3On: 0,
            p6On: 1,
            p6SFactor: 0.0,
            pWrite: 0,
            pNWrite: 0.05,
            pAWrite: 0,
            pSWrite: 1.0,
            muHigh: mu,
            muLow: mu,
            kappaRep: 1.0,
            r0: 0.25,
            kappaBond: 0.0,
            rStar: 0.22,
            lambdaW: 0.0,
            lW: 4,
            lambdaN: 0.0,
            lN: 6,
            lambdaA: 0.0,
            lA: 6,
            lambdaS: 0.0,
            lS: 1,
            gridSize,
            rPropose: 0.12,
            metaLayers: 2,
            eta: 0.0,
            etaDrive,
            codeNoiseRate,
            codeNoiseBatch: 1,
            codeNoiseLayer: 0,
            clockOn: 1,
            clockK,
            clockFrac: 1.0,
            clockUsesP6: 1,
            repairClockGated: 1,
            repairGateMode: 1,
            repairGateSpan: gateSpan,
          };

          const gapDrift = await calibrateGateGaps({
            presetParams: baseParams,
            variant: "drift",
            steps: 300_000,
            reportEvery,
            regionType: "stripe",
            regionIndex,
            gateSpan,
          });
          const gapRandom = await calibrateGateGaps({
            presetParams: baseParams,
            variant: "random",
            steps: 300_000,
            reportEvery,
            regionType: "stripe",
            regionIndex,
            gateSpan,
          });

          const deadline = Math.ceil(deadlineScale * (gapDrift.gapP95 ?? reportEvery));

          const drift = await runDeadlineEvents({
            presetParams: baseParams,
            variant: "drift",
            seeds,
            steps,
            reportEvery,
            eventEvery,
            deadline,
            regionType: "stripe",
            regionIndex,
            gateSpan,
            corruptFrac,
            errGood: 0.1,
            sdiffGood: 1.0,
            tailWindow,
          });
          const random = await runDeadlineEvents({
            presetParams: baseParams,
            variant: "random",
            seeds,
            steps,
            reportEvery,
            eventEvery,
            deadline,
            regionType: "stripe",
            regionIndex,
            gateSpan,
            corruptFrac,
            errGood: 0.1,
            sdiffGood: 1.0,
            tailWindow,
          });

          const driftMiss = mean(drift.runs.map((r) => r.missFrac));
          const randomMiss = mean(random.runs.map((r) => r.missFrac));
          const driftP95 = mean(drift.runs.map((r) => r.recoveryP95 ?? 0));
          const randomP95 = mean(random.runs.map((r) => r.recoveryP95 ?? 0));
          const driftUptimeTail = mean(drift.runs.map((r) => r.uptimeTail));
          const randomUptimeTail = mean(random.runs.map((r) => r.uptimeTail));
          const driftErrTail = mean(drift.runs.map((r) => r.errTailMean));
          const randomErrTail = mean(random.runs.map((r) => r.errTailMean));
          const driftEp = mean(drift.runs.map((r) => r.epTotalRate));
          const randomEp = mean(random.runs.map((r) => r.epTotalRate));
          const driftClock = mean(drift.runs.map((r) => r.epClockRate));
          const randomClock = mean(random.runs.map((r) => r.epClockRate));
          const driftRepair = mean(drift.runs.map((r) => r.epRepairRate));
          const randomRepair = mean(random.runs.map((r) => r.epRepairRate));
          const driftOther = mean(drift.runs.map((r) => r.epOtherRate));
          const randomOther = mean(random.runs.map((r) => r.epOtherRate));

          const sepMiss = randomMiss - driftMiss;
          const sepUptimeTail = driftUptimeTail - randomUptimeTail;
          const sepErrTail = randomErrTail - driftErrTail;
          const sepScore = sepMiss + 0.7 * sepUptimeTail + 0.7 * sepErrTail;
          if (driftMiss <= 0.2 && driftUptimeTail >= 0.7 && sepScore >= 0.5) {
            strongCount += 1;
          }

          rows.push({
            gridSize,
            gateSpan,
            codeNoiseRate,
            deadline,
            mu,
            etaDrive,
            gapP95Drift: gapDrift.gapP95 ?? 0,
            gapP95Random: gapRandom.gapP95 ?? 0,
            driftMiss,
            randomMiss,
            driftP95,
            randomP95,
            driftUptimeTail,
            randomUptimeTail,
            driftErrTail,
            randomErrTail,
            driftEp,
            randomEp,
            driftClock,
            randomClock,
            driftRepair,
            randomRepair,
            driftOther,
            randomOther,
            sepMiss,
            sepUptimeTail,
            sepErrTail,
            sepScore,
          });
          }
        }
      }
    }
  }
}

await evaluateGrid({
  gridSizeList: gridSizes,
  gateSpanList: gateSpans,
  noiseRateList: noiseRates,
  muList: mus,
  etaDriveList: etaDrives,
});

if (strongCount < 10) {
  console.log(
    `Strong separation count ${strongCount} < 10; expanding grid to size 64 with reduced sweep.`,
  );
  await evaluateGrid({
    gridSizeList: extraGridSizes,
    gateSpanList: gateSpans,
    noiseRateList: extraNoiseRates,
    muList: mus,
    etaDriveList: extraEtaDrives,
  });
}

const rawPath = path.join(outDir, "deadline_phase_v3_raw.jsonl");
const csvPath = path.join(outDir, "deadline_phase_v3.csv");
fs.writeFileSync(rawPath, rows.map((r) => JSON.stringify(r)).join("\n"));

const header = [
  "gridSize",
  "gateSpan",
  "codeNoiseRate",
  "deadline",
  "mu",
  "etaDrive",
  "gapP95Drift",
  "gapP95Random",
  "driftMiss",
  "randomMiss",
  "driftP95",
  "randomP95",
  "driftUptimeTail",
  "randomUptimeTail",
  "driftErrTail",
  "randomErrTail",
  "driftEpRate",
  "randomEpRate",
  "driftClockRate",
  "randomClockRate",
  "driftRepairRate",
  "randomRepairRate",
  "driftOtherRate",
  "randomOtherRate",
  "sepMiss",
  "sepUptimeTail",
  "sepErrTail",
  "sepScore",
];
const csvLines = [
  header.join(","),
  ...rows.map((r) =>
    [
      r.gridSize,
      r.gateSpan,
      r.codeNoiseRate,
      r.deadline,
      r.mu,
      r.etaDrive,
      r.gapP95Drift,
      r.gapP95Random,
      r.driftMiss,
      r.randomMiss,
      r.driftP95,
      r.randomP95,
      r.driftUptimeTail,
      r.randomUptimeTail,
      r.driftErrTail,
      r.randomErrTail,
      r.driftEp,
      r.randomEp,
      r.driftClock,
      r.randomClock,
      r.driftRepair,
      r.randomRepair,
      r.driftOther,
      r.randomOther,
      r.sepMiss,
      r.sepUptimeTail,
      r.sepErrTail,
      r.sepScore,
    ].join(","),
  ),
];
fs.writeFileSync(csvPath, csvLines.join("\n"));

console.log(`Phase diagram complete: ${rows.length} points, strong sep ${strongCount}`);
</file>

<file path="scripts/run-deadline-success-ci.mjs">
#!/usr/bin/env node
import fs from "node:fs";
import path from "node:path";
import {
  loadWasm,
  mean,
  percentile,
  parseSeedList,
  readJson,
  runDeadlineEvents,
} from "./deadline-event-utils.mjs";

const rootDir = path.resolve(path.dirname(new URL(import.meta.url).pathname), "..");
const outDir = path.resolve(rootDir, ".tmp", "clock_code");

function ensureDir(dir) {
  fs.mkdirSync(dir, { recursive: true });
}

function parseArgs(argv) {
  const out = {
    found: "scripts/params/clock_code/deadline_fidelity_found.json",
    seeds: null,
    seedCount: 30,
    steps: 2_000_000,
    reportEvery: 1_000,
    eventEvery: 50_000,
    corruptFrac: 0.1,
    tailWindow: 200_000,
    errGood: 0.1,
    sdiffGood: 1.0,
    region: "stripe",
    regionIndex: null,
  };
  const args = argv.slice(2);
  for (let i = 0; i < args.length; i += 1) {
    const arg = args[i];
    if (arg === "--found") out.found = args[++i];
    else if (arg === "--seeds") out.seeds = parseSeedList(args[++i]);
    else if (arg === "--seedCount") out.seedCount = Number(args[++i]);
    else if (arg === "--steps") out.steps = Number(args[++i]);
    else if (arg === "--reportEvery") out.reportEvery = Number(args[++i]);
    else if (arg === "--eventEvery") out.eventEvery = Number(args[++i]);
    else if (arg === "--corruptFrac") out.corruptFrac = Number(args[++i]);
    else if (arg === "--tailWindow") out.tailWindow = Number(args[++i]);
    else if (arg === "--errGood") out.errGood = Number(args[++i]);
    else if (arg === "--sdiffGood") out.sdiffGood = Number(args[++i]);
    else if (arg === "--region") out.region = args[++i];
    else if (arg === "--regionIndex") out.regionIndex = Number(args[++i]);
  }
  return out;
}

function makeSeedList(opts) {
  if (opts.seeds && opts.seeds.length > 0) return opts.seeds;
  const count = Number.isFinite(opts.seedCount) ? opts.seedCount : 30;
  return Array.from({ length: count }, (_, i) => i + 1);
}

function lcg(seed) {
  let state = seed >>> 0;
  return () => {
    state = (1664525 * state + 1013904223) >>> 0;
    return state / 0x1_0000_0000;
  };
}

function bootstrapMean(values, samples = 2000, seed = 12345) {
  const rand = lcg(seed);
  const n = values.length;
  const means = [];
  for (let i = 0; i < samples; i += 1) {
    let acc = 0;
    for (let j = 0; j < n; j += 1) {
      const idx = Math.floor(rand() * n);
      acc += values[idx];
    }
    means.push(acc / n);
  }
  return means;
}

function ciFromSamples(samples) {
  return {
    low: percentile(samples, 0.025),
    high: percentile(samples, 0.975),
  };
}

function successRate(runs) {
  const ok = runs.map(
    (r) => r.missFrac <= 0.2 && r.uptimeTail >= 0.8 && r.errTailMean <= 0.05,
  );
  const count = ok.filter(Boolean).length;
  return { count, rate: ok.length ? count / ok.length : 0 };
}

ensureDir(outDir);
await loadWasm();

const opts = parseArgs(process.argv);
const foundPath = path.resolve(rootDir, opts.found);
const found = readJson(foundPath);
const baseParams = found.params ?? found;
const deadline = found.deadline ?? opts.deadline;
const seeds = makeSeedList(opts);

const regionIndex =
  Number.isFinite(opts.regionIndex) && opts.regionIndex !== null
    ? opts.regionIndex
    : Math.floor((baseParams.clockK ?? baseParams.gridSize) / 2);

const runArgs = {
  presetParams: baseParams,
  seeds,
  steps: opts.steps,
  reportEvery: opts.reportEvery,
  eventEvery: opts.eventEvery,
  deadline,
  regionType: opts.region,
  regionIndex,
  gateSpan: baseParams.repairGateSpan ?? 1,
  corruptFrac: opts.corruptFrac,
  errGood: opts.errGood,
  sdiffGood: opts.sdiffGood,
  tailWindow: opts.tailWindow,
};

const drift = await runDeadlineEvents({ ...runArgs, variant: "drift" });
const random = await runDeadlineEvents({ ...runArgs, variant: "random" });
const staticCtrl = await runDeadlineEvents({ ...runArgs, variant: "static" });

const variants = { drift, random, static: staticCtrl };

function summarize(runs) {
  return {
    missFrac: runs.map((r) => r.missFrac),
    uptimeTail: runs.map((r) => r.uptimeTail),
    errTail: runs.map((r) => r.errTailMean),
    epClock: runs.map((r) => r.epClockRate),
  };
}

const summary = {};
for (const [name, data] of Object.entries(variants)) {
  const stats = summarize(data.runs);
  const missSamples = bootstrapMean(stats.missFrac, 2000, 101 + stats.missFrac.length);
  const uptimeSamples = bootstrapMean(stats.uptimeTail, 2000, 202 + stats.uptimeTail.length);
  const errSamples = bootstrapMean(stats.errTail, 2000, 303 + stats.errTail.length);
  const epSamples = bootstrapMean(stats.epClock, 2000, 404 + stats.epClock.length);
  const success = successRate(data.runs);
  const successSamples = bootstrapMean(
    data.runs.map((r) =>
      r.missFrac <= 0.2 && r.uptimeTail >= 0.8 && r.errTailMean <= 0.05 ? 1 : 0,
    ),
    2000,
    505 + data.runs.length,
  );
  summary[name] = {
    missMean: mean(stats.missFrac),
    missCI: ciFromSamples(missSamples),
    uptimeMean: mean(stats.uptimeTail),
    uptimeCI: ciFromSamples(uptimeSamples),
    errMean: mean(stats.errTail),
    errCI: ciFromSamples(errSamples),
    epClockMean: mean(stats.epClock),
    epClockCI: ciFromSamples(epSamples),
    successRate: success.rate,
    successCI: ciFromSamples(successSamples),
  };
}

function diffCI(aVals, bVals, seedBase) {
  const aSamples = bootstrapMean(aVals, 2000, seedBase);
  const bSamples = bootstrapMean(bVals, 2000, seedBase + 1234);
  const diffs = aSamples.map((v, i) => v - bSamples[i]);
  return { mean: mean(diffs), ...ciFromSamples(diffs) };
}

const driftStats = summarize(drift.runs);
const randomStats = summarize(random.runs);
const diffMiss = diffCI(driftStats.missFrac, randomStats.missFrac, 7001);
const diffUptime = diffCI(driftStats.uptimeTail, randomStats.uptimeTail, 8001);
const diffErr = diffCI(randomStats.errTail, driftStats.errTail, 9001);
const diffEpClock = diffCI(driftStats.epClock, randomStats.epClock, 10001);

const rawPath = path.join(outDir, "deadline_success_ci_raw.jsonl");
const summaryPath = path.join(outDir, "deadline_success_ci_summary.csv");
const diffPath = path.join(outDir, "deadline_success_ci_diffs.csv");

const rawLines = [];
for (const [variant, data] of Object.entries(variants)) {
  for (const run of data.runs) {
    rawLines.push(JSON.stringify({ variant, ...run }));
  }
}
fs.writeFileSync(rawPath, rawLines.join("\n"));

const summaryHeader = [
  "variant",
  "seeds",
  "missFracMean",
  "missFracCI_low",
  "missFracCI_high",
  "uptimeTailMean",
  "uptimeTailCI_low",
  "uptimeTailCI_high",
  "errTailMean",
  "errTailCI_low",
  "errTailCI_high",
  "epClockRateMean",
  "epClockCI_low",
  "epClockCI_high",
  "successRate",
  "successCI_low",
  "successCI_high",
];
const summaryLines = [summaryHeader.join(",")];
for (const [variant, stats] of Object.entries(summary)) {
  summaryLines.push(
    [
      variant,
      seeds.length,
      stats.missMean,
      stats.missCI.low,
      stats.missCI.high,
      stats.uptimeMean,
      stats.uptimeCI.low,
      stats.uptimeCI.high,
      stats.errMean,
      stats.errCI.low,
      stats.errCI.high,
      stats.epClockMean,
      stats.epClockCI.low,
      stats.epClockCI.high,
      stats.successRate,
      stats.successCI.low,
      stats.successCI.high,
    ].join(","),
  );
}
fs.writeFileSync(summaryPath, summaryLines.join("\n"));

const diffHeader = [
  "metric",
  "mean",
  "ci_low",
  "ci_high",
];
const diffLines = [
  diffHeader.join(","),
  ["missFrac_drift_minus_random", diffMiss.mean, diffMiss.low, diffMiss.high].join(","),
  ["uptimeTail_drift_minus_random", diffUptime.mean, diffUptime.low, diffUptime.high].join(","),
  ["errTail_random_minus_drift", diffErr.mean, diffErr.low, diffErr.high].join(","),
  ["deltaEPClockRate", diffEpClock.mean, diffEpClock.low, diffEpClock.high].join(","),
];
fs.writeFileSync(diffPath, diffLines.join("\n"));

console.log("Deadline success + CI summary:");
for (const [variant, stats] of Object.entries(summary)) {
  console.log(
    `${variant} | success ${(stats.successRate * 100).toFixed(1)}% | uptimeTail ${stats.uptimeMean.toFixed(
      3,
    )} | errTail ${stats.errMean.toFixed(3)} | miss ${stats.missMean.toFixed(3)}`,
  );
}
console.log("Diffs (drift vs random):");
console.log(`missFrac diff mean ${diffMiss.mean.toFixed(3)} CI [${diffMiss.low.toFixed(3)}, ${diffMiss.high.toFixed(3)}]`);
console.log(
  `uptimeTail diff mean ${diffUptime.mean.toFixed(3)} CI [${diffUptime.low.toFixed(3)}, ${diffUptime.high.toFixed(3)}]`,
);
console.log(`errTail diff mean ${diffErr.mean.toFixed(3)} CI [${diffErr.low.toFixed(3)}, ${diffErr.high.toFixed(3)}]`);
console.log(
  `deltaEPClockRate mean ${diffEpClock.mean.toFixed(4)} CI [${diffEpClock.low.toFixed(4)}, ${diffEpClock.high.toFixed(4)}]`,
);
</file>

<file path="scripts/run-fidelity-separation-search.mjs">
#!/usr/bin/env node
import fs from "node:fs";
import path from "node:path";
import {
  loadWasm,
  mean,
  parseSeedList,
  readJson,
  runDeadlineEvents,
} from "./deadline-event-utils.mjs";

const rootDir = path.resolve(path.dirname(new URL(import.meta.url).pathname), "..");
const outDir = path.resolve(rootDir, ".tmp", "clock_code");

function ensureDir(dir) {
  fs.mkdirSync(dir, { recursive: true });
}

ensureDir(outDir);
await loadWasm();

function safeDiv(num, den) {
  return den === 0 ? 0 : num / den;
}

const phasePath = path.join(outDir, "deadline_phase_v3.csv");
if (!fs.existsSync(phasePath)) {
  console.error("Missing phase diagram CSV. Run run-deadline-phase-diagram.mjs first.");
  process.exit(1);
}

const lines = fs.readFileSync(phasePath, "utf8").trim().split("\n");
const header = lines[0].split(",");
const rows = lines.slice(1).map((line) => {
  const parts = line.split(",");
  const row = {};
  header.forEach((h, i) => {
    row[h] = Number(parts[i]);
  });
  return row;
});

rows.sort((a, b) => b.sepScore - a.sepScore);
const top = rows.slice(0, 30);

const steps = 2_000_000;
const reportEvery = 1_000;
const eventEvery = 50_000;
const seeds = Array.from({ length: 10 }, (_, i) => i + 1);
const corruptFrac = 0.1;
const tailWindow = 200_000;

let found = null;
let foundStats = null;
let foundDeadline = null;
let criteriaMet = false;
let bestCandidate = null;
let bestScore = -Infinity;

const candidates = top;

for (const row of candidates) {
  const baseParams = {
    beta: 2.0,
    stepSize: 0.01,
    p3On: 0,
    p6On: 1,
    p6SFactor: 0.0,
    pWrite: 0,
    pNWrite: 0.05,
    pAWrite: 0,
    pSWrite: 1.0,
    muHigh: row.mu,
    muLow: row.mu,
    kappaRep: 1.0,
    r0: 0.25,
    kappaBond: 0.0,
    rStar: 0.22,
    lambdaW: 0.0,
    lW: 4,
    lambdaN: 0.0,
    lN: 6,
    lambdaA: 0.0,
    lA: 6,
    lambdaS: 0.0,
    lS: 1,
    gridSize: row.gridSize,
    rPropose: 0.12,
    metaLayers: 2,
    eta: 0.0,
    etaDrive: row.etaDrive,
    codeNoiseRate: row.codeNoiseRate,
    codeNoiseBatch: 1,
    codeNoiseLayer: 0,
    clockOn: 1,
    clockK: row.gridSize,
    clockFrac: 1.0,
    clockUsesP6: 1,
    repairClockGated: 1,
    repairGateMode: 1,
    repairGateSpan: row.gateSpan,
  };

  const regionIndex = Math.floor(row.gridSize / 2);
  const drift = await runDeadlineEvents({
    presetParams: baseParams,
    variant: "drift",
    seeds,
    steps,
    reportEvery,
    eventEvery,
    deadline: row.deadline,
    regionType: "stripe",
    regionIndex,
    gateSpan: row.gateSpan,
    corruptFrac,
    errGood: 0.1,
    sdiffGood: 1.0,
    tailWindow,
  });
  const random = await runDeadlineEvents({
    presetParams: baseParams,
    variant: "random",
    seeds,
    steps,
    reportEvery,
    eventEvery,
    deadline: row.deadline,
    regionType: "stripe",
    regionIndex,
    gateSpan: row.gateSpan,
    corruptFrac,
    errGood: 0.1,
    sdiffGood: 1.0,
    tailWindow,
  });
  const staticCtrl = await runDeadlineEvents({
    presetParams: baseParams,
    variant: "static",
    seeds,
    steps,
    reportEvery,
    eventEvery,
    deadline: row.deadline,
    regionType: "stripe",
    regionIndex,
    gateSpan: row.gateSpan,
    corruptFrac,
    errGood: 0.1,
    sdiffGood: 1.0,
    tailWindow,
  });

  const driftMiss = mean(drift.runs.map((r) => r.missFrac));
  const driftUptimeTail = mean(drift.runs.map((r) => r.uptimeTail));
  const driftErrTail = mean(drift.runs.map((r) => r.errTailMean));
  const randomMiss = mean(random.runs.map((r) => r.missFrac));
  const randomUptimeTail = mean(random.runs.map((r) => r.uptimeTail));
  const randomErrTail = mean(random.runs.map((r) => r.errTailMean));
  const staticMiss = mean(staticCtrl.runs.map((r) => r.missFrac));
  const staticUptimeTail = mean(staticCtrl.runs.map((r) => r.uptimeTail));

  const okDrift = driftMiss <= 0.2 && driftUptimeTail >= 0.8 && driftErrTail <= 0.05;
  const okRandom = randomMiss >= 0.6 || randomUptimeTail <= 0.5 || randomErrTail >= 0.15;
  const okStatic = staticMiss >= 0.8 && staticUptimeTail <= 0.2;

  const sepMiss = randomMiss - driftMiss;
  const sepUptime = driftUptimeTail - randomUptimeTail;
  const sepErr = randomErrTail - driftErrTail;
  const score = sepMiss + 0.7 * sepUptime + 0.7 * sepErr;
  if (score > bestScore) {
    bestScore = score;
    bestCandidate = { params: baseParams, stats: { drift, random, static: staticCtrl }, deadline: row.deadline };
  }

  if (okDrift && okRandom && okStatic) {
    found = baseParams;
    foundStats = { drift, random, static: staticCtrl };
    foundDeadline = row.deadline;
    criteriaMet = true;
    break;
  }
}

if (!found && bestCandidate) {
  found = bestCandidate.params;
  foundStats = bestCandidate.stats;
  foundDeadline = bestCandidate.deadline;
}

const foundPath = path.resolve(rootDir, "scripts/params/clock_code/deadline_fidelity_found.json");
fs.writeFileSync(
  foundPath,
  JSON.stringify(
    {
      params: found,
      deadline: foundDeadline,
      notes: criteriaMet
        ? "Found via run-fidelity-separation-search.mjs"
        : "Best available candidate; strict fidelity criteria not met",
    },
    null,
    2,
  ),
);

const driftPath = path.resolve(rootDir, "scripts/params/clock_code/deadline_fidelity_drift.json");
const randomPath = path.resolve(rootDir, "scripts/params/clock_code/deadline_fidelity_random.json");
const staticPath = path.resolve(rootDir, "scripts/params/clock_code/deadline_fidelity_static.json");

fs.writeFileSync(driftPath, JSON.stringify({ ...found, clockOn: 1, clockUsesP6: 1 }, null, 2));
fs.writeFileSync(randomPath, JSON.stringify({ ...found, clockOn: 1, clockUsesP6: 0 }, null, 2));
fs.writeFileSync(staticPath, JSON.stringify({ ...found, clockOn: 0, clockUsesP6: 1 }, null, 2));

const rawPath = path.join(outDir, "deadline_fidelity_v3_raw.jsonl");
const summaryPath = path.join(outDir, "deadline_fidelity_v3_summary.csv");
const rawLines = [];
for (const [variant, data] of Object.entries(foundStats)) {
  for (const run of data.runs) {
    rawLines.push(JSON.stringify({ variant, ...run }));
  }
}
fs.writeFileSync(rawPath, rawLines.join("\n"));

const summaryLines = [
  "variant,missFrac,uptimeTail,errTailMean,recoveryP95,epTotalRate,epClockRate,epRepairRate,epNoiseRate",
];
for (const [variant, data] of Object.entries(foundStats)) {
  const missFrac = mean(data.runs.map((r) => r.missFrac));
  const uptimeTail = mean(data.runs.map((r) => r.uptimeTail));
  const errTail = mean(data.runs.map((r) => r.errTailMean));
  const recoveryP95 = mean(data.runs.map((r) => r.recoveryP95 ?? 0));
  const epTotalRate = mean(data.runs.map((r) => r.epTotalRate));
  const epClockRate = mean(data.runs.map((r) => r.epClockRate));
  const epRepairRate = mean(data.runs.map((r) => r.epRepairRate));
  const epNoiseRate = mean(data.runs.map((r) => r.epNoiseRate));
  summaryLines.push(
    [variant, missFrac, uptimeTail, errTail, recoveryP95, epTotalRate, epClockRate, epRepairRate, epNoiseRate].join(","),
  );
}
fs.writeFileSync(summaryPath, summaryLines.join("\n"));

const driftStats = foundStats.drift.runs;
const randomStats = foundStats.random.runs;
const driftMiss = mean(driftStats.map((r) => r.missFrac));
const randomMiss = mean(randomStats.map((r) => r.missFrac));
const driftUptime = mean(driftStats.map((r) => r.uptimeTail));
const randomUptime = mean(randomStats.map((r) => r.uptimeTail));
const driftEp = mean(driftStats.map((r) => r.epTotalRate));
const randomEp = mean(randomStats.map((r) => r.epTotalRate));
const driftClock = mean(driftStats.map((r) => r.epClockRate));
const randomClock = mean(randomStats.map((r) => r.epClockRate));
const driftRepair = mean(driftStats.map((r) => r.epRepairRate));
const randomRepair = mean(randomStats.map((r) => r.epRepairRate));
const avoidedMiss = Math.max(1e-6, randomMiss - driftMiss);
const uptimeGain = Math.max(1e-6, driftUptime - randomUptime);
const deltaEP = driftEp - randomEp;
const deltaEPClock = driftClock - randomClock;
const deltaEPRepair = driftRepair - randomRepair;
const driftClockFrac = safeDiv(driftClock, driftEp);
const driftRepairFrac = safeDiv(driftRepair, driftEp);
const randomClockFrac = safeDiv(randomClock, randomEp);
const randomRepairFrac = safeDiv(randomRepair, randomEp);

const effRows = [
  "source,gridSize,gateSpan,codeNoiseRate,deadline,mu,etaDrive,avoidedMiss,uptimeTailGain,deltaEP,deltaEPClock,deltaEPRepair,EP_per_avoided_miss,EPClock_per_avoided_miss,EPRepair_per_avoided_miss,EPClock_per_uptimeTail_gain,driftClockFrac,driftRepairFrac,randomClockFrac,randomRepairFrac",
];
for (const row of top.slice(0, 10)) {
  const avoided = Math.max(1e-6, row.randomMiss - row.driftMiss);
  const uptime = Math.max(1e-6, row.driftUptimeTail - row.randomUptimeTail);
  const delta = row.driftEpRate - row.randomEpRate;
  const deltaClock = row.driftClockRate - row.randomClockRate;
  const deltaRepair = row.driftRepairRate - row.randomRepairRate;
  effRows.push(
    [
      "phaseTop",
      row.gridSize,
      row.gateSpan,
      row.codeNoiseRate,
      row.deadline,
      row.mu,
      row.etaDrive,
      avoided,
      uptime,
      delta,
      deltaClock,
      deltaRepair,
      safeDiv(delta, avoided),
      safeDiv(deltaClock, avoided),
      safeDiv(deltaRepair, avoided),
      safeDiv(deltaClock, uptime),
      safeDiv(row.driftClockRate, row.driftEpRate),
      safeDiv(row.driftRepairRate, row.driftEpRate),
      safeDiv(row.randomClockRate, row.randomEpRate),
      safeDiv(row.randomRepairRate, row.randomEpRate),
    ].join(","),
  );
}
effRows.push(
  [
    "found",
    found.gridSize,
    found.repairGateSpan,
    found.codeNoiseRate,
    foundDeadline,
    found.muHigh,
    found.etaDrive,
    avoidedMiss,
    uptimeGain,
    deltaEP,
    deltaEPClock,
    deltaEPRepair,
    safeDiv(deltaEP, avoidedMiss),
    safeDiv(deltaEPClock, avoidedMiss),
    safeDiv(deltaEPRepair, avoidedMiss),
    safeDiv(deltaEPClock, uptimeGain),
    driftClockFrac,
    driftRepairFrac,
    randomClockFrac,
    randomRepairFrac,
  ].join(","),
);

const effPath = path.join(outDir, "ep_efficiency_v3.csv");
fs.writeFileSync(effPath, effRows.join("\n"));

console.log("Fidelity separation found:");
console.log(foundPath);
console.log("Summary:");
console.log(summaryLines.join("\n"));

if (!criteriaMet) {
  console.error("No fidelity separation config met strict criteria; wrote best candidate.");
  process.exitCode = 1;
}
</file>

<file path="scripts/run-meta-sweep.mjs">
#!/usr/bin/env node
import fs from "node:fs";
import path from "node:path";
import { fileURLToPath, pathToFileURL } from "node:url";

const __filename = fileURLToPath(import.meta.url);
const __dirname = path.dirname(__filename);
const rootDir = path.resolve(__dirname, "..");
const logsDir = path.resolve(rootDir, ".tmp", "experiments_meta");

const seeds = [1, 2, 3];
const runs = [
  {
    id: "meta2_null_decoupled",
    steps: 3_000_000,
    reportEvery: 1_000_000,
    params: "scripts/params/meta/meta2_null_decoupled.json",
  },
  {
    id: "meta2_null_coupled",
    steps: 3_000_000,
    reportEvery: 1_000_000,
    params: "scripts/params/meta/meta2_null_coupled.json",
  },
  {
    id: "meta2_p6_drive_coupled",
    steps: 2_000_000,
    reportEvery: 1_000_000,
    params: "scripts/params/meta/meta2_p6_drive_coupled.json",
  },
  {
    id: "meta2_p3_pump_coupled",
    steps: 1_000_000,
    reportEvery: 200_000,
    loopEvery: 50_000,
    params: "scripts/params/meta/meta2_p3_pump_coupled.json",
  },
  {
    id: "meta2_p3p6_combo_coupled",
    steps: 2_000_000,
    reportEvery: 500_000,
    loopEvery: 50_000,
    params: "scripts/params/meta/meta2_p3p6_combo_coupled.json",
  },
];

function ensureDir(dir) {
  fs.mkdirSync(dir, { recursive: true });
}

function meanStd(values) {
  const nums = values.filter((v) => Number.isFinite(v));
  if (nums.length === 0) return { mean: null, std: null };
  const mean = nums.reduce((acc, v) => acc + v, 0) / nums.length;
  const variance = nums.reduce((acc, v) => acc + (v - mean) ** 2, 0) / nums.length;
  return { mean, std: Math.sqrt(variance) };
}

function fmt(value, digits = 4) {
  if (value === null || value === undefined || !Number.isFinite(value)) return "n/a";
  return value.toFixed(digits);
}

function graphStats(n, bonds) {
  const parent = Array.from({ length: n }, (_, i) => i);
  const size = Array.from({ length: n }, () => 1);
  const find = (x) => {
    let p = parent[x];
    while (p !== parent[p]) p = parent[p];
    while (x !== p) {
      const next = parent[x];
      parent[x] = p;
      x = next;
    }
    return p;
  };
  const unite = (a, b) => {
    let ra = find(a);
    let rb = find(b);
    if (ra === rb) return;
    if (size[ra] < size[rb]) {
      [ra, rb] = [rb, ra];
    }
    parent[rb] = ra;
    size[ra] += size[rb];
  };
  for (let i = 0; i < bonds.length; i += 2) {
    unite(bonds[i], bonds[i + 1]);
  }
  let components = 0;
  let largest = 0;
  for (let i = 0; i < n; i += 1) {
    if (parent[i] === i) {
      components += 1;
      largest = Math.max(largest, size[i]);
    }
  }
  return { edges: bonds.length / 2, components, largest, n };
}

function formatSummary(totalSteps, energy, diag, graph) {
  const lines = [];
  lines.push(
    `E=${energy.total.toFixed(3)} (Urep ${energy.uRep.toFixed(3)}, Ubond ${energy.uBond.toFixed(3)}, Ew ${energy.eW.toFixed(3)}, En ${energy.eN.toFixed(3)}, Ea ${energy.eA.toFixed(3)}, Es ${energy.eS.toFixed(3)})`,
  );
  lines.push(`Steps: ${totalSteps}`);
  lines.push(
    `P1 steps ${diag.window} | N+ ${diag.wPlus} N- ${diag.wMinus} | Jw ${diag.jW.toFixed(4)} Aw ${diag.aW.toFixed(4)} Σmem ${diag.sigmaMem.toFixed(4)}`,
  );
  lines.push(
    `P2 steps ${diag.window} | N+ ${diag.aPlus} N- ${diag.aMinus} | Ja ${diag.jA.toFixed(4)} Aa ${diag.aA.toFixed(4)}`,
  );
  lines.push(
    `P4 steps ${diag.window} | N+ ${diag.nPlus} N- ${diag.nMinus} | Jn ${diag.jN.toFixed(4)} An ${diag.aN.toFixed(4)}`,
  );
  lines.push(
    `P5 steps ${diag.window} | N+ ${diag.sPlus} N- ${diag.sMinus} | Js ${diag.jS.toFixed(4)} As ${diag.aS.toFixed(4)}`,
  );
  lines.push(`P3 cycle ${diag.p3CycleLen} | disp ${diag.p3DispMag.toFixed(4)} | loop ${diag.p3LoopArea.toFixed(4)}`);
  lines.push(
    `P6 M6 | W ${diag.aM6W.toFixed(4)} N ${diag.aM6N.toFixed(4)} A ${diag.aM6A.toFixed(4)} S ${diag.aM6S.toFixed(4)}`,
  );
  lines.push(
    `Graph edges ${graph.edges} | components ${graph.components} | largest ${graph.largest}/${graph.n}`,
  );
  return lines;
}

function meanAbsDiff(a, b) {
  let sum = 0;
  for (let i = 0; i < a.length; i++) {
    sum += Math.abs(a[i] - b[i]);
  }
  return a.length === 0 ? 0 : sum / a.length;
}

function nonzeroFraction(arr) {
  if (arr.length === 0) return 0;
  let nz = 0;
  for (const v of arr) {
    if (v !== 0) nz += 1;
  }
  return nz / arr.length;
}

function crossLayerMetrics(sim) {
  const baseS = sim.base_s_field();
  const metaS = sim.meta_field();
  const metaW = sim.meta_w_edges();
  const metaA = sim.meta_a_field();
  const metaN = sim.meta_n_field();
  const cells = baseS.length;
  const edgeCount = cells * 2;
  const metaLayers = cells > 0 ? Math.floor(metaS.length / cells) : 0;
  const s0 = metaS.subarray(0, cells);
  const s1 = metaS.subarray(cells, 2 * cells);
  const w0 = metaW.subarray(0, edgeCount);
  const w1 = metaW.subarray(edgeCount, 2 * edgeCount);
  const sdiffBase = meanAbsDiff(baseS, s0);
  const sdiffMeta = meanAbsDiff(s0, s1);
  const wdiffMeta = meanAbsDiff(w0, w1);
  return {
    metaLayers,
    sdiffBase,
    sdiffMeta,
    wdiffMeta,
    nzMetaS: nonzeroFraction(metaS),
    nzMetaW: nonzeroFraction(metaW),
    nzMetaA: nonzeroFraction(metaA),
    nzMetaN: nonzeroFraction(metaN),
  };
}

function readParams(relPath) {
  const file = path.resolve(rootDir, relPath);
  return JSON.parse(fs.readFileSync(file, "utf8"));
}

function checkEtaEffect(mod) {
  const base = {
    gridSize: 12,
    metaLayers: 2,
    p3On: 0,
    p6On: 0,
  };
  const steps = 1_000_000;

  // S-only coupling sanity.
  const sParams = {
    ...base,
    pSWrite: 1,
    pWrite: 0,
    pNWrite: 0,
    pAWrite: 0,
    lambdaS: 0,
    lS: 2,
  };
  const s0 = new mod.Sim(50, 1);
  s0.set_params({ ...sParams, eta: 0.0 });
  s0.step(steps);
  const s1 = new mod.Sim(50, 1);
  s1.set_params({ ...sParams, eta: 1.0 });
  s1.step(steps);
  const sDiff0 = crossLayerMetrics(s0);
  const sDiff1 = crossLayerMetrics(s1);
  const okS0 = sDiff1.sdiffBase <= 0.85 * sDiff0.sdiffBase;
  const okS1 = sDiff1.sdiffMeta <= 0.85 * sDiff0.sdiffMeta;

  // W-only coupling sanity (meta edges).
  const wParams = {
    ...base,
    pSWrite: 0,
    pWrite: 1,
    pNWrite: 0,
    pAWrite: 0,
    lambdaW: 0,
    kappaBond: 0,
    lW: 2,
  };
  const w0 = new mod.Sim(50, 1);
  w0.set_params({ ...wParams, eta: 0.0 });
  w0.step(steps);
  const w1 = new mod.Sim(50, 1);
  w1.set_params({ ...wParams, eta: 1.0 });
  w1.step(steps);
  const wDiff0 = crossLayerMetrics(w0);
  const wDiff1 = crossLayerMetrics(w1);
  const okW = wDiff1.wdiffMeta <= 0.85 * wDiff0.wdiffMeta;

  console.log(
    `Eta sanity S: base/meta0 ${sDiff0.sdiffBase.toFixed(4)} -> ${sDiff1.sdiffBase.toFixed(4)}, ` +
      `meta0/meta1 ${sDiff0.sdiffMeta.toFixed(4)} -> ${sDiff1.sdiffMeta.toFixed(4)}`,
  );
  console.log(
    `Eta sanity W: meta0/meta1 ${wDiff0.wdiffMeta.toFixed(4)} -> ${wDiff1.wdiffMeta.toFixed(4)}`,
  );
  if (!okS0 || !okS1 || !okW) {
    throw new Error("Eta sanity check failed (diff reduction < 15%).");
  }
}

ensureDir(logsDir);

const wasmDir = path.resolve(rootDir, "apps/web/src/wasm/sim_core");
const wasmJs = pathToFileURL(path.join(wasmDir, "sim_core.js")).href;
const wasmBytes = fs.readFileSync(path.join(wasmDir, "sim_core_bg.wasm"));
const mod = await import(wasmJs);
mod.initSync({ module: wasmBytes });

checkEtaEffect(mod);

const summaries = [];
for (const run of runs) {
  const seedResults = [];
  for (const seed of seeds) {
    const params = readParams(run.params);
    const sim = new mod.Sim(200, seed);
    sim.set_params(params);
    const logPath = path.join(logsDir, `${run.id}-seed${seed}.log`);
    const logLines = [];
    let totalSteps = 0;
    const loopSamples = [];
    const dispSamples = [];
    const reportEvery = Math.max(1, Math.floor(run.reportEvery));
    const loopEvery = Math.max(1, Math.floor(run.loopEvery ?? run.reportEvery));
    const chunk = Math.min(reportEvery, loopEvery);
    while (totalSteps < run.steps) {
      const stepNow = Math.min(chunk, run.steps - totalSteps);
      sim.step(stepNow);
      totalSteps += stepNow;
      if (totalSteps % loopEvery === 0 || totalSteps === run.steps) {
        const diag = sim.diagnostics();
        loopSamples.push(Math.abs(diag.p3LoopArea));
        dispSamples.push(Math.abs(diag.p3DispMag));
      }
      if (totalSteps % reportEvery === 0 || totalSteps === run.steps) {
        const energy = sim.energy_breakdown();
        const diag = sim.diagnostics();
        const bonds = sim.bonds(3);
        const graph = graphStats(200, bonds);
        for (const line of formatSummary(totalSteps, energy, diag, graph)) {
          logLines.push(line);
        }
        logLines.push("");
      }
    }
    const cross = crossLayerMetrics(sim);
    logLines.push(
      `Meta S diff base/meta0 ${cross.sdiffBase.toFixed(4)} | meta0/meta1 ${cross.sdiffMeta.toFixed(4)} | ` +
        `W diff meta0/meta1 ${cross.wdiffMeta.toFixed(4)}`,
    );
    logLines.push(
      `Meta nz S ${cross.nzMetaS.toFixed(4)} | W ${cross.nzMetaW.toFixed(4)} | A ${cross.nzMetaA.toFixed(4)} | N ${cross.nzMetaN.toFixed(4)}`,
    );
    fs.writeFileSync(logPath, logLines.join("\n"));

    const loopAbsMean =
      loopSamples.length === 0 ? null : loopSamples.reduce((a, v) => a + v, 0) / loopSamples.length;
    const loopNonzero =
      loopSamples.length === 0
        ? null
        : loopSamples.filter((v) => v > 1e-6).length / loopSamples.length;

    const diag = sim.diagnostics();
    const bonds = sim.bonds(3);
    const graph = graphStats(200, bonds);
    seedResults.push({
      seed,
      metrics: {
        sigmaMem: diag.sigmaMem,
        aw: diag.aW,
        aa: diag.aA,
        an: diag.aN,
        as: diag.aS,
        m6w: diag.aM6W,
        m6n: diag.aM6N,
        m6a: diag.aM6A,
        m6s: diag.aM6S,
        edges: graph.edges,
        largest: graph.largest,
        sdiffBase: cross.sdiffBase,
        sdiffMeta: cross.sdiffMeta,
        wdiffMeta: cross.wdiffMeta,
        nzMetaS: cross.nzMetaS,
        nzMetaW: cross.nzMetaW,
        nzMetaA: cross.nzMetaA,
        nzMetaN: cross.nzMetaN,
      },
      loopAbsMean,
      loopNonzero,
    });
  }

  const metricsKeys = [
    "sigmaMem",
    "aw",
    "aa",
    "an",
    "as",
    "m6w",
    "m6n",
    "m6a",
    "m6s",
    "sdiffBase",
    "sdiffMeta",
    "wdiffMeta",
    "nzMetaS",
    "nzMetaW",
    "nzMetaA",
    "nzMetaN",
    "edges",
    "largest",
  ];
  const summary = { id: run.id };
  for (const key of metricsKeys) {
    const values = seedResults.map((s) => s.metrics[key]).filter((v) => Number.isFinite(v));
    summary[key] = meanStd(values);
  }
  summary.loopAbs = meanStd(seedResults.map((s) => s.loopAbsMean));
  summary.loopNonzero = meanStd(seedResults.map((s) => s.loopNonzero));
  summary.seeds = seedResults;
  summaries.push(summary);
}

console.log("Summary table (mean ± std):");
console.log(
  [
    "preset",
    "SigmaMem",
    "Aw",
    "Aa",
    "An",
    "As",
    "M6W",
    "M6N",
    "M6A",
    "M6S",
    "loop|mean|",
    "loop>0 frac",
    "Sdiff base/meta0",
    "Sdiff meta0/meta1",
    "Wdiff meta0/meta1",
    "nz metaS",
    "nz metaW",
    "nz metaA",
    "nz metaN",
    "edges",
    "largest",
  ].join("\t"),
);
for (const s of summaries) {
  console.log(
    [
      s.id,
      `${fmt(s.sigmaMem.mean)}±${fmt(s.sigmaMem.std)}`,
      `${fmt(s.aw.mean)}±${fmt(s.aw.std)}`,
      `${fmt(s.aa.mean)}±${fmt(s.aa.std)}`,
      `${fmt(s.an.mean)}±${fmt(s.an.std)}`,
      `${fmt(s.as.mean)}±${fmt(s.as.std)}`,
      `${fmt(s.m6w.mean)}±${fmt(s.m6w.std)}`,
      `${fmt(s.m6n.mean)}±${fmt(s.m6n.std)}`,
      `${fmt(s.m6a.mean)}±${fmt(s.m6a.std)}`,
      `${fmt(s.m6s.mean)}±${fmt(s.m6s.std)}`,
      `${fmt(s.loopAbs.mean)}±${fmt(s.loopAbs.std)}`,
      `${fmt(s.loopNonzero.mean)}±${fmt(s.loopNonzero.std)}`,
      `${fmt(s.sdiffBase.mean)}±${fmt(s.sdiffBase.std)}`,
      `${fmt(s.sdiffMeta.mean)}±${fmt(s.sdiffMeta.std)}`,
      `${fmt(s.wdiffMeta.mean)}±${fmt(s.wdiffMeta.std)}`,
      `${fmt(s.nzMetaS.mean)}±${fmt(s.nzMetaS.std)}`,
      `${fmt(s.nzMetaW.mean)}±${fmt(s.nzMetaW.std)}`,
      `${fmt(s.nzMetaA.mean)}±${fmt(s.nzMetaA.std)}`,
      `${fmt(s.nzMetaN.mean)}±${fmt(s.nzMetaN.std)}`,
      `${fmt(s.edges.mean, 0)}±${fmt(s.edges.std, 0)}`,
      `${fmt(s.largest.mean, 0)}±${fmt(s.largest.std, 0)}`,
    ].join("\t"),
  );
}
</file>

<file path="scripts/run-opk-composed-hierarchy.mjs">
#!/usr/bin/env node
import fs from "node:fs";
import path from "node:path";
import { loadWasm, mean, std } from "./deadline-event-utils.mjs";
import { parseOpOffsets, computeSpearman } from "./opk-metrics.mjs";

const rootDir = path.resolve(path.dirname(new URL(import.meta.url).pathname), "..");
const outDir = path.resolve(rootDir, ".tmp", "op_coupling");

function ensureDir(dir) {
  fs.mkdirSync(dir, { recursive: true });
}

class LcgRng {
  constructor(seed) {
    this.state = seed >>> 0;
  }
  next() {
    this.state = (1664525 * this.state + 1013904223) >>> 0;
    return this.state / 0xffffffff;
  }
  int(max) {
    return Math.floor(this.next() * max);
  }
}

function sampleOffsetIndex(tokens, start, rCount, rng) {
  let total = 0;
  for (let r = 0; r < rCount; r += 1) total += tokens[start + r];
  if (total <= 0) return 0;
  let pick = rng.int(total);
  for (let r = 0; r < rCount; r += 1) {
    pick -= tokens[start + r];
    if (pick < 0) return r;
  }
  return rCount - 1;
}

function computeR2Eff({
  metaLayers,
  rCount,
  opOffsets,
  opKTokens,
  gridSize,
  samples,
  rng,
}) {
  const offsets = parseOpOffsets(opOffsets);
  const cells = gridSize * gridSize;
  const r2Eff = [];
  const hEff = [];

  for (let d = 1; d <= metaLayers; d += 1) {
    let r2Sum = 0;
    const counts = new Map();
    for (let i = 0; i < samples; i += 1) {
      let dx = 0;
      let dy = 0;
      for (let step = 0; step < d; step += 1) {
        const iface = rng.int(d);
        const q = rng.int(cells);
        const start = (iface * cells + q) * rCount;
        const rIdx = sampleOffsetIndex(opKTokens, start, rCount, rng);
        const [ox, oy] = offsets[rIdx];
        dx += ox;
        dy += oy;
      }
      const key = `${dx},${dy}`;
      counts.set(key, (counts.get(key) ?? 0) + 1);
      r2Sum += dx * dx + dy * dy;
    }
    const r2Mean = samples > 0 ? r2Sum / samples : 0;
    let h = 0;
    for (const count of counts.values()) {
      const p = count / samples;
      if (p > 0) h += -p * Math.log(p);
    }
    r2Eff.push(r2Mean);
    hEff.push(h);
  }

  return { r2Eff, hEff };
}

function tvField(field, g) {
  const cells = g * g;
  let acc = 0;
  for (let q = 0; q < cells; q += 1) {
    const x = q % g;
    const y = Math.floor(q / g);
    const right = y * g + ((x + 1) % g);
    const down = ((y + 1) % g) * g + x;
    acc += Math.abs(field[q] - field[right]) + Math.abs(field[q] - field[down]);
  }
  return acc / cells;
}

function meanArray(arrays, idx) {
  return mean(arrays.map((a) => a[idx]));
}

function stdArray(arrays, idx, meanVal) {
  const variance = arrays.reduce((acc, a) => acc + (a[idx] - meanVal) ** 2, 0) / arrays.length;
  return Math.sqrt(variance);
}

ensureDir(outDir);
const mod = await loadWasm();

const configs = [
  {
    id: "null",
    params: {
      gridSize: 16,
      metaLayers: 4,
      lS: 10,
      lambdaS: 0,
      pWrite: 0,
      pNWrite: 0,
      pAWrite: 0,
      pSWrite: 1,
      opCouplingOn: 1,
      sCouplingMode: 1,
      opStencil: 0,
      opBudgetK: 8,
      initRandom: 1,
      p3On: 0,
      p6On: 0,
      eta: 0.6,
      etaDrive: 0,
      opDriveOnK: 0,
      muHigh: 1.0,
      muLow: -1.0,
    },
  },
  {
    id: "drive",
    params: {
      gridSize: 16,
      metaLayers: 3,
      lS: 10,
      lambdaS: 0,
      pWrite: 0,
      pNWrite: 0,
      pAWrite: 0,
      pSWrite: 1,
      opCouplingOn: 1,
      sCouplingMode: 1,
      opStencil: 0,
      opBudgetK: 8,
      initRandom: 1,
      p3On: 0,
      p6On: 1,
      eta: 0,
      etaDrive: 0.6,
      opDriveOnK: 1,
      muHigh: 1.0,
      muLow: 1.0,
    },
  },
];

const seeds = Array.from({ length: 10 }, (_, i) => i + 1);
const steps = 1_000_000;
const samplesPerDepth = 50_000;

const rawRows = [];
const summaryRows = [];

for (const cfg of configs) {
  const r2ArrAll = [];
  const hArrAll = [];
  const tvArrAll = [];
  const rhoR2Arr = [];
  const rhoTVArr = [];

  for (const seed of seeds) {
    const sim = new mod.Sim(200, seed);
    sim.set_params(cfg.params);
    sim.step(steps);

    const baseS = sim.base_s_field();
    const metaS = sim.meta_field();
    const opK = sim.op_k_tokens();
    const opOffsets = sim.op_offsets();
    const rCount = sim.op_r_count();
    const metaLayers = cfg.params.metaLayers;

    const rng = new LcgRng(seed * 99991);
    const { r2Eff, hEff } = computeR2Eff({
      metaLayers,
      rCount,
      opOffsets,
      opKTokens: opK,
      gridSize: cfg.params.gridSize,
      samples: samplesPerDepth,
      rng,
    });

    const tvByLayer = [];
    const cells = cfg.params.gridSize * cfg.params.gridSize;
    tvByLayer.push(tvField(baseS, cfg.params.gridSize));
    for (let layer = 0; layer < metaLayers; layer += 1) {
      const start = layer * cells;
      const slice = metaS.subarray(start, start + cells);
      tvByLayer.push(tvField(slice, cfg.params.gridSize));
    }

    const depthIdx = r2Eff.map((_, i) => i + 1);
    const layerIdx = tvByLayer.map((_, i) => i);
    const rhoR2 = computeSpearman(depthIdx, r2Eff);
    const rhoTV = computeSpearman(layerIdx, tvByLayer);

    r2ArrAll.push(r2Eff);
    hArrAll.push(hEff);
    tvArrAll.push(tvByLayer);
    rhoR2Arr.push(rhoR2);
    rhoTVArr.push(rhoTV);

    rawRows.push(
      JSON.stringify({
        config: cfg.id,
        seed,
        r2Eff,
        hEff,
        tvByLayer,
        rhoR2,
        rhoTV,
      }),
    );
  }

  const depthCount = r2ArrAll[0]?.length ?? 0;
  const layerCount = tvArrAll[0]?.length ?? 0;
  const r2Mean = [];
  const r2Std = [];
  const hMean = [];
  const hStd = [];
  const tvMean = [];
  const tvStd = [];

  for (let i = 0; i < depthCount; i += 1) {
    const m = meanArray(r2ArrAll, i);
    r2Mean.push(m);
    r2Std.push(stdArray(r2ArrAll, i, m));
    const hm = meanArray(hArrAll, i);
    hMean.push(hm);
    hStd.push(stdArray(hArrAll, i, hm));
  }

  for (let i = 0; i < layerCount; i += 1) {
    const m = meanArray(tvArrAll, i);
    tvMean.push(m);
    tvStd.push(stdArray(tvArrAll, i, m));
  }

  const rhoR2Mean = mean(rhoR2Arr);
  const rhoR2Std = std(rhoR2Arr);
  const rhoTVMean = mean(rhoTVArr);
  const rhoTVStd = std(rhoTVArr);

  const r2Pos = rhoR2Arr.filter((v) => v >= 0.7).length;
  const r2Neg = rhoR2Arr.filter((v) => v <= -0.7).length;
  const tvPos = rhoTVArr.filter((v) => v >= 0.7).length;
  const tvNeg = rhoTVArr.filter((v) => v <= -0.7).length;

  const r2Consistent = Math.max(r2Pos, r2Neg);
  const tvConsistent = Math.max(tvPos, tvNeg);
  const signalFound = r2Consistent >= 7 || tvConsistent >= 7;

  summaryRows.push({
    config: cfg.id,
    metaLayers: cfg.params.metaLayers,
    opStencil: cfg.params.opStencil,
    opBudgetK: cfg.params.opBudgetK,
    r2Mean: JSON.stringify(r2Mean),
    r2Std: JSON.stringify(r2Std),
    hMean: JSON.stringify(hMean),
    hStd: JSON.stringify(hStd),
    tvMean: JSON.stringify(tvMean),
    tvStd: JSON.stringify(tvStd),
    rhoR2Mean,
    rhoR2Std,
    rhoTVMean,
    rhoTVStd,
    r2Consistent,
    tvConsistent,
    signal: signalFound ? "COMPOSED_HIERARCHY_SIGNAL_FOUND" : "NO_COMPOSED_HIERARCHY_SIGNAL_FOUND",
  });
}

const summaryPath = path.join(outDir, "opk_composed_hierarchy_summary.csv");
const header = [
  "config",
  "metaLayers",
  "opStencil",
  "opBudgetK",
  "r2Mean",
  "r2Std",
  "hMean",
  "hStd",
  "tvMean",
  "tvStd",
  "rhoR2Mean",
  "rhoR2Std",
  "rhoTVMean",
  "rhoTVStd",
  "r2Consistent",
  "tvConsistent",
  "signal",
];
const lines = [
  header.join(","),
  ...summaryRows.map((row) =>
    [
      row.config,
      row.metaLayers,
      row.opStencil,
      row.opBudgetK,
      row.r2Mean,
      row.r2Std,
      row.hMean,
      row.hStd,
      row.tvMean,
      row.tvStd,
      row.rhoR2Mean,
      row.rhoR2Std,
      row.rhoTVMean,
      row.rhoTVStd,
      row.r2Consistent,
      row.tvConsistent,
      row.signal,
    ].join(","),
  ),
].join("\n");
fs.writeFileSync(summaryPath, `${lines}\n`);
fs.writeFileSync(path.join(outDir, "opk_composed_hierarchy_raw.jsonl"), rawRows.join("\n"));

for (const row of summaryRows) {
  console.log(`${row.config} ${row.signal} rhoR2=${row.rhoR2Mean.toFixed(3)} rhoTV=${row.rhoTVMean.toFixed(3)}`);
}
</file>

<file path="scripts/run-opk-diagnostics.mjs">
#!/usr/bin/env node
import fs from "node:fs";
import path from "node:path";
import { fileURLToPath, pathToFileURL } from "node:url";
import { computeOpkMetrics, finiteCheck } from "./opk-metrics.mjs";

const __filename = fileURLToPath(import.meta.url);
const __dirname = path.dirname(__filename);
const rootDir = path.resolve(__dirname, "..");
const outDir = path.resolve(rootDir, ".tmp", "op_coupling");

function ensureDir(dir) {
  fs.mkdirSync(dir, { recursive: true });
}

const wasmDir = path.resolve(rootDir, "apps/web/src/wasm/sim_core");
const wasmJs = pathToFileURL(path.join(wasmDir, "sim_core.js")).href;
const wasmBytes = fs.readFileSync(path.join(wasmDir, "sim_core_bg.wasm"));
const mod = await import(wasmJs);
mod.initSync({ module: wasmBytes });

const sampleSteps = [0, 200_000, 400_000, 800_000, 1_200_000];

function sampleKInvariants({ opK, rCount, budget, interfaces, cells, rng }) {
  const samples = Math.min(50, interfaces * cells);
  for (let i = 0; i < samples; i += 1) {
    const iface = rng() % interfaces;
    const q = rng() % cells;
    const start = (iface * cells + q) * rCount;
    let sum = 0;
    for (let r = 0; r < rCount; r += 1) sum += opK[start + r];
    if (sum !== budget) return false;
  }
  return true;
}

function makeLCG(seed) {
  let state = seed >>> 0;
  return () => {
    state = (Math.imul(1664525, state) + 1013904223) >>> 0;
    return state;
  };
}

function runVariant(variant, overrides) {
  const params = {
    beta: 5.0,
    gridSize: 16,
    metaLayers: 2,
    lS: 10,
    lambdaS: 0,
    p3On: 0,
    p6On: 0,
    pWrite: 0,
    pNWrite: 0,
    pAWrite: 0,
    pSWrite: 1,
    etaDrive: 0,
    opCouplingOn: 1,
    sCouplingMode: 1,
    opStencil: 1,
    opBudgetK: 16,
    initRandom: 1,
    ...overrides,
  };

  const sim = new mod.Sim(200, 1);
  sim.set_params(params);

  const rows = [];
  let lastStep = 0;
  let lastEpExact = sim.ep_exact_total();
  let lastEpOpK = sim.ep_exact_by_move()[9] ?? 0;

  for (const targetStep of sampleSteps) {
    const delta = targetStep - lastStep;
    if (delta > 0) sim.step(delta);
    const epExact = sim.ep_exact_total();
    const epByMove = sim.ep_exact_by_move();
    const epOpK = epByMove[9] ?? 0;
    const window = targetStep - lastStep || 1;
    const epExactRateWindow = (epExact - lastEpExact) / window;
    const epOpKRateWindow = (epOpK - lastEpOpK) / window;

    const baseS = sim.base_s_field();
    const metaS = sim.meta_field();
    const opK = sim.op_k_tokens();
    const opOffsets = sim.op_offsets();
    const rCount = sim.op_r_count();
    const budget = sim.op_budget_k();
    const interfaces = sim.op_interfaces();
    const cells = params.gridSize * params.gridSize;
    const metrics = computeOpkMetrics({
      gridSize: params.gridSize,
      metaLayers: params.metaLayers,
      rCount,
      opBudgetK: budget,
      opOffsets,
      opKTokens: opK,
      baseS,
      metaS,
      lS: params.lS,
    });

    const rng = makeLCG(targetStep + 17);
    const kSampleOk = sampleKInvariants({
      opK,
      rCount,
      budget,
      interfaces,
      cells,
      rng,
    });

    rows.push({
      variant,
      step: targetStep,
      params,
      epExact,
      epExactRateWindow,
      epOpK,
      epOpKRateWindow,
      metrics,
      kSampleOk,
    });

    lastStep = targetStep;
    lastEpExact = epExact;
    lastEpOpK = epOpK;
  }

  return rows;
}

ensureDir(outDir);
const rawRows = [];
const summaryRows = [];

for (const { variant, eta } of [
  { variant: "eta0", eta: 0.0 },
  { variant: "eta06", eta: 0.6 },
]) {
  const rows = runVariant(variant, { eta });
  for (const row of rows) {
    if (!finiteCheck(row.metrics)) {
      throw new Error(`Non-finite metrics for ${variant} at step ${row.step}`);
    }
    rawRows.push({
      variant: row.variant,
      step: row.step,
      epExact: row.epExact,
      epExactRateWindow: row.epExactRateWindow,
      epOpK: row.epOpK,
      epOpKRateWindow: row.epOpKRateWindow,
      kSampleOk: row.kSampleOk,
      metrics: row.metrics,
    });
  }

  const first = rows[0];
  const last = rows[rows.length - 1];
  for (const row of [first, last]) {
    summaryRows.push({
      variant: row.variant,
      step: row.step,
      m0Mean: row.metrics.summary.m0Mean,
      HMean: row.metrics.summary.hMean,
      R2Mean: row.metrics.summary.r2Mean,
      AMean: row.metrics.summary.aMean,
      cohMean: row.metrics.summary.cohMean,
      SdiffMean: row.metrics.summary.sdiffMean,
      epExactRateWindow: row.epExactRateWindow,
      epOpKRateWindow: row.epOpKRateWindow,
      kSampleOk: row.kSampleOk ? 1 : 0,
    });
  }
}

const rawPath = path.join(outDir, "opk_diag_raw.jsonl");
const summaryPath = path.join(outDir, "opk_diag_summary.csv");
fs.writeFileSync(rawPath, rawRows.map((r) => JSON.stringify(r)).join("\n"));

const header = [
  "variant",
  "step",
  "m0Mean",
  "HMean",
  "R2Mean",
  "AMean",
  "cohMean",
  "SdiffMean",
  "epExactRateWindow",
  "epOpKRateWindow",
  "kSampleOk",
];
const lines = [
  header.join(","),
  ...summaryRows.map((row) =>
    [
      row.variant,
      row.step,
      row.m0Mean,
      row.HMean,
      row.R2Mean,
      row.AMean,
      row.cohMean,
      row.SdiffMean,
      row.epExactRateWindow,
      row.epOpKRateWindow,
      row.kSampleOk,
    ].join(","),
  ),
];
fs.writeFileSync(summaryPath, `${lines.join("\n")}\n`);

console.log(`opk diagnostics written to ${summaryPath}`);
</file>

<file path="scripts/run-opk-hierarchy-search.mjs">
#!/usr/bin/env node
import fs from "node:fs";
import path from "node:path";
import { fileURLToPath, pathToFileURL } from "node:url";
import { computeOpkMetrics, computeSpearman, finiteCheck } from "./opk-metrics.mjs";

const __filename = fileURLToPath(import.meta.url);
const __dirname = path.dirname(__filename);
const rootDir = path.resolve(__dirname, "..");
const outDir = path.resolve(rootDir, ".tmp", "op_coupling");

function ensureDir(dir) {
  fs.mkdirSync(dir, { recursive: true });
}

const wasmDir = path.resolve(rootDir, "apps/web/src/wasm/sim_core");
const wasmJs = pathToFileURL(path.join(wasmDir, "sim_core.js")).href;
const wasmBytes = fs.readFileSync(path.join(wasmDir, "sim_core_bg.wasm"));
const mod = await import(wasmJs);
mod.initSync({ module: wasmBytes });

function mean(values) {
  return values.reduce((acc, v) => acc + v, 0) / values.length;
}

function std(values, meanVal) {
  const variance = values.reduce((acc, v) => acc + (v - meanVal) ** 2, 0) / values.length;
  return Math.sqrt(variance);
}

const metaLayersList = [2, 3, 4];
const stencilList = [0, 1];
const budgetList = [8, 16, 32];
const seeds = [1, 2, 3];
const steps = 1_000_000;

const regimes = [
  {
    id: "null",
    params: {
      p3On: 0,
      p6On: 0,
      eta: 0.6,
      etaDrive: 0,
      opDriveOnK: 0,
      muHigh: 1.0,
      muLow: -1.0,
    },
  },
  {
    id: "drive",
    params: {
      p3On: 0,
      p6On: 1,
      eta: 0.0,
      etaDrive: 0.6,
      opDriveOnK: 1,
      muHigh: 1.0,
      muLow: 1.0,
    },
  },
];

ensureDir(outDir);
const rawPath = path.join(outDir, "opk_hierarchy_raw.jsonl");
const summaryPath = path.join(outDir, "opk_hierarchy_summary.csv");
const bestPath = path.join(outDir, "opk_hierarchy_best.json");

const rawRows = [];
const summaryRows = [];
const bestByRegime = {
  null: { R2: null, H: null, coh: null },
  drive: { R2: null, H: null, coh: null },
};

for (const regime of regimes) {
  const rhoR2All = [];
  const rhoHAll = [];
  const rhoCohAll = [];
  const deltaR2All = [];
  const deltaHAll = [];
  const deltaCohAll = [];

  for (const metaLayers of metaLayersList) {
    for (const opStencil of stencilList) {
      for (const opBudgetK of budgetList) {
        const rhoR2Seeds = [];
        const rhoHSeeds = [];
        const rhoCohSeeds = [];
        const deltaR2Seeds = [];
        const deltaHSeeds = [];
        const deltaCohSeeds = [];

        for (const seed of seeds) {
          const params = {
            beta: 5.0,
            gridSize: 16,
            metaLayers,
            lS: 10,
            lambdaS: 0,
            pWrite: 0,
            pNWrite: 0,
            pAWrite: 0,
            pSWrite: 1,
            opCouplingOn: 1,
            sCouplingMode: 1,
            opStencil,
            opBudgetK,
            initRandom: 1,
            ...regime.params,
          };

          const sim = new mod.Sim(200, seed);
          sim.set_params(params);
          sim.step(steps);

          const baseS = sim.base_s_field();
          const metaS = sim.meta_field();
          const opK = sim.op_k_tokens();
          const opOffsets = sim.op_offsets();
          const rCount = sim.op_r_count();
          const budget = sim.op_budget_k();

          const metrics = computeOpkMetrics({
            gridSize: params.gridSize,
            metaLayers,
            rCount,
            opBudgetK: budget,
            opOffsets,
            opKTokens: opK,
            baseS,
            metaS,
            lS: params.lS,
          });

          if (!finiteCheck(metrics)) {
            throw new Error(`Non-finite metrics for ${regime.id} seed ${seed}`);
          }

          const ifaceIndex = metrics.r2Arr.map((_, idx) => idx);
          const rhoEligible = metaLayers >= 3;
          const rhoR2 = rhoEligible ? computeSpearman(ifaceIndex, metrics.r2Arr) : NaN;
          const rhoH = rhoEligible ? computeSpearman(ifaceIndex, metrics.hArr) : NaN;
          const rhoCoh = rhoEligible ? computeSpearman(ifaceIndex, metrics.cohArr) : NaN;
          const deltaR2 = metrics.r2Arr[metrics.r2Arr.length - 1] - metrics.r2Arr[0];
          const deltaH = metrics.hArr[metrics.hArr.length - 1] - metrics.hArr[0];
          const deltaCoh = metrics.cohArr[metrics.cohArr.length - 1] - metrics.cohArr[0];

          rhoR2Seeds.push(rhoR2);
          rhoHSeeds.push(rhoH);
          rhoCohSeeds.push(rhoCoh);
          deltaR2Seeds.push(deltaR2);
          deltaHSeeds.push(deltaH);
          deltaCohSeeds.push(deltaCoh);

          rawRows.push({
            regime: regime.id,
            metaLayers,
            opStencil,
            opBudgetK,
            seed,
            rhoR2,
            rhoH,
            rhoCoh,
            deltaR2,
            deltaH,
            deltaCoh,
            metrics,
          });
        }

        const validRhoR2 = rhoR2Seeds.filter((v) => Number.isFinite(v));
        const validRhoH = rhoHSeeds.filter((v) => Number.isFinite(v));
        const validRhoCoh = rhoCohSeeds.filter((v) => Number.isFinite(v));
        const rhoR2Mean = validRhoR2.length ? mean(validRhoR2) : NaN;
        const rhoHMean = validRhoH.length ? mean(validRhoH) : NaN;
        const rhoCohMean = validRhoCoh.length ? mean(validRhoCoh) : NaN;
        const deltaR2Mean = mean(deltaR2Seeds);
        const deltaHMean = mean(deltaHSeeds);
        const deltaCohMean = mean(deltaCohSeeds);
        const deltaR2Std = std(deltaR2Seeds, deltaR2Mean);
        const deltaHStd = std(deltaHSeeds, deltaHMean);
        const deltaCohStd = std(deltaCohSeeds, deltaCohMean);

        const signalThreshold = 0.02;
        const signalCheck = (arr) => {
          const pos = arr.filter((v) => v >= signalThreshold).length;
          const neg = arr.filter((v) => v <= -signalThreshold).length;
          const total = arr.length;
          const dominant = Math.max(pos, neg);
          return dominant >= Math.ceil((2 * total) / 3);
        };
        const signalR2 = signalCheck(deltaR2Seeds);
        const signalH = signalCheck(deltaHSeeds);
        const signalCoh = signalCheck(deltaCohSeeds);

        if (Number.isFinite(rhoR2Mean)) rhoR2All.push(rhoR2Mean);
        if (Number.isFinite(rhoHMean)) rhoHAll.push(rhoHMean);
        if (Number.isFinite(rhoCohMean)) rhoCohAll.push(rhoCohMean);
        deltaR2All.push(deltaR2Mean);
        deltaHAll.push(deltaHMean);
        deltaCohAll.push(deltaCohMean);

        const row = {
          regime: regime.id,
          metaLayers,
          opStencil,
          opBudgetK,
          seeds: seeds.length,
          rhoR2Mean,
          rhoHMean,
          rhoCohMean,
          deltaR2Mean,
          deltaHMean,
          deltaCohMean,
          deltaR2Std,
          deltaHStd,
          deltaCohStd,
          signalR2,
          signalH,
          signalCoh,
        };
        summaryRows.push(row);

        const updateBest = (metric, value) => {
          const current = bestByRegime[regime.id][metric];
          if (!Number.isFinite(value)) {
            return;
          }
          if (!current || Math.abs(value) > Math.abs(current.value)) {
            bestByRegime[regime.id][metric] = { value, row };
          }
        };
        updateBest("R2", rhoR2Mean);
        updateBest("H", rhoHMean);
        updateBest("coh", rhoCohMean);
      }
    }
  }

  const rhoR2MeanAll = rhoR2All.length ? mean(rhoR2All) : NaN;
  const rhoHMeanAll = rhoHAll.length ? mean(rhoHAll) : NaN;
  const rhoCohMeanAll = rhoCohAll.length ? mean(rhoCohAll) : NaN;
  summaryRows.push({
    regime: regime.id,
    metaLayers: "",
    opStencil: "",
    opBudgetK: "",
    seeds: "",
    rhoR2Mean: rhoR2MeanAll,
    rhoHMean: rhoHMeanAll,
    rhoCohMean: rhoCohMeanAll,
    deltaR2Mean: std(rhoR2All, rhoR2MeanAll),
    deltaHMean: std(rhoHAll, rhoHMeanAll),
    deltaCohMean: std(rhoCohAll, rhoCohMeanAll),
    deltaR2Std: std(deltaR2All, mean(deltaR2All)),
    deltaHStd: std(deltaHAll, mean(deltaHAll)),
    deltaCohStd: std(deltaCohAll, mean(deltaCohAll)),
    signalR2: "",
    signalH: "",
    signalCoh: "",
    note: "SUMMARY_MEAN_STD_RHO",
  });
}

for (const regime of ["null", "drive"]) {
  const bestR2 = bestByRegime[regime].R2;
  const bestH = bestByRegime[regime].H;
  const bestCoh = bestByRegime[regime].coh;
  if (bestR2) {
    summaryRows.push({
      regime,
      metaLayers: bestR2.row.metaLayers,
      opStencil: bestR2.row.opStencil,
      opBudgetK: bestR2.row.opBudgetK,
      seeds: bestR2.row.seeds,
      rhoR2Mean: bestR2.row.rhoR2Mean,
      rhoHMean: bestR2.row.rhoHMean,
      rhoCohMean: bestR2.row.rhoCohMean,
      deltaR2Mean: bestR2.row.deltaR2Mean,
      deltaHMean: bestR2.row.deltaHMean,
      deltaCohMean: bestR2.row.deltaCohMean,
      note: "BEST_R2",
    });
  }
  if (bestH) {
    summaryRows.push({
      regime,
      metaLayers: bestH.row.metaLayers,
      opStencil: bestH.row.opStencil,
      opBudgetK: bestH.row.opBudgetK,
      seeds: bestH.row.seeds,
      rhoR2Mean: bestH.row.rhoR2Mean,
      rhoHMean: bestH.row.rhoHMean,
      rhoCohMean: bestH.row.rhoCohMean,
      deltaR2Mean: bestH.row.deltaR2Mean,
      deltaHMean: bestH.row.deltaHMean,
      deltaCohMean: bestH.row.deltaCohMean,
      note: "BEST_H",
    });
  }
  if (bestCoh) {
    summaryRows.push({
      regime,
      metaLayers: bestCoh.row.metaLayers,
      opStencil: bestCoh.row.opStencil,
      opBudgetK: bestCoh.row.opBudgetK,
      seeds: bestCoh.row.seeds,
      rhoR2Mean: bestCoh.row.rhoR2Mean,
      rhoHMean: bestCoh.row.rhoHMean,
      rhoCohMean: bestCoh.row.rhoCohMean,
      deltaR2Mean: bestCoh.row.deltaR2Mean,
      deltaHMean: bestCoh.row.deltaHMean,
      deltaCohMean: bestCoh.row.deltaCohMean,
      note: "BEST_COH",
    });
  }
}

const header = [
  "regime",
  "metaLayers",
  "opStencil",
  "opBudgetK",
  "seeds",
  "rhoR2Mean",
  "rhoHMean",
  "rhoCohMean",
  "deltaR2Mean",
  "deltaHMean",
  "deltaCohMean",
  "deltaR2Std",
  "deltaHStd",
  "deltaCohStd",
  "signalR2",
  "signalH",
  "signalCoh",
  "note",
];
const lines = [
  header.join(","),
  ...summaryRows.map((row) =>
    [
      row.regime,
      row.metaLayers,
      row.opStencil,
      row.opBudgetK,
      row.seeds,
      row.rhoR2Mean,
      row.rhoHMean,
      row.rhoCohMean,
      row.deltaR2Mean,
      row.deltaHMean,
      row.deltaCohMean,
      row.deltaR2Std ?? "",
      row.deltaHStd ?? "",
      row.deltaCohStd ?? "",
      row.signalR2 ?? "",
      row.signalH ?? "",
      row.signalCoh ?? "",
      row.note ?? "",
    ].join(","),
  ),
  ["NOTE", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "metaLayers=2 excluded from rho ranking"].join(
    ",",
  ),
  [
    "NOTE",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    summaryRows.some((row) => row.signalR2 || row.signalH || row.signalCoh)
      ? "HIERARCHY_SIGNAL_FOUND"
      : "NO_HIERARCHY_SIGNAL_FOUND",
  ].join(","),
].join("\n");

fs.writeFileSync(rawPath, rawRows.map((r) => JSON.stringify(r)).join("\n"));
fs.writeFileSync(summaryPath, `${lines}\n`);
fs.writeFileSync(bestPath, JSON.stringify(bestByRegime, null, 2));

const bestR2Null = bestByRegime.null.R2?.row;
const bestHNull = bestByRegime.null.H?.row;
const bestCohNull = bestByRegime.null.coh?.row;
const bestR2Drive = bestByRegime.drive.R2?.row;
const bestHDrive = bestByRegime.drive.H?.row;
const bestCohDrive = bestByRegime.drive.coh?.row;

if (bestR2Null) {
  console.log(
    `BEST_R2 null: metaLayers=${bestR2Null.metaLayers} stencil=${bestR2Null.opStencil} budget=${bestR2Null.opBudgetK} rho=${bestR2Null.rhoR2Mean.toFixed(3)}`,
  );
}
if (bestR2Drive) {
  console.log(
    `BEST_R2 drive: metaLayers=${bestR2Drive.metaLayers} stencil=${bestR2Drive.opStencil} budget=${bestR2Drive.opBudgetK} rho=${bestR2Drive.rhoR2Mean.toFixed(3)}`,
  );
}
if (bestHNull) {
  console.log(
    `BEST_H null: metaLayers=${bestHNull.metaLayers} stencil=${bestHNull.opStencil} budget=${bestHNull.opBudgetK} rho=${bestHNull.rhoHMean.toFixed(3)}`,
  );
}
if (bestHDrive) {
  console.log(
    `BEST_H drive: metaLayers=${bestHDrive.metaLayers} stencil=${bestHDrive.opStencil} budget=${bestHDrive.opBudgetK} rho=${bestHDrive.rhoHMean.toFixed(3)}`,
  );
}
if (bestCohNull) {
  console.log(
    `BEST_COH null: metaLayers=${bestCohNull.metaLayers} stencil=${bestCohNull.opStencil} budget=${bestCohNull.opBudgetK} rho=${bestCohNull.rhoCohMean.toFixed(3)}`,
  );
}
if (bestCohDrive) {
  console.log(
    `BEST_COH drive: metaLayers=${bestCohDrive.metaLayers} stencil=${bestCohDrive.opStencil} budget=${bestCohDrive.opBudgetK} rho=${bestCohDrive.rhoCohMean.toFixed(3)}`,
  );
}

console.log("metaLayers=2 excluded from rho ranking");
const signalFound = summaryRows.some((row) => row.signalR2 || row.signalH || row.signalCoh);
if (!signalFound) {
  console.log("NO_HIERARCHY_SIGNAL_FOUND");
}
console.log(
  `opk hierarchy search complete: ${summaryRows.filter((r) => r.note !== "SUMMARY_MEAN_STD_RHO").length} configs`,
);
</file>

<file path="scripts/run-opk-motif-language-compare.mjs">
#!/usr/bin/env node
import { spawnSync } from "node:child_process";
import path from "node:path";

const rootDir = path.resolve(path.dirname(new URL(import.meta.url).pathname), "..");
const runner = path.resolve(rootDir, "scripts/run-opk-motif-language.mjs");
const baseOut = path.resolve(rootDir, ".tmp/op_motifs");

const common = [
  "--set",
  "gridSize=32",
  "--set",
  "metaLayers=2",
  "--set",
  "opCouplingOn=1",
  "--set",
  "sCouplingMode=1",
  "--set",
  "opStencil=1",
  "--set",
  "opBudgetK=16",
  "--set",
  "pWrite=0",
  "--set",
  "pNWrite=0",
  "--set",
  "pAWrite=0",
  "--set",
  "pSWrite=0.5",
  "--set",
  "initRandom=1",
];

const regimes = [
  {
    id: "R0_null",
    sets: [
      "--set",
      "p6On=0",
      "--set",
      "p3On=0",
      "--set",
      "eta=0",
      "--set",
      "etaDrive=0",
      "--set",
      "opDriveOnK=0",
    ],
  },
  {
    id: "R1_drive_noK",
    sets: [
      "--set",
      "p6On=1",
      "--set",
      "muHigh=1.0",
      "--set",
      "muLow=1.0",
      "--set",
      "eta=0",
      "--set",
      "etaDrive=0.8",
      "--set",
      "opDriveOnK=0",
    ],
  },
  {
    id: "R2_drive_withK",
    sets: [
      "--set",
      "p6On=1",
      "--set",
      "muHigh=1.0",
      "--set",
      "muLow=1.0",
      "--set",
      "eta=0",
      "--set",
      "etaDrive=0.8",
      "--set",
      "opDriveOnK=1",
    ],
  },
];

for (const regime of regimes) {
  const outDir = path.join(baseOut, regime.id);
  const args = [
    runner,
    "--outDir",
    outDir,
    "--seed",
    "1",
    "--burnIn",
    "200000",
    "--stepsMain",
    "1000000",
    ...common,
    ...regime.sets,
  ];
  const result = spawnSync("node", args, { stdio: "inherit" });
  if (result.status !== 0) {
    process.exit(result.status ?? 1);
  }
}
</file>

<file path="scripts/run-opk-motif-language.mjs">
#!/usr/bin/env node
import fs from "node:fs";
import path from "node:path";
import { fileURLToPath, pathToFileURL } from "node:url";
import {
  motifKeyFromTokens,
  motifFeatures,
  buildTopVocab,
  asymmetryScore,
  coarseEPFromCounts,
} from "./opk-motif-utils.mjs";
import { parseOpOffsets } from "./opk-metrics.mjs";

const __filename = fileURLToPath(import.meta.url);
const __dirname = path.dirname(__filename);
const rootDir = path.resolve(__dirname, "..");

const wasmDir = path.resolve(rootDir, "apps/web/src/wasm/sim_core");
const wasmJs = pathToFileURL(path.join(wasmDir, "sim_core.js")).href;
const wasmBytes = fs.readFileSync(path.join(wasmDir, "sim_core_bg.wasm"));
const mod = await import(wasmJs);
mod.initSync({ module: wasmBytes });

function ensureDir(dir) {
  fs.mkdirSync(dir, { recursive: true });
}

function parseArgs(argv) {
  const out = {
    outDir: path.resolve(rootDir, ".tmp", "op_motifs"),
    seed: 1,
    stepsMain: 1_000_000,
    burnIn: 200_000,
    topN: 50,
    sampleEvery: null,
    paramsPath: null,
    sets: [],
  };
  for (let i = 2; i < argv.length; i += 1) {
    const arg = argv[i];
    if (arg === "--outDir") out.outDir = path.resolve(argv[++i]);
    else if (arg === "--seed") out.seed = Number(argv[++i]);
    else if (arg === "--stepsMain") out.stepsMain = Number(argv[++i]);
    else if (arg === "--burnIn") out.burnIn = Number(argv[++i]);
    else if (arg === "--topN") out.topN = Number(argv[++i]);
    else if (arg === "--sampleEvery") out.sampleEvery = Number(argv[++i]);
    else if (arg === "--params") out.paramsPath = argv[++i];
    else if (arg === "--set") out.sets.push(argv[++i] ?? "");
  }
  return out;
}

function readJson(filePath) {
  return JSON.parse(fs.readFileSync(filePath, "utf8"));
}

function applySets(target, sets) {
  for (const item of sets) {
    if (!item) continue;
    const [key, raw] = item.split("=");
    if (!key) continue;
    const val = Number(raw);
    if (!Number.isFinite(val)) continue;
    target[key] = val;
  }
}

function mean(values) {
  return values.reduce((acc, v) => acc + v, 0) / values.length;
}

function entropyFromCounts(counts) {
  const total = Array.from(counts.values()).reduce((acc, v) => acc + v, 0);
  if (total <= 0) return 0;
  let h = 0;
  for (const v of counts.values()) {
    if (v <= 0) continue;
    const p = v / total;
    h += -p * Math.log(p);
  }
  return h;
}

function topMass(counts, topN) {
  const entries = Array.from(counts.values()).sort((a, b) => b - a);
  const total = entries.reduce((acc, v) => acc + v, 0);
  const top = entries.slice(0, topN).reduce((acc, v) => acc + v, 0);
  return total > 0 ? top / total : 0;
}

function computeKeys(tokens, rCount, layers, cells) {
  const keys = new Array(layers);
  for (let iface = 0; iface < layers; iface += 1) {
    const arr = new Array(cells);
    for (let q = 0; q < cells; q += 1) {
      const idx = iface * cells + q;
      arr[q] = motifKeyFromTokens(tokens, idx, rCount);
    }
    keys[iface] = arr;
  }
  return keys;
}

function updateCounts(countMap, key) {
  countMap.set(key, (countMap.get(key) ?? 0) + 1);
}

function updateTransition(countMap, fromKey, toKey) {
  const key = `${fromKey}|${toKey}`;
  countMap.set(key, (countMap.get(key) ?? 0) + 1);
}

const args = parseArgs(process.argv);
ensureDir(args.outDir);

const baseParams = {
  gridSize: 32,
  metaLayers: 2,
  opCouplingOn: 1,
  sCouplingMode: 1,
  opStencil: 1,
  opBudgetK: 16,
  pWrite: 0,
  pNWrite: 0,
  pAWrite: 0,
  pSWrite: 0.5,
  eta: 0.0,
  etaDrive: 0.0,
  p3On: 0,
  p6On: 0,
  opDriveOnK: 0,
  initRandom: 1,
};

const params = args.paramsPath ? { ...baseParams, ...readJson(args.paramsPath) } : { ...baseParams };
applySets(params, args.sets);

const sim = new mod.Sim(200, args.seed);
sim.set_params(params);
sim.step(args.burnIn);

const layers = sim.meta_layers ? sim.meta_layers() : params.metaLayers;
const gridSize = params.gridSize ?? 32;
const cells = gridSize * gridSize;
const tokens = sim.op_k_tokens();
const rCount = sim.op_r_count();
const offsets = parseOpOffsets(sim.op_offsets());

let prevKeys = computeKeys(tokens, rCount, layers, cells);
let epByMove = sim.ep_exact_by_move();
let lastEpOpK = epByMove[9] ?? 0;

const candidateDeltas = [1000, 5000, 20000];
const calibration = [];
let sampleEvery = args.sampleEvery;
if (!sampleEvery) {
  let chosen = null;
  for (const cand of candidateDeltas) {
    sim.step(cand);
    const currentKeys = computeKeys(sim.op_k_tokens(), rCount, layers, cells);
    let changed = 0;
    const total = layers * cells;
    for (let iface = 0; iface < layers; iface += 1) {
      for (let q = 0; q < cells; q += 1) {
        if (currentKeys[iface][q] !== prevKeys[iface][q]) changed += 1;
      }
    }
    const frac = total > 0 ? changed / total : 0;
    calibration.push({ candidate: cand, changeFrac: frac });
    prevKeys = currentKeys;
    if (frac >= 0.02 && frac <= 0.1 && !chosen) {
      chosen = cand;
      break;
    }
  }
  if (!chosen) {
    calibration.sort((a, b) => Math.abs(a.changeFrac - 0.05) - Math.abs(b.changeFrac - 0.05));
    chosen = calibration[0]?.candidate ?? candidateDeltas[0];
  }
  sampleEvery = chosen;
}

const perIfaceCounts = Array.from({ length: layers }, () => new Map());
const perIfaceTrans = Array.from({ length: layers }, () => new Map());
const perIfaceEp = Array.from({ length: layers }, () => new Map());
const pooledCounts = new Map();
const pooledTrans = new Map();
const pooledEp = new Map();

const timeseries = [];
let stepsRemaining = args.stepsMain;
let stepsElapsed = 0;

while (stepsRemaining > 0) {
  const step = Math.min(sampleEvery, stepsRemaining);
  sim.step(step);
  stepsElapsed += step;
  stepsRemaining -= step;

  const currentKeys = computeKeys(sim.op_k_tokens(), rCount, layers, cells);
  let changed = 0;
  const transInterval = new Map();

  for (let iface = 0; iface < layers; iface += 1) {
    for (let q = 0; q < cells; q += 1) {
      const fromKey = prevKeys[iface][q];
      const toKey = currentKeys[iface][q];
      updateCounts(perIfaceCounts[iface], toKey);
      updateCounts(pooledCounts, toKey);
      if (fromKey !== toKey) {
        changed += 1;
        updateTransition(perIfaceTrans[iface], fromKey, toKey);
        updateTransition(pooledTrans, fromKey, toKey);
        updateTransition(transInterval, fromKey, toKey);
      }
    }
  }

  const totalCells = layers * cells;
  const changeFrac = totalCells > 0 ? changed / totalCells : 0;
  epByMove = sim.ep_exact_by_move();
  const epOpK = epByMove[9] ?? 0;
  const epDelta = epOpK - lastEpOpK;
  lastEpOpK = epOpK;
  const epPerChange = changed > 0 ? epDelta / changed : 0;

  if (changed > 0) {
    for (const [key, count] of transInterval.entries()) {
      const epShare = (epDelta * count) / changed;
      updateTransition(pooledEp, key, epShare);
      const [fromKey] = key.split("|");
      const ifaceIdx = perIfaceCounts.findIndex((counts) => counts.has(fromKey));
      if (ifaceIdx >= 0) {
        updateTransition(perIfaceEp[ifaceIdx], key, epShare);
      }
    }
  }

  timeseries.push({
    step: stepsElapsed,
    changeFrac,
    epOpKDelta: epDelta,
    epPerChange,
  });
  prevKeys = currentKeys;
}

const vocabSummary = [];
const topMotifs = [];

const topMasses = [10, 20, 50];
const summaryFor = (label, counts) => {
  const h = entropyFromCounts(counts);
  const vEff = Math.exp(h);
  const total = Array.from(counts.values()).reduce((acc, v) => acc + v, 0);
  const row = {
    scope: label,
    U_exact: counts.size,
    H_vocab: h,
    V_eff: vEff,
    totalCount: total,
  };
  for (const n of topMasses) {
    row[`topMass${n}`] = topMass(counts, n);
  }
  return row;
};

vocabSummary.push(summaryFor("pooled", pooledCounts));
for (let iface = 0; iface < layers; iface += 1) {
  vocabSummary.push(summaryFor(`iface_${iface}`, perIfaceCounts[iface]));
}

const pooledEntries = Array.from(pooledCounts.entries()).sort((a, b) => b[1] - a[1]);
for (const [key, count] of pooledEntries.slice(0, 10)) {
  const tokensVec = key.split(",").map((v) => Number(v));
  const feats = motifFeatures(tokensVec, offsets);
  topMotifs.push({ key, count, frac: count / pooledEntries.reduce((acc, [, v]) => acc + v, 0), feats });
}

const { vocabKeys, keyToId, OTHER_ID } = buildTopVocab(pooledCounts, args.topN);

const aggregateCounts = new Map();
const aggregateEp = new Map();
for (const [key, count] of pooledTrans.entries()) {
  const [fromKey, toKey] = key.split("|");
  const fromId = keyToId.get(fromKey) ?? OTHER_ID;
  const toId = keyToId.get(toKey) ?? OTHER_ID;
  const aggKey = `${fromId}|${toId}`;
  aggregateCounts.set(aggKey, (aggregateCounts.get(aggKey) ?? 0) + count);
}
for (const [key, ep] of pooledEp.entries()) {
  const [fromKey, toKey] = key.split("|");
  const fromId = keyToId.get(fromKey) ?? OTHER_ID;
  const toId = keyToId.get(toKey) ?? OTHER_ID;
  const aggKey = `${fromId}|${toId}`;
  aggregateEp.set(aggKey, (aggregateEp.get(aggKey) ?? 0) + ep);
}

const asym = asymmetryScore(aggregateCounts);
const coarseEP = coarseEPFromCounts(aggregateCounts, 1e-12);

const vocabPath = path.join(args.outDir, "vocab_summary.csv");
const vocabHeader = [
  "scope",
  "U_exact",
  "H_vocab",
  "V_eff",
  "topMass10",
  "topMass20",
  "topMass50",
  "totalCount",
];
const vocabLines = [
  vocabHeader.join(","),
  ...vocabSummary.map((row) =>
    [
      row.scope,
      row.U_exact,
      row.H_vocab,
      row.V_eff,
      row.topMass10,
      row.topMass20,
      row.topMass50,
      row.totalCount,
    ].join(","),
  ),
];
fs.writeFileSync(vocabPath, `${vocabLines.join("\n")}\n`);

const transPath = path.join(args.outDir, "transitions_top.csv");
const transHeader = ["fromId", "toId", "count", "countRev"];
const transLines = [transHeader.join(",")];
const transEntries = Array.from(aggregateCounts.entries()).sort((a, b) => b[1] - a[1]);
for (const [key, count] of transEntries) {
  const [fromId, toId] = key.split("|");
  const revKey = `${toId}|${fromId}`;
  const rev = aggregateCounts.get(revKey) ?? 0;
  transLines.push([fromId, toId, count, rev].join(","));
}
fs.writeFileSync(transPath, `${transLines.join("\n")}\n`);

const epOut = new Map();
const outCounts = new Map();
for (const [key, count] of aggregateCounts.entries()) {
  const [fromId] = key.split("|");
  outCounts.set(fromId, (outCounts.get(fromId) ?? 0) + count);
}
for (const [key, ep] of aggregateEp.entries()) {
  const [fromId] = key.split("|");
  epOut.set(fromId, (epOut.get(fromId) ?? 0) + ep);
}

const epPath = path.join(args.outDir, "motif_ep.csv");
const epHeader = ["motifId", "key", "outCount", "epOut", "epPerOutTransition"];
const epLines = [epHeader.join(",")];
for (const [key, id] of keyToId.entries()) {
  const out = outCounts.get(String(id)) ?? 0;
  const ep = epOut.get(String(id)) ?? 0;
  const per = out > 0 ? ep / out : 0;
  epLines.push([id, key, out, ep, per].join(","));
}
const otherOut = outCounts.get(String(OTHER_ID)) ?? 0;
const otherEp = epOut.get(String(OTHER_ID)) ?? 0;
epLines.push([OTHER_ID, "OTHER", otherOut, otherEp, otherOut > 0 ? otherEp / otherOut : 0].join(","));
fs.writeFileSync(epPath, `${epLines.join("\n")}\n`);

const tsPath = path.join(args.outDir, "timeseries.csv");
const tsHeader = ["step", "changeFrac", "epOpKDelta", "epPerChange"];
const tsLines = [tsHeader.join(",")];
for (const row of timeseries) {
  tsLines.push([row.step, row.changeFrac, row.epOpKDelta, row.epPerChange].join(","));
}
fs.writeFileSync(tsPath, `${tsLines.join("\n")}\n`);

const summary = {
  params,
  seed: args.seed,
  burnIn: args.burnIn,
  stepsMain: args.stepsMain,
  sampleEvery,
  calibration,
  topMotifs,
  asymmetry: asym,
  coarseEP,
  epOpKTotal: lastEpOpK,
};
fs.writeFileSync(path.join(args.outDir, "summary.json"), JSON.stringify(summary, null, 2));
</file>

<file path="scripts/run-sweep.mjs">
#!/usr/bin/env node
import fs from "node:fs";
import path from "node:path";
import { fileURLToPath } from "node:url";
import { spawnSync } from "node:child_process";

const __filename = fileURLToPath(import.meta.url);
const __dirname = path.dirname(__filename);
const rootDir = path.resolve(__dirname, "..");
const logsDir = path.resolve(rootDir, ".tmp", "experiments");

const seeds = [1, 2, 3];
const runs = [
  {
    id: "base_null_balanced",
    steps: 5_000_000,
    reportEvery: 1_000_000,
    params: "scripts/params/base_null_balanced.json",
  },
  {
    id: "base_p6_drive",
    steps: 3_000_000,
    reportEvery: 1_000_000,
    params: "scripts/params/base_p6_drive.json",
  },
  {
    id: "base_p3_pump_minimal",
    steps: 1_000_000,
    reportEvery: 200_000,
    params: "scripts/params/base_p3_pump_minimal.json",
  },
  {
    id: "base_p3p6_combo_minimal",
    steps: 2_000_000,
    reportEvery: 500_000,
    params: "scripts/params/base_p3p6_combo_minimal.json",
  },
];

function ensureDir(dir) {
  fs.mkdirSync(dir, { recursive: true });
}

function parseNumber(value) {
  const n = Number(value);
  return Number.isFinite(n) ? n : null;
}

function parseLine(line, metrics, loopSamples) {
  if (line.startsWith("P1 steps")) {
    const jw = line.match(/Jw\s+([-\d.]+)/);
    const aw = line.match(/Aw\s+([-\d.]+)/);
    const mem = line.match(/mem\s+([-\d.]+)/);
    if (jw) metrics.jw = parseNumber(jw[1]);
    if (aw) metrics.aw = parseNumber(aw[1]);
    if (mem) metrics.sigmaMem = parseNumber(mem[1]);
  } else if (line.startsWith("P2 steps")) {
    const ja = line.match(/Ja\s+([-\d.]+)/);
    const aa = line.match(/Aa\s+([-\d.]+)/);
    if (ja) metrics.ja = parseNumber(ja[1]);
    if (aa) metrics.aa = parseNumber(aa[1]);
  } else if (line.startsWith("P4 steps")) {
    const jn = line.match(/Jn\s+([-\d.]+)/);
    const an = line.match(/An\s+([-\d.]+)/);
    if (jn) metrics.jn = parseNumber(jn[1]);
    if (an) metrics.an = parseNumber(an[1]);
  } else if (line.startsWith("P5 steps")) {
    const js = line.match(/Js\s+([-\d.]+)/);
    const as = line.match(/As\s+([-\d.]+)/);
    if (js) metrics.js = parseNumber(js[1]);
    if (as) metrics.as = parseNumber(as[1]);
  } else if (line.startsWith("P3 cycle")) {
    const disp = line.match(/disp\s+([-\d.]+)/);
    const loop = line.match(/loop\s+([-\d.]+)/);
    if (disp) metrics.p3Disp = parseNumber(disp[1]);
    if (loop) metrics.p3Loop = parseNumber(loop[1]);
    if (loop && disp) {
      const loopVal = Number(loop[1]);
      const dispVal = Number(disp[1]);
      if (Number.isFinite(loopVal) && Number.isFinite(dispVal)) {
        loopSamples.push({ loop: loopVal, disp: dispVal });
      }
    }
  } else if (line.startsWith("P6 M6")) {
    const m6 = line.match(/W\s+([-\d.]+)\s+N\s+([-\d.]+)\s+A\s+([-\d.]+)\s+S\s+([-\d.]+)/);
    if (m6) {
      metrics.m6w = parseNumber(m6[1]);
      metrics.m6n = parseNumber(m6[2]);
      metrics.m6a = parseNumber(m6[3]);
      metrics.m6s = parseNumber(m6[4]);
    }
  } else if (line.startsWith("Graph edges")) {
    const edges = line.match(/Graph edges\s+(\d+)/);
    const comps = line.match(/components\s+(\d+)/);
    const largest = line.match(/largest\s+(\d+)\/(\d+)/);
    if (edges) metrics.edges = parseNumber(edges[1]);
    if (comps) metrics.components = parseNumber(comps[1]);
    if (largest) {
      metrics.largest = parseNumber(largest[1]);
      metrics.graphN = parseNumber(largest[2]);
    }
  }
}

function parseOutput(output) {
  const lines = output.split("\n");
  const metrics = {};
  const loopSamples = [];
  const blocks = [];
  let currentBlock = null;
  for (const raw of lines) {
    const line = raw.trim();
    if (!line) {
      if (currentBlock) {
        blocks.push(currentBlock);
        currentBlock = null;
      }
      continue;
    }
    if (line.startsWith("E=")) {
      if (currentBlock) blocks.push(currentBlock);
      currentBlock = [line];
    } else if (currentBlock) {
      currentBlock.push(line);
    }
    parseLine(line, metrics, loopSamples);
  }
  if (currentBlock) blocks.push(currentBlock);

  const loopAbsMean =
    loopSamples.length === 0
      ? null
      : loopSamples.reduce((acc, v) => acc + Math.abs(v.loop), 0) / loopSamples.length;
  const dispAbsMean =
    loopSamples.length === 0
      ? null
      : loopSamples.reduce((acc, v) => acc + Math.abs(v.disp), 0) / loopSamples.length;
  const loopNonzero =
    loopSamples.length === 0
      ? null
      : loopSamples.filter((v) => Math.abs(v.loop) > 1e-6).length / loopSamples.length;

  return { metrics, loopAbsMean, dispAbsMean, loopNonzero, blocks };
}

function meanStd(values) {
  const nums = values.filter((v) => Number.isFinite(v));
  if (nums.length === 0) return { mean: null, std: null };
  const mean = nums.reduce((acc, v) => acc + v, 0) / nums.length;
  const variance = nums.reduce((acc, v) => acc + (v - mean) ** 2, 0) / nums.length;
  return { mean, std: Math.sqrt(variance) };
}

function fmt(value, digits = 4) {
  if (value === null || value === undefined || !Number.isFinite(value)) return "n/a";
  return value.toFixed(digits);
}

function runCommand(args, logPath) {
  const result = spawnSync(process.execPath, args, { encoding: "utf8", cwd: rootDir });
  if (logPath) {
    fs.writeFileSync(logPath, (result.stdout ?? "") + (result.stderr ?? ""));
  }
  if (result.status !== 0) {
    console.error(result.stderr || result.stdout || "Command failed.");
    process.exit(result.status ?? 1);
  }
  return result.stdout ?? "";
}

ensureDir(logsDir);

const summaries = [];
for (const run of runs) {
  const seedResults = [];
  for (const seed of seeds) {
    const logPath = path.join(logsDir, `${run.id}-seed${seed}.log`);
    const args = [
      path.join("scripts", "ratchet-cli.mjs"),
      "run",
      "--steps",
      String(run.steps),
      "--report-every",
      String(run.reportEvery),
      "--params",
      run.params,
      "--seed",
      String(seed),
      "--set",
      "metaLayers=0",
      "--set",
      "eta=0",
    ];
    const output = runCommand(args, logPath);
    const parsed = parseOutput(output);
    seedResults.push({
      seed,
      metrics: parsed.metrics,
      loopAbsMean: parsed.loopAbsMean,
      dispAbsMean: parsed.dispAbsMean,
      loopNonzero: parsed.loopNonzero,
      blocks: parsed.blocks,
    });
  }

  const metricsKeys = [
    "sigmaMem",
    "aw",
    "aa",
    "an",
    "as",
    "m6w",
    "m6n",
    "m6a",
    "m6s",
    "edges",
    "largest",
  ];
  const summary = { id: run.id };
  for (const key of metricsKeys) {
    const values = seedResults.map((s) => s.metrics[key]).filter((v) => Number.isFinite(v));
    const stats = meanStd(values);
    summary[key] = stats;
  }
  const loopMean = meanStd(seedResults.map((s) => s.loopAbsMean));
  const loopFrac = meanStd(seedResults.map((s) => s.loopNonzero));
  summary.loopAbs = loopMean;
  summary.loopNonzero = loopFrac;
  summary.seeds = seedResults;
  summaries.push(summary);
}

console.log("Summary table (mean ± std):");
console.log(
  [
    "preset",
    "SigmaMem",
    "Aw",
    "Aa",
    "An",
    "As",
    "M6W",
    "M6N",
    "M6A",
    "M6S",
    "loop|mean|",
    "loop>0 frac",
    "edges",
    "largest",
  ].join("\t"),
);
for (const s of summaries) {
  console.log(
    [
      s.id,
      `${fmt(s.sigmaMem.mean)}±${fmt(s.sigmaMem.std)}`,
      `${fmt(s.aw.mean)}±${fmt(s.aw.std)}`,
      `${fmt(s.aa.mean)}±${fmt(s.aa.std)}`,
      `${fmt(s.an.mean)}±${fmt(s.an.std)}`,
      `${fmt(s.as.mean)}±${fmt(s.as.std)}`,
      `${fmt(s.m6w.mean)}±${fmt(s.m6w.std)}`,
      `${fmt(s.m6n.mean)}±${fmt(s.m6n.std)}`,
      `${fmt(s.m6a.mean)}±${fmt(s.m6a.std)}`,
      `${fmt(s.m6s.mean)}±${fmt(s.m6s.std)}`,
      `${fmt(s.loopAbs.mean)}±${fmt(s.loopAbs.std)}`,
      `${fmt(s.loopNonzero.mean)}±${fmt(s.loopNonzero.std)}`,
      `${fmt(s.edges.mean, 0)}±${fmt(s.edges.std, 0)}`,
      `${fmt(s.largest.mean, 0)}±${fmt(s.largest.std, 0)}`,
    ].join("\t"),
  );
}
</file>

<file path="scripts/test-clock-deadline-traversal.mjs">
#!/usr/bin/env node
import assert from "node:assert/strict";
import fs from "node:fs";
import path from "node:path";
import { fileURLToPath, pathToFileURL } from "node:url";

const __filename = fileURLToPath(import.meta.url);
const __dirname = path.dirname(__filename);
const rootDir = path.resolve(__dirname, "..");
const outDir = path.resolve(rootDir, ".tmp", "clock_code");

const wasmDir = path.resolve(rootDir, "apps/web/src/wasm/sim_core");
const wasmJs = pathToFileURL(path.join(wasmDir, "sim_core.js")).href;
const wasmBytes = fs.readFileSync(path.join(wasmDir, "sim_core_bg.wasm"));
const mod = await import(wasmJs);
mod.initSync({ module: wasmBytes });

const seeds = [1, 2, 3, 4, 5];
const steps = 1_000_000;
const reportEvery = 5_000;
const perturbStep = 200_000;
const deadline = 25_000;

function ensureDir(dir) {
  fs.mkdirSync(dir, { recursive: true });
}

function mean(values) {
  return values.reduce((acc, v) => acc + v, 0) / values.length;
}

function percentile(values, p) {
  const nums = values.filter((v) => Number.isFinite(v)).sort((a, b) => a - b);
  if (nums.length === 0) return null;
  const idx = Math.min(nums.length - 1, Math.floor(p * (nums.length - 1)));
  return nums[idx];
}

function meanAbsDiff(a, b) {
  let sum = 0;
  for (let i = 0; i < a.length; i += 1) {
    sum += Math.abs(a[i] - b[i]);
  }
  return a.length === 0 ? 0 : sum / a.length;
}

function meanAbsDiffQuadrant(a, b, g, quadrant) {
  let sum = 0;
  let count = 0;
  for (let i = 0; i < a.length; i += 1) {
    const x = i % g;
    const y = Math.floor(i / g);
    const qx = x < g / 2 ? 0 : 1;
    const qy = y < g / 2 ? 0 : 1;
    const q = qy * 2 + qx;
    if (q !== quadrant) continue;
    sum += Math.abs(a[i] - b[i]);
    count += 1;
  }
  return count === 0 ? 0 : sum / count;
}

function quadrantIndex(idx, g) {
  const x = idx % g;
  const y = Math.floor(idx / g);
  const qx = x < g / 2 ? 0 : 1;
  const qy = y < g / 2 ? 0 : 1;
  return qy * 2 + qx;
}

function logicalBitsFromField(field, g, lS, mask) {
  const sums = [0, 0, 0, 0];
  const counts = [0, 0, 0, 0];
  for (let i = 0; i < field.length; i += 1) {
    if (mask && !mask[i]) continue;
    const q = quadrantIndex(i, g);
    sums[q] += field[i];
    counts[q] += 1;
  }
  const threshold = lS / 2;
  return sums.map((sum, i) => {
    const meanVal = counts[i] > 0 ? sum / counts[i] : 0;
    return meanVal >= threshold ? 1 : 0;
  });
}

function errorRate(bitsA, bitsB) {
  let mismatches = 0;
  for (let i = 0; i < bitsA.length; i += 1) {
    if (bitsA[i] !== bitsB[i]) mismatches += 1;
  }
  return mismatches / bitsA.length;
}

function makeMask(seed, size, frac) {
  let x = seed >>> 0;
  if (x === 0) x = 1;
  const mask = new Array(size);
  for (let i = 0; i < size; i += 1) {
    x ^= x << 13;
    x ^= x >>> 17;
    x ^= x << 5;
    const r = (x >>> 8) / (1 << 24);
    mask[i] = r < frac;
  }
  return mask;
}

function errF05(baseBits, metaField, g, lS, seed) {
  const trials = 20;
  let acc = 0;
  for (let t = 0; t < trials; t += 1) {
    const mask = makeMask(seed + t * 101, metaField.length, 0.5);
    const bits = logicalBitsFromField(metaField, g, lS, mask);
    acc += errorRate(bits, baseBits);
  }
  return acc / trials;
}

const baseParams = {
  beta: 2.0,
  stepSize: 0.01,
  p3On: 0,
  p6On: 1,
  p6SFactor: 0.0,
  pWrite: 0,
  pNWrite: 0.01,
  pAWrite: 0,
  pSWrite: 0.99,
  muHigh: 1.0,
  muLow: 1.0,
  kappaRep: 1.0,
  r0: 0.25,
  kappaBond: 0.0,
  rStar: 0.22,
  lambdaW: 0.0,
  lW: 4,
  lambdaN: 0.0,
  lN: 6,
  lambdaA: 0.0,
  lA: 6,
  lambdaS: 0.0,
  lS: 20,
  gridSize: 16,
  rPropose: 0.12,
  metaLayers: 2,
  eta: 0.0,
  etaDrive: 1.0,
  codeNoiseRate: 0.02,
  codeNoiseBatch: 2,
  codeNoiseLayer: 0,
  clockK: 8,
  clockFrac: 1.0,
  repairClockGated: 1,
  repairGateMode: 1,
  repairGateSpan: 1,
};

function runPreset(id, overrides) {
  const params = { ...baseParams, ...overrides };
  const runs = [];
  for (const seed of seeds) {
    const sim = new mod.Sim(50, seed);
    sim.set_params(params);
    let baselineTarget = null;
    let recoverySteps = null;
    let perturbApplied = false;

    for (let step = reportEvery; step <= steps; step += reportEvery) {
      sim.step(reportEvery);
      const baseS = sim.base_s_field();
      const metaS = sim.meta_field();
      const cells = baseS.length;
      const meta0 = metaS.subarray(0, cells);
      const sdiffBase = meanAbsDiffQuadrant(baseS, meta0, params.gridSize, 2);
      if (!perturbApplied) {
        if (step >= perturbStep) {
          baselineTarget = 4.0;
          sim.apply_perturbation({
            target: "metaS",
            layer: 0,
            frac: 1.0,
            mode: "randomize",
            seed: seed * 1000 + 13,
            region: "quadrant",
            quadrant: 2,
          });
          perturbApplied = true;
        }
      } else if (recoverySteps === null && baselineTarget !== null) {
        if (sdiffBase <= baselineTarget * 1.1) {
          recoverySteps = step - perturbStep;
        }
      }
    }

    const baseS = sim.base_s_field();
    const metaS = sim.meta_field();
    const cells = baseS.length;
    const meta0 = metaS.subarray(0, cells);
    const lS = params.lS ?? 1;
    const baseBits = logicalBitsFromField(baseS, params.gridSize, lS);
    const err = errF05(baseBits, meta0, params.gridSize, lS, seed + 9000);
    const sdiffBase = meanAbsDiffQuadrant(baseS, meta0, params.gridSize, 2);
    const missDeadline = recoverySteps === null || recoverySteps > deadline;

    runs.push({
      id,
      seed,
      errF05: err,
      sdiffBase,
      recoverySteps: recoverySteps ?? Infinity,
      missDeadline,
    });
  }
  return runs;
}

ensureDir(outDir);

const presets = [
  { id: "drift", overrides: { clockOn: 1, clockUsesP6: 1 } },
  { id: "random", overrides: { clockOn: 1, clockUsesP6: 0 } },
  { id: "static", overrides: { clockOn: 0, clockUsesP6: 1 } },
];

const rawLines = [];
const summaries = [];

for (const preset of presets) {
  const runs = runPreset(preset.id, preset.overrides);
  for (const row of runs) rawLines.push(JSON.stringify(row));
  const err = runs.map((r) => r.errF05);
  const sdiff = runs.map((r) => r.sdiffBase);
  const recovery = runs.map((r) => r.recoverySteps);
  const missDeadline = runs.filter((r) => r.missDeadline).length;
  summaries.push({
    id: preset.id,
    errMean: mean(err),
    sdiffMean: mean(sdiff),
    recoveryMean: mean(recovery.filter((v) => Number.isFinite(v))),
    recoveryP95: percentile(recovery, 0.95),
    missDeadlineCount: missDeadline,
  });
}

const summaryPath = path.join(outDir, "clock_deadline_traversal_summary.csv");
const rawPath = path.join(outDir, "clock_deadline_traversal_raw.jsonl");
fs.writeFileSync(rawPath, rawLines.join("\n"));

const header = [
  "preset",
  "errF05Mean",
  "sdiffMean",
  "recoveryMean",
  "recoveryP95",
  "missDeadlineCount",
];
const csvLines = [
  header.join(","),
  ...summaries.map((s) =>
    [
      s.id,
      s.errMean,
      s.sdiffMean,
      Number.isFinite(s.recoveryMean) ? s.recoveryMean : "",
      s.recoveryP95 ?? "",
      s.missDeadlineCount,
    ].join(","),
  ),
];
fs.writeFileSync(summaryPath, csvLines.join("\n"));

const drift = summaries.find((s) => s.id === "drift");
const random = summaries.find((s) => s.id === "random");
const staticCtrl = summaries.find((s) => s.id === "static");

assert.ok(drift.missDeadlineCount <= random.missDeadlineCount);
assert.ok(random.missDeadlineCount <= staticCtrl.missDeadlineCount);

console.log("Clock deadline traversal summary:");
for (const row of summaries) {
  console.log(
    `${row.id} | err(f=0.5) ${row.errMean.toFixed(3)} | sdiff ${row.sdiffMean.toFixed(3)} | recovery mean ${row.recoveryMean.toFixed(1)} | miss deadline ${row.missDeadlineCount}/${seeds.length}`,
  );
}
</file>

<file path="scripts/test-ep-null-tight.mjs">
#!/usr/bin/env node
import assert from "node:assert/strict";
import fs from "node:fs";
import path from "node:path";
import { fileURLToPath, pathToFileURL } from "node:url";

const __filename = fileURLToPath(import.meta.url);
const __dirname = path.dirname(__filename);
const rootDir = path.resolve(__dirname, "..");
const outDir = path.resolve(rootDir, ".tmp", "clock_code");

const wasmDir = path.resolve(rootDir, "apps/web/src/wasm/sim_core");
const wasmJs = pathToFileURL(path.join(wasmDir, "sim_core.js")).href;
const wasmBytes = fs.readFileSync(path.join(wasmDir, "sim_core_bg.wasm"));
const mod = await import(wasmJs);
mod.initSync({ module: wasmBytes });

function ensureDir(dir) {
  fs.mkdirSync(dir, { recursive: true });
}

function readJson(filePath) {
  return JSON.parse(fs.readFileSync(filePath, "utf8"));
}

function mean(values) {
  return values.reduce((acc, v) => acc + v, 0) / values.length;
}

function std(values, meanVal) {
  const variance = values.reduce((acc, v) => acc + (v - meanVal) ** 2, 0) / values.length;
  return Math.sqrt(variance);
}

function ciHalfWidth(stdVal, n) {
  return 1.96 * stdVal / Math.sqrt(n);
}

function runCase({ id, paramsPath, steps, seeds, overrides }) {
  const baseParams = readJson(path.resolve(rootDir, paramsPath));
  const params = { ...baseParams, ...overrides };
  const reportEvery = 1_000_000;
  const results = [];
  for (const seed of seeds) {
    const sim = new mod.Sim(200, seed);
    sim.set_params(params);
    let lastEpExact = sim.ep_exact_total();
    let lastEpNaive = sim.ep_naive_total();
    let lastWindowExact = 0;
    let lastWindowNaive = 0;
    for (let t = reportEvery; t <= steps; t += reportEvery) {
      sim.step(reportEvery);
      const epExact = sim.ep_exact_total();
      const epNaive = sim.ep_naive_total();
      lastWindowExact = epExact - lastEpExact;
      lastWindowNaive = epNaive - lastEpNaive;
      lastEpExact = epExact;
      lastEpNaive = epNaive;
    }
    results.push({
      id,
      seed,
      steps,
      epExactRateWindow: lastWindowExact / reportEvery,
      epNaiveRateWindow: lastWindowNaive / reportEvery,
      epExactRateTotal: lastEpExact / steps,
      epNaiveRateTotal: lastEpNaive / steps,
    });
  }
  return results;
}

ensureDir(outDir);

const cases = [
  {
    id: "base_null",
    paramsPath: "scripts/params/base_null_balanced.json",
    steps: 10_000_000,
    seeds: [1, 2, 3, 4, 5],
    overrides: {
      p3On: 0,
      p6On: 0,
      metaLayers: 0,
      eta: 0,
      initRandom: 1,
    },
  },
  {
    id: "meta_null",
    paramsPath: "scripts/params/meta/meta2_null_coupled.json",
    steps: 10_000_000,
    seeds: [1, 2, 3, 4, 5],
    overrides: {
      p3On: 0,
      p6On: 0,
      initRandom: 1,
    },
  },
  {
    id: "p6_drive",
    paramsPath: "scripts/params/meta/meta2_p6_drive_coupled.json",
    steps: 2_000_000,
    seeds: [1, 2, 3],
    overrides: {
      p3On: 0,
      p6On: 1,
    },
  },
];

const rawLines = [];
const summaryRows = [];

for (const entry of cases) {
  const results = runCase(entry);
  for (const row of results) {
    rawLines.push(JSON.stringify(row));
  }

  const exactWindowRates = results.map((r) => r.epExactRateWindow);
  const naiveWindowRates = results.map((r) => r.epNaiveRateWindow);
  const meanExact = mean(exactWindowRates);
  const stdExact = std(exactWindowRates, meanExact);
  const ci = ciHalfWidth(stdExact, exactWindowRates.length);

  summaryRows.push({
    id: entry.id,
    meanExact,
    stdExact,
    ciHalfWidth: ci,
    meanNaive: mean(naiveWindowRates),
    stdNaive: std(naiveWindowRates, mean(naiveWindowRates)),
  });

  if (entry.id !== "p6_drive") {
    for (const rate of exactWindowRates) {
      assert.ok(Math.abs(rate) <= 2e-4);
    }
    assert.ok(meanExact - ci <= 0 && meanExact + ci >= 0);
    assert.ok(ci <= 2e-4);
  } else {
    for (const rate of exactWindowRates) {
      assert.ok(rate > 1e-4);
    }
  }
}

const rawPath = path.join(outDir, "ep_null_tight_raw.jsonl");
const summaryPath = path.join(outDir, "ep_null_tight_summary.csv");
fs.writeFileSync(rawPath, rawLines.join("\n"));

const header = [
  "case",
  "meanExactWindow",
  "stdExactWindow",
  "ciHalfWidth",
  "meanNaiveWindow",
  "stdNaiveWindow",
];
const csvLines = [
  header.join(","),
  ...summaryRows.map((row) =>
    [
      row.id,
      row.meanExact,
      row.stdExact,
      row.ciHalfWidth,
      row.meanNaive,
      row.stdNaive,
    ].join(","),
  ),
];
fs.writeFileSync(summaryPath, csvLines.join("\n"));

console.log("EP null tight summary:");
for (const row of summaryRows) {
  console.log(
    `${row.id} | exact window mean ${row.meanExact.toExponential(3)} ± ${row.ciHalfWidth.toExponential(2)} | naive mean ${row.meanNaive.toExponential(3)}`,
  );
}
</file>

<file path="scripts/test-opcoupling-effect.mjs">
#!/usr/bin/env node
import assert from "node:assert/strict";
import fs from "node:fs";
import path from "node:path";
import { fileURLToPath, pathToFileURL } from "node:url";

const __filename = fileURLToPath(import.meta.url);
const __dirname = path.dirname(__filename);
const rootDir = path.resolve(__dirname, "..");

const wasmDir = path.resolve(rootDir, "apps/web/src/wasm/sim_core");
const wasmJs = pathToFileURL(path.join(wasmDir, "sim_core.js")).href;
const wasmBytes = fs.readFileSync(path.join(wasmDir, "sim_core_bg.wasm"));
const mod = await import(wasmJs);
mod.initSync({ module: wasmBytes });

const OP_STENCIL_CROSS = [
  [0, 0],
  [1, 0],
  [-1, 0],
  [0, 1],
  [0, -1],
];
const OP_STENCIL_FULL = [
  [0, 0],
  [1, 0],
  [-1, 0],
  [0, 1],
  [0, -1],
  [1, 1],
  [-1, 1],
  [1, -1],
  [-1, -1],
];

function offsetIndex(q, dx, dy, g) {
  const x = q % g;
  const y = Math.floor(q / g);
  const nx = (x + dx + g) % g;
  const ny = (y + dy + g) % g;
  return ny * g + nx;
}

function opMismatchMean({
  baseS,
  metaS,
  opK,
  gridSize,
  metaLayers,
  rCount,
  budget,
  stencil,
  lS,
}) {
  const cells = gridSize * gridSize;
  const denom = Math.max(1, lS);
  let sum = 0;
  let count = 0;
  for (let iface = 0; iface < metaLayers; iface += 1) {
    for (let q = 0; q < cells; q += 1) {
      let pred = 0;
      const start = (iface * cells + q) * rCount;
      for (let r = 0; r < rCount; r += 1) {
        const [dx, dy] = stencil[r];
        const qOff = offsetIndex(q, dx, dy, gridSize);
        const lower = iface === 0
          ? baseS[qOff]
          : metaS[(iface - 1) * cells + qOff];
        pred += (opK[start + r] / budget) * (lower / denom);
      }
      const upper = metaS[iface * cells + q] / denom;
      sum += Math.abs(upper - pred);
      count += 1;
    }
  }
  return count > 0 ? sum / count : 0;
}

function runCase(eta) {
  const params = {
    beta: 12.0,
    gridSize: 16,
    metaLayers: 2,
    lS: 10,
    lambdaS: 0,
    pSWrite: 1,
    pWrite: 0,
    pNWrite: 0,
    pAWrite: 0,
    p3On: 0,
    p6On: 0,
    eta,
    etaDrive: 0,
    opCouplingOn: 1,
    sCouplingMode: 1,
    opBudgetK: 16,
    opStencil: 1,
  };
  const sim = new mod.Sim(200, 1);
  sim.set_params(params);
  sim.step(3_000_000);
  const baseS = sim.base_s_field();
  const metaS = sim.meta_field();
  const opK = sim.op_k_tokens();
  const rCount = sim.op_r_count();
  const budget = sim.op_budget_k();
  const stencil = params.opStencil === 1 ? OP_STENCIL_FULL : OP_STENCIL_CROSS;
  const mismatch = opMismatchMean({
    baseS,
    metaS,
    opK,
    gridSize: params.gridSize,
    metaLayers: params.metaLayers,
    rCount,
    budget,
    stencil,
    lS: params.lS,
  });
  return mismatch;
}

const diff0 = runCase(0.0);
const diff1 = runCase(0.6);
const ratio = diff1 / diff0;

console.log(
  `Sdiff_op eta0=${diff0.toFixed(6)} eta0.6=${diff1.toFixed(6)} ratio=${ratio.toFixed(4)}`,
);

assert.ok(diff1 <= 0.85 * diff0);
</file>

<file path="scripts/test-opcoupling-invariants.mjs">
#!/usr/bin/env node
import assert from "node:assert/strict";
import fs from "node:fs";
import path from "node:path";
import { fileURLToPath, pathToFileURL } from "node:url";

const __filename = fileURLToPath(import.meta.url);
const __dirname = path.dirname(__filename);
const rootDir = path.resolve(__dirname, "..");

const wasmDir = path.resolve(rootDir, "apps/web/src/wasm/sim_core");
const wasmJs = pathToFileURL(path.join(wasmDir, "sim_core.js")).href;
const wasmBytes = fs.readFileSync(path.join(wasmDir, "sim_core_bg.wasm"));
const mod = await import(wasmJs);
mod.initSync({ module: wasmBytes });

function makeLCG(seed) {
  let state = seed >>> 0;
  return () => {
    state = (Math.imul(1664525, state) + 1013904223) >>> 0;
    return state;
  };
}

const params = {
  gridSize: 16,
  metaLayers: 2,
  opCouplingOn: 1,
  sCouplingMode: 1,
  opBudgetK: 16,
  opStencil: 1,
  p3On: 0,
  p6On: 0,
  pWrite: 0,
  pNWrite: 0,
  pAWrite: 0,
  pSWrite: 1,
};

const sim = new mod.Sim(200, 1);
sim.set_params(params);
sim.step(200_000);

const rCount = sim.op_r_count();
const budget = sim.op_budget_k();
const interfaces = sim.op_interfaces();
const opK = sim.op_k_tokens();
const g = params.gridSize;
const cells = g * g;

assert.ok(rCount > 0, "op_r_count should be > 0");
assert.equal(opK.length, interfaces * cells * rCount);

let min = Number.POSITIVE_INFINITY;
let max = Number.NEGATIVE_INFINITY;
for (let i = 0; i < opK.length; i += 1) {
  const v = opK[i];
  if (v < min) min = v;
  if (v > max) max = v;
}
assert.ok(min >= 0);
assert.ok(max <= budget);

const rng = makeLCG(0x1234abcd);
const samples = 200;
for (let i = 0; i < samples; i += 1) {
  const iface = rng() % interfaces;
  const q = rng() % cells;
  const start = (iface * cells + q) * rCount;
  let sum = 0;
  for (let r = 0; r < rCount; r += 1) {
    sum += opK[start + r];
  }
  assert.equal(sum, budget, "budget sum mismatch");
}

console.log(
  `OK opK invariants: interfaces=${interfaces} cells=${cells} rCount=${rCount} budget=${budget} min=${min} max=${max}`,
);
</file>

<file path="scripts/test-opcoupling-null-ep.mjs">
#!/usr/bin/env node
import assert from "node:assert/strict";
import fs from "node:fs";
import path from "node:path";
import { fileURLToPath, pathToFileURL } from "node:url";

const __filename = fileURLToPath(import.meta.url);
const __dirname = path.dirname(__filename);
const rootDir = path.resolve(__dirname, "..");
const outDir = path.resolve(rootDir, ".tmp", "clock_code");

const wasmDir = path.resolve(rootDir, "apps/web/src/wasm/sim_core");
const wasmJs = pathToFileURL(path.join(wasmDir, "sim_core.js")).href;
const wasmBytes = fs.readFileSync(path.join(wasmDir, "sim_core_bg.wasm"));
const mod = await import(wasmJs);
mod.initSync({ module: wasmBytes });

function ensureDir(dir) {
  fs.mkdirSync(dir, { recursive: true });
}

function readJson(filePath) {
  return JSON.parse(fs.readFileSync(filePath, "utf8"));
}

function mean(values) {
  return values.reduce((acc, v) => acc + v, 0) / values.length;
}

function std(values, meanVal) {
  const variance = values.reduce((acc, v) => acc + (v - meanVal) ** 2, 0) / values.length;
  return Math.sqrt(variance);
}

function ciHalfWidth(stdVal, n) {
  const t95 = n > 1 && n < 30 ? 2.776 : 1.96;
  return t95 * stdVal / Math.sqrt(n);
}

function opKChanged(opK, interfaces, cells, rCount, budget) {
  if (opK.length === 0 || rCount === 0) return false;
  const base = Math.floor(budget / rCount);
  const rem = budget % rCount;
  for (let iface = 0; iface < interfaces; iface += 1) {
    for (let q = 0; q < cells; q += 1) {
      const start = (iface * cells + q) * rCount;
      for (let r = 0; r < rCount; r += 1) {
        const expected = r < rem ? base + 1 : base;
        if (opK[start + r] !== expected) {
          return true;
        }
      }
    }
  }
  return false;
}

function runCase({ id, paramsPath, steps, seeds, overrides }) {
  const baseParams = readJson(path.resolve(rootDir, paramsPath));
  const params = { ...baseParams, ...overrides };
  const reportEvery = 1_000_000;
  const results = [];
  for (const seed of seeds) {
    const sim = new mod.Sim(200, seed);
    sim.set_params(params);
    let lastEpExact = sim.ep_exact_total();
    let lastEpNaive = sim.ep_naive_total();
    let lastWindowExact = 0;
    let lastWindowNaive = 0;
    for (let t = reportEvery; t <= steps; t += reportEvery) {
      sim.step(reportEvery);
      const epExact = sim.ep_exact_total();
      const epNaive = sim.ep_naive_total();
      lastWindowExact = epExact - lastEpExact;
      lastWindowNaive = epNaive - lastEpNaive;
      lastEpExact = epExact;
      lastEpNaive = epNaive;
    }
    const opK = sim.op_k_tokens();
    const rCount = sim.op_r_count();
    const budget = sim.op_budget_k();
    const interfaces = sim.op_interfaces();
    const g = params.gridSize ?? 16;
    const cells = g * g;
    const kChanged = opKChanged(opK, interfaces, cells, rCount, budget);
    results.push({
      id,
      seed,
      steps,
      epExactRateWindow: lastWindowExact / reportEvery,
      epNaiveRateWindow: lastWindowNaive / reportEvery,
      epExactRateTotal: lastEpExact / steps,
      epNaiveRateTotal: lastEpNaive / steps,
      kChanged,
    });
  }
  return results;
}

ensureDir(outDir);

const cases = [
  {
    id: "op_null_eta",
    paramsPath: "scripts/params/meta/meta2_null_coupled.json",
    steps: 10_000_000,
    seeds: [1, 2, 3, 4, 5],
    overrides: {
      p3On: 0,
      p6On: 0,
      initRandom: 1,
      opCouplingOn: 1,
      sCouplingMode: 1,
      opBudgetK: 16,
      opStencil: 1,
      eta: 0.6,
      etaDrive: 0,
    },
  },
  {
    id: "op_null_eta0",
    paramsPath: "scripts/params/meta/meta2_null_coupled.json",
    steps: 10_000_000,
    seeds: [1, 2, 3, 4, 5],
    overrides: {
      p3On: 0,
      p6On: 0,
      initRandom: 1,
      opCouplingOn: 1,
      sCouplingMode: 1,
      opBudgetK: 16,
      opStencil: 1,
      eta: 0.0,
      etaDrive: 0,
    },
  },
];

const rawLines = [];
const summaryRows = [];
const caseResults = [];

for (const entry of cases) {
  const results = runCase(entry);
  caseResults.push({ entry, results });
  for (const row of results) {
    rawLines.push(JSON.stringify(row));
  }

  const exactWindowRates = results.map((r) => r.epExactRateWindow);
  const naiveWindowRates = results.map((r) => r.epNaiveRateWindow);
  const meanExact = mean(exactWindowRates);
  const stdExact = std(exactWindowRates, meanExact);
  const ci = ciHalfWidth(stdExact, exactWindowRates.length);
  const changedCount = results.filter((r) => r.kChanged).length;

  summaryRows.push({
    id: entry.id,
    meanExact,
    stdExact,
    ciHalfWidth: ci,
    meanNaive: mean(naiveWindowRates),
    stdNaive: std(naiveWindowRates, mean(naiveWindowRates)),
    kChanged: changedCount,
  });
}

const summaryCsv = [
  "id,meanExact,stdExact,ciHalfWidth,meanNaive,stdNaive,kChanged",
  ...summaryRows.map(
    (row) =>
      `${row.id},${row.meanExact},${row.stdExact},${row.ciHalfWidth},${row.meanNaive},${row.stdNaive},${row.kChanged}`,
  ),
].join("\n");

fs.writeFileSync(path.join(outDir, "op_null_ep_raw.jsonl"), rawLines.join("\n"));
fs.writeFileSync(path.join(outDir, "op_null_ep_summary.csv"), `${summaryCsv}\n`);

console.log("op coupling null EP summary:");
console.log(summaryCsv);

for (const { entry, results } of caseResults) {
  const exactWindowRates = results.map((r) => r.epExactRateWindow);
  const summary = summaryRows.find((row) => row.id === entry.id);
  for (const rate of exactWindowRates) {
    assert.ok(Math.abs(rate) <= 2e-4);
  }
  assert.ok(summary.meanExact - summary.ciHalfWidth <= 0 && summary.meanExact + summary.ciHalfWidth >= 0);
  assert.equal(summary.kChanged, results.length, "opK remained at initialization");
}
</file>

<file path="scripts/test-opk-drive-selection.mjs">
#!/usr/bin/env node
import assert from "node:assert/strict";
import fs from "node:fs";
import path from "node:path";
import { fileURLToPath, pathToFileURL } from "node:url";
import { computeOpkMetrics } from "./opk-metrics.mjs";

const __filename = fileURLToPath(import.meta.url);
const __dirname = path.dirname(__filename);
const rootDir = path.resolve(__dirname, "..");
const outDir = path.resolve(rootDir, ".tmp", "op_coupling");

const wasmDir = path.resolve(rootDir, "apps/web/src/wasm/sim_core");
const wasmJs = pathToFileURL(path.join(wasmDir, "sim_core.js")).href;
const wasmBytes = fs.readFileSync(path.join(wasmDir, "sim_core_bg.wasm"));
const mod = await import(wasmJs);
mod.initSync({ module: wasmBytes });

function ensureDir(dir) {
  fs.mkdirSync(dir, { recursive: true });
}

function mean(values) {
  return values.reduce((acc, v) => acc + v, 0) / values.length;
}

function std(values, meanVal) {
  const variance = values.reduce((acc, v) => acc + (v - meanVal) ** 2, 0) / values.length;
  return Math.sqrt(variance);
}

const seeds = [1, 2, 3, 4, 5];
const stepsCompare = 1_000_000;
const reportEvery = 100_000;
const extraStepsNull = 2_000_000;

function snapshotMetrics(sim, params) {
  const baseS = sim.base_s_field();
  const metaS = sim.meta_field();
  const opK = sim.op_k_tokens();
  const opOffsets = sim.op_offsets();
  const rCount = sim.op_r_count();
  const budget = sim.op_budget_k();
  return computeOpkMetrics({
    gridSize: params.gridSize,
    metaLayers: params.metaLayers,
    rCount,
    opBudgetK: budget,
    opOffsets,
    opKTokens: opK,
    baseS,
    metaS,
    lS: params.lS,
  });
}

function runVariant(id, overrides, extraSteps = 0) {
  const params = {
    beta: 5.0,
    gridSize: 16,
    metaLayers: 2,
    lS: 10,
    lambdaS: 0.0,
    p3On: 0,
    p6On: 0,
    p6SFactor: 0.0,
    pWrite: 0,
    pNWrite: 0,
    pAWrite: 0,
    pSWrite: 0.01,
    eta: 0,
    etaDrive: 0,
    muHigh: 1.0,
    muLow: 1.0,
    opCouplingOn: 1,
    sCouplingMode: 1,
    opStencil: 1,
    opBudgetK: 16,
    opDriveOnK: 1,
    initRandom: 1,
    repairClockGated: 1,
    clockOn: 0,
    clockK: 8,
    clockFrac: 0,
    ...overrides,
  };
  const rows = [];
  for (const seed of seeds) {
    const sim = new mod.Sim(200, seed);
    sim.set_params(params);
    const startMetrics = snapshotMetrics(sim, params);
    let lastEpExact = sim.ep_exact_total();
    let lastEpOpK = sim.ep_exact_by_move()[9] ?? 0;
    let lastWindowExact = 0;
    let lastWindowOpK = 0;
    for (let t = reportEvery; t <= stepsCompare; t += reportEvery) {
      sim.step(reportEvery);
      const epExact = sim.ep_exact_total();
      const epOpK = sim.ep_exact_by_move()[9] ?? 0;
      lastWindowExact = epExact - lastEpExact;
      lastWindowOpK = epOpK - lastEpOpK;
      lastEpExact = epExact;
      lastEpOpK = epOpK;
    }
    const endMetrics = snapshotMetrics(sim, params);
    if (extraSteps > 0) {
      const extraChunks = Math.ceil(extraSteps / reportEvery);
      for (let i = 0; i < extraChunks; i += 1) {
        sim.step(reportEvery);
        const epExact = sim.ep_exact_total();
        const epOpK = sim.ep_exact_by_move()[9] ?? 0;
        lastWindowExact = epExact - lastEpExact;
        lastWindowOpK = epOpK - lastEpOpK;
        lastEpExact = epExact;
        lastEpOpK = epOpK;
      }
    }
    rows.push({
      id,
      seed,
      sdiffStart: startMetrics.summary.sdiffMean,
      sdiffEnd: endMetrics.summary.sdiffMean,
      hEnd: endMetrics.summary.hMean,
      r2End: endMetrics.summary.r2Mean,
      epExactRateWindowLast: lastWindowExact / reportEvery,
      epOpKRateWindowLast: lastWindowOpK / reportEvery,
    });
  }
  return { params, rows };
}

ensureDir(outDir);
const variants = [
  {
    id: "A_drive_selects",
    overrides: { p6On: 1, etaDrive: 1.0, opDriveOnK: 1, muHigh: 10.0, muLow: 10.0 },
    extraSteps: 0,
  },
  {
    id: "B_drive_no_k",
    overrides: { p6On: 1, etaDrive: 1.0, opDriveOnK: 0, muHigh: 10.0, muLow: 10.0 },
    extraSteps: 0,
  },
  {
    id: "C_null",
    overrides: { p6On: 0, etaDrive: 0.0, opDriveOnK: 0, eta: 0.0 },
    extraSteps: extraStepsNull,
  },
  {
    id: "D_equilibrium",
    overrides: { p6On: 0, etaDrive: 0.0, opDriveOnK: 0, eta: 0.6 },
    extraSteps: extraStepsNull,
  },
];

const rawRows = [];
const summaryRows = [];

for (const variant of variants) {
  const { rows } = runVariant(variant.id, variant.overrides, variant.extraSteps);
  for (const row of rows) {
    rawRows.push(JSON.stringify(row));
  }

  const sdiffEnd = rows.map((r) => r.sdiffEnd);
  const sdiffStart = rows.map((r) => r.sdiffStart);
  const deltaSdiff = rows.map((r) => r.sdiffEnd - r.sdiffStart);
  const epExactWindow = rows.map((r) => r.epExactRateWindowLast);
  const epOpKWindow = rows.map((r) => r.epOpKRateWindowLast);

  summaryRows.push({
    id: variant.id,
    sdiffStartMean: mean(sdiffStart),
    sdiffEndMean: mean(sdiffEnd),
    deltaSdiffMean: mean(deltaSdiff),
    epExactWindowMean: mean(epExactWindow),
    epOpKWindowMean: mean(epOpKWindow),
    sdiffEndStd: std(sdiffEnd, mean(sdiffEnd)),
  });
}

const rawPath = path.join(outDir, "opk_drive_selection_raw.jsonl");
const summaryPath = path.join(outDir, "opk_drive_selection_summary.csv");
fs.writeFileSync(rawPath, rawRows.join("\n"));

const header = [
  "id",
  "sdiffStartMean",
  "sdiffEndMean",
  "deltaSdiffMean",
  "sdiffEndStd",
  "epExactWindowMean",
  "epOpKWindowMean",
];
const lines = [
  header.join(","),
  ...summaryRows.map((row) =>
    [
      row.id,
      row.sdiffStartMean,
      row.sdiffEndMean,
      row.deltaSdiffMean,
      row.sdiffEndStd,
      row.epExactWindowMean,
      row.epOpKWindowMean,
    ].join(","),
  ),
].join("\n");
fs.writeFileSync(summaryPath, `${lines}\n`);

const rowA = summaryRows.find((r) => r.id === "A_drive_selects");
const rowB = summaryRows.find((r) => r.id === "B_drive_no_k");
const rowC = summaryRows.find((r) => r.id === "C_null");

console.log("opk drive selection summary:");
console.log(lines);

assert.ok(rowA.sdiffEndMean <= 0.85 * rowB.sdiffEndMean);
assert.ok(rowA.sdiffEndMean <= 0.85 * rowC.sdiffEndMean);
assert.ok(rowA.epOpKWindowMean > 1e-4);
assert.ok(rowA.epOpKWindowMean >= 2 * rowB.epOpKWindowMean);
assert.ok(Math.abs(rowC.epExactWindowMean) <= 2e-4);
assert.ok(Math.abs(rowC.deltaSdiffMean) <= 0.05 * rowC.sdiffStartMean);
</file>

<file path="scripts/test-opk-null-ep-weights.mjs">
#!/usr/bin/env node
import assert from "node:assert/strict";
import fs from "node:fs";
import path from "node:path";
import { fileURLToPath, pathToFileURL } from "node:url";

const __filename = fileURLToPath(import.meta.url);
const __dirname = path.dirname(__filename);
const rootDir = path.resolve(__dirname, "..");
const outDir = path.resolve(rootDir, ".tmp", "op_coupling");

const wasmDir = path.resolve(rootDir, "apps/web/src/wasm/sim_core");
const wasmJs = pathToFileURL(path.join(wasmDir, "sim_core.js")).href;
const wasmBytes = fs.readFileSync(path.join(wasmDir, "sim_core_bg.wasm"));
const mod = await import(wasmJs);
mod.initSync({ module: wasmBytes });

function ensureDir(dir) {
  fs.mkdirSync(dir, { recursive: true });
}

ensureDir(outDir);

const weights = [0.0, 0.25, 0.5, 1.0, 2.0];
const seeds = [1, 2, 3];
const steps = 3_000_000;
const reportEvery = 1_000_000;

const rows = [];
const summary = [];

for (const weight of weights) {
  const rates = [];
  for (const seed of seeds) {
    const sim = new mod.Sim(200, seed);
    sim.set_params({
      gridSize: 16,
      metaLayers: 2,
      opCouplingOn: 1,
      sCouplingMode: 1,
      opKTargetWeight: weight,
      p3On: 0,
      p6On: 0,
      eta: 0.6,
      etaDrive: 0,
      pWrite: 0,
      pNWrite: 0,
      pAWrite: 0,
      pSWrite: 1,
      initRandom: 1,
    });
    let lastEp = sim.ep_exact_total();
    let lastWindow = 0;
    for (let t = reportEvery; t <= steps; t += reportEvery) {
      sim.step(reportEvery);
      const ep = sim.ep_exact_total();
      lastWindow = ep - lastEp;
      lastEp = ep;
    }
    const rate = lastWindow / reportEvery;
    rates.push(rate);
    rows.push({
      weight,
      seed,
      epExactRateWindowLast: rate,
      pass: Math.abs(rate) <= 2e-4,
    });
    assert.ok(Math.abs(rate) <= 2e-4);
  }
  const mean = rates.reduce((acc, v) => acc + v, 0) / rates.length;
  const variance = rates.reduce((acc, v) => acc + (v - mean) ** 2, 0) / rates.length;
  const std = Math.sqrt(variance);
  summary.push({ weight, mean, std });
}

const summaryPath = path.join(outDir, "opk_null_ep_weights_summary.csv");
const lines = [
  "weight,seed,epExactRateWindowLast,pass",
  ...rows.map((row) =>
    [row.weight, row.seed, row.epExactRateWindowLast, row.pass].join(","),
  ),
  "weight,seed,mean,std",
  ...summary.map((row) => [row.weight, "mean", row.mean, row.std].join(",")),
];
fs.writeFileSync(summaryPath, `${lines.join("\n")}\n`);

console.log("opK null EP weights summary:");
for (const row of summary) {
  console.log(`weight ${row.weight} mean ${row.mean.toExponential(3)} std ${row.std.toExponential(2)}`);
}
</file>

<file path="scripts/test-step2-meta-layers.mjs">
#!/usr/bin/env node
import assert from "node:assert/strict";
import fs from "node:fs";
import path from "node:path";
import { fileURLToPath, pathToFileURL } from "node:url";

const __filename = fileURLToPath(import.meta.url);
const __dirname = path.dirname(__filename);
const rootDir = path.resolve(__dirname, "..");

const wasmDir = path.resolve(rootDir, "apps/web/src/wasm/sim_core");
const wasmJs = pathToFileURL(path.join(wasmDir, "sim_core.js")).href;
const wasmBytes = fs.readFileSync(path.join(wasmDir, "sim_core_bg.wasm"));
const mod = await import(wasmJs);
mod.initSync({ module: wasmBytes });

function countNonZero(arr) {
  let count = 0;
  for (const v of arr) {
    if (v !== 0) count += 1;
  }
  return count;
}

function minMaxI16(arr) {
  let min = 0;
  let max = 0;
  let init = false;
  for (const v of arr) {
    if (!init) {
      min = v;
      max = v;
      init = true;
      continue;
    }
    if (v < min) min = v;
    if (v > max) max = v;
  }
  return { min, max };
}

function maxU16(arr) {
  let max = 0;
  for (const v of arr) {
    if (v > max) max = v;
  }
  return max;
}

function maxU8(arr) {
  let max = 0;
  for (const v of arr) {
    if (v > max) max = v;
  }
  return max;
}

function newSim() {
  return new mod.Sim(50, 1);
}

function logCase(label, detail) {
  console.log(`${label}: ${detail}`);
}

const gridSize = 16;
const metaLayers = 2;
const edgeCount = 2 * gridSize * gridSize;

// Case 0: sizing invariants
{
  const sim = newSim();
  sim.set_params({ gridSize, metaLayers });
  const metaField = sim.meta_field();
  const metaN = sim.meta_n_field();
  const metaA = sim.meta_a_field();
  const metaW = sim.meta_w_edges();
  assert.equal(metaField.length, metaLayers * gridSize * gridSize);
  assert.equal(metaN.length, metaLayers * gridSize * gridSize);
  assert.equal(metaA.length, metaLayers * gridSize * gridSize);
  assert.equal(metaW.length, metaLayers * edgeCount);
  logCase(
    "Case 0 sizes",
    `metaField=${metaField.length} metaN=${metaN.length} metaA=${metaA.length} metaW=${metaW.length}`,
  );
}

// Case 1: P5 updates meta_field
{
  const sim = newSim();
  sim.set_params({
    gridSize,
    metaLayers,
    pSWrite: 1,
    pWrite: 0,
    pNWrite: 0,
    pAWrite: 0,
    p3On: 0,
    p6On: 0,
    lambdaS: 0,
    lS: 6,
  });
  sim.step(2000);
  const metaField = sim.meta_field();
  const nonZero = countNonZero(metaField);
  const max = maxU8(metaField);
  assert.ok(nonZero > 0);
  logCase("Case 1 metaField", `nonZero=${nonZero} max=${max}`);
}

{
  const sim = newSim();
  sim.set_params({
    gridSize,
    metaLayers,
    pSWrite: 0,
    pWrite: 0,
    pNWrite: 0,
    pAWrite: 0,
    p3On: 0,
    p6On: 0,
    lambdaS: 0,
    lS: 6,
  });
  sim.step(1000);
  const metaField = sim.meta_field();
  const nonZero = countNonZero(metaField);
  assert.equal(nonZero, 0);
  logCase("Case 1b metaField", `nonZero=${nonZero}`);
}

// Case 2: P4 updates meta_n_field
{
  const sim = newSim();
  const lN = 6;
  sim.set_params({
    gridSize,
    metaLayers,
    pSWrite: 0,
    pWrite: 0,
    pNWrite: 1,
    pAWrite: 0,
    p3On: 0,
    p6On: 0,
    lambdaN: 0,
    lN,
  });
  sim.step(2000);
  const metaN = sim.meta_n_field();
  const nonZero = countNonZero(metaN);
  const { min, max } = minMaxI16(metaN);
  assert.ok(nonZero > 0);
  assert.ok(min >= -lN);
  assert.ok(max <= lN);
  logCase("Case 2 metaN", `nonZero=${nonZero} min=${min} max=${max}`);
}

// Case 3: P2 updates meta_a_field
{
  const sim = newSim();
  const lA = 6;
  sim.set_params({
    gridSize,
    metaLayers,
    pSWrite: 0,
    pWrite: 0,
    pNWrite: 0,
    pAWrite: 1,
    p3On: 0,
    p6On: 0,
    lambdaA: 0,
    lA,
  });
  sim.step(2000);
  const metaA = sim.meta_a_field();
  const nonZero = countNonZero(metaA);
  const max = maxU16(metaA);
  assert.ok(nonZero > 0);
  assert.ok(max <= lA);
  logCase("Case 3 metaA", `nonZero=${nonZero} max=${max}`);
}

// Case 4: P1 updates meta_w_edges
{
  const sim = newSim();
  const lW = 6;
  sim.set_params({
    gridSize,
    metaLayers,
    pSWrite: 0,
    pWrite: 1,
    pNWrite: 0,
    pAWrite: 0,
    p3On: 0,
    p6On: 0,
    lambdaW: 0,
    kappaBond: 0,
    lW,
  });
  sim.step(2000);
  const metaW = sim.meta_w_edges();
  const nonZero = countNonZero(metaW);
  const max = maxU8(metaW);
  assert.ok(nonZero > 0);
  assert.ok(max <= lW);
  logCase("Case 4 metaW", `nonZero=${nonZero} max=${max}`);
}
</file>

<file path="scripts/test-step3-eta-coupling.mjs">
#!/usr/bin/env node
import assert from "node:assert/strict";
import fs from "node:fs";
import path from "node:path";
import { fileURLToPath, pathToFileURL } from "node:url";

const __filename = fileURLToPath(import.meta.url);
const __dirname = path.dirname(__filename);
const rootDir = path.resolve(__dirname, "..");

const wasmDir = path.resolve(rootDir, "apps/web/src/wasm/sim_core");
const wasmJs = pathToFileURL(path.join(wasmDir, "sim_core.js")).href;
const wasmBytes = fs.readFileSync(path.join(wasmDir, "sim_core_bg.wasm"));
const mod = await import(wasmJs);
mod.initSync({ module: wasmBytes });

function meanAbsDiff(a, b) {
  let sum = 0;
  const len = a.length;
  for (let i = 0; i < len; i += 1) {
    sum += Math.abs(a[i] - b[i]);
  }
  return sum / len;
}

function runSim(params, steps) {
  const sim = new mod.Sim(50, 1);
  sim.set_params(params);
  sim.step(steps);
  return sim;
}

function logCase(label, detail) {
  console.log(`${label}: ${detail}`);
}

const gridSize = 12;
const metaLayers = 2;
const cells = gridSize * gridSize;
const steps = 50000;

// Case A: S coupling reduces base/meta mismatch.
{
  const baseParams = {
    beta: 10.0,
    gridSize,
    metaLayers,
    lS: 10,
    pSWrite: 1,
    pWrite: 0,
    pNWrite: 0,
    pAWrite: 0,
    p3On: 0,
    p6On: 0,
    lambdaS: 0,
  };

  const sim0 = runSim({ ...baseParams, eta: 0.0 }, steps);
  const sim1 = runSim({ ...baseParams, eta: 1.0 }, steps);

  const base0 = sim0.base_s_field();
  const meta0 = sim0.meta_field();
  const base1 = sim1.base_s_field();
  const meta1 = sim1.meta_field();

  const meta0L0 = meta0.subarray(0, cells);
  const meta0L1 = meta0.subarray(cells, 2 * cells);
  const meta1L0 = meta1.subarray(0, cells);
  const meta1L1 = meta1.subarray(cells, 2 * cells);

  const diffBase0 = meanAbsDiff(base0, meta0L0);
  const diffBase1 = meanAbsDiff(base1, meta1L0);
  const diffMeta0 = meanAbsDiff(meta0L0, meta0L1);
  const diffMeta1 = meanAbsDiff(meta1L0, meta1L1);

  logCase(
    "Case A diff base/meta",
    `eta0=${diffBase0.toFixed(4)} eta1=${diffBase1.toFixed(4)}`,
  );
  logCase(
    "Case A diff meta/meta",
    `eta0=${diffMeta0.toFixed(4)} eta1=${diffMeta1.toFixed(4)}`,
  );

  assert.ok(diffBase1 <= 0.85 * diffBase0);
  assert.ok(diffMeta1 <= 0.85 * diffMeta0);
}

// Case B: W coupling reduces meta edge mismatch.
{
  const baseParams = {
    beta: 10.0,
    gridSize,
    metaLayers,
    lW: 10,
    pWrite: 1,
    pSWrite: 0,
    pNWrite: 0,
    pAWrite: 0,
    p3On: 0,
    p6On: 0,
    lambdaW: 0,
    kappaBond: 0,
  };

  const sim0 = runSim({ ...baseParams, eta: 0.0 }, steps);
  const sim1 = runSim({ ...baseParams, eta: 1.0 }, steps);

  const edges = 2 * gridSize * gridSize;
  const w0 = sim0.meta_w_edges();
  const w1 = sim1.meta_w_edges();

  const w0L0 = w0.subarray(0, edges);
  const w0L1 = w0.subarray(edges, 2 * edges);
  const w1L0 = w1.subarray(0, edges);
  const w1L1 = w1.subarray(edges, 2 * edges);

  const diff0 = meanAbsDiff(w0L0, w0L1);
  const diff1 = meanAbsDiff(w1L0, w1L1);

  logCase("Case B diff metaW", `eta0=${diff0.toFixed(4)} eta1=${diff1.toFixed(4)}`);

  assert.ok(diff1 <= 0.85 * diff0);
}
</file>

<file path=".gitignore">
node_modules/
dist/
.DS_Store
.tmp/

# Rust / WASM build outputs
target/
crates/**/pkg/
apps/web/src/wasm/sim_core/

# Logs
npm-debug.log*
yarn-debug.log*
yarn-error.log*
</file>

<file path=".repomixrc">
{
  "output": {
    "filePath": "repomix-output.md",
    "style": "markdown",
    "includeEmptyDirectories": false
  },
  "include": [
    "**/sim/**/*.ts",
    "**/sim/**/*.tsx",
    "**/sim-core/**/*.rs",
    "**/sim-core/**/Cargo.toml",
    "**/worker*.ts",
    "**/worker*.tsx",
    "**/scripts/**/*.mjs",
    "**/scripts/**/*.js",
    "**/package.json",
    "**/tsconfig.json",
    "**/.repomixrc",
    "**/*.config.js",
    "**/*.config.mjs",
    "**/*.md"
  ],
  "ignore": {
    "useGitignore": true,
    "useDefaultPatterns": true,
    "customPatterns": [
      "**/node_modules/**",
      "**/dist/**",
      "**/build/**",
      "**/target/**",
      "**/.turbo/**",
      "**/coverage/**",
      "**/*.tsbuildinfo",
      "**/package-lock.json",
      "**/yarn.lock",
      "**/pnpm-lock.yaml",
      "**/.next/**",
      "**/.vite/**",
      "**/out/**",
      "**/*.log",
      "**/.DS_Store",
      "**/Cargo.lock",
      "**/repomix-output.md",
      "**/*.wasm",
      "**/*.map",
      "**/results*.json",
      "**/experiments*.json",
      "**/output*.json",
      "**/data*.json",
      "**/*.css",
      "**/*.html",
      "**/App.tsx",
      "**/App.jsx",
      "**/components/**",
      "**/ui/**",
      "**/style*.ts",
      "**/style*.tsx",
      "**/*View.tsx",
      "**/*View.ts",
      "**/*Component.tsx",
      "**/*Component.ts"
    ]
  }
}
</file>

<file path="AGENTS.md">
# Ratchet Playground — Implementation Notes (Agent Guidance)

## Tech stack (record of choices)

- UI language/framework: TypeScript + React (Vite)
- Simulation core: Rust compiled to WebAssembly (wasm-bindgen)
- Concurrency model: WebWorker owns the simulation loop; UI receives snapshots/diagnostics
- Rendering: Canvas2D initially (optionally OffscreenCanvas in the worker); WebGL2 is a later optimization
- Diagnostics/plots: lightweight charting (uPlot or custom canvas plots)

## Design constraints from `/docs`

- Null regime (P3=OFF, P6=OFF) must satisfy detailed balance w.r.t. an explicit stationary measure.
- P3 and P6 are the only primitives allowed to break reversibility.
- Diagnostics must be descriptive (no implicit optimization/progress).

## Repo layout

- `apps/web`: Vite React UI + worker bridge
- `crates/sim-core`: Rust/WASM simulation core (no UI code)
- `docs`: local theory/spec reference (source of truth)

## Coding conventions

- Prefer small, composable modules and explicit types.
- Keep simulation state inside WASM/worker; avoid copying large arrays per frame.
- Make “null regime correctness” easy to test/inspect (energy breakdown + affinity ~ 0).
</file>

<file path="Cargo.toml">
[workspace]
resolver = "2"
members = [
  "crates/sim-core",
]
</file>

<file path="Makefile">
.PHONY: wasm web-install dev build

wasm:
	mkdir -p .tmp
	TMPDIR="$(PWD)/.tmp" CARGO_TARGET_DIR="$(PWD)/.tmp/cargo-target" \
		cd crates/sim-core && wasm-pack build --target web --out-dir ../../apps/web/src/wasm/sim_core --out-name sim_core

web-install:
	cd apps/web && npm install

dev: wasm web-install
	cd apps/web && npm run dev

build: wasm
	cd apps/web && npm run build
</file>

<file path="apps/web/src/sim/runCache.ts">
export type RunMeta = {
  runId: number;
  startedAt: number;
  n: number;
  seed: number;
  bondThreshold: number;
  params: unknown;
  captureEverySteps: number;
};

export type SnapshotEntry = {
  runId: number;
  index: number;
  t: number;
  n: number;
  energy: unknown;
  diagnostics: unknown;
  graphStats: unknown;
  positions: Float32Array;
  bonds: Uint32Array;
  counters: Int16Array;
  apparatus: Uint16Array;
  field: Uint8Array;
  stepsDelta: number;
};

const DB_NAME = "ratchet-run-cache";
const DB_VERSION = 1;
const STORE = "snapshots";
const META = "meta";

const FLUSH_EVERY = 50;
const MEMORY_KEEP = 200;

let runMeta: RunMeta | null = null;
let index = 0;
let stepAccumulator = 0;
let captureEverySteps = 2000;
let pending: SnapshotEntry[] = [];
let memory: SnapshotEntry[] = [];
let flushing = false;
let dbPromise: Promise<IDBDatabase> | null = null;

function openDb(): Promise<IDBDatabase> {
  if (dbPromise) return dbPromise;
  dbPromise = new Promise((resolve, reject) => {
    if (!("indexedDB" in window)) {
      reject(new Error("IndexedDB not available"));
      return;
    }
    const req = indexedDB.open(DB_NAME, DB_VERSION);
    req.onupgradeneeded = () => {
      const db = req.result;
      if (!db.objectStoreNames.contains(STORE)) {
        db.createObjectStore(STORE, { keyPath: "id", autoIncrement: true });
      }
      if (!db.objectStoreNames.contains(META)) {
        db.createObjectStore(META);
      }
    };
    req.onsuccess = () => resolve(req.result);
    req.onerror = () => reject(req.error ?? new Error("Failed to open IndexedDB"));
  });
  return dbPromise;
}

async function clearDb() {
  const db = await openDb();
  await new Promise<void>((resolve, reject) => {
    const tx = db.transaction([STORE, META], "readwrite");
    tx.objectStore(STORE).clear();
    tx.objectStore(META).clear();
    tx.oncomplete = () => resolve();
    tx.onerror = () => reject(tx.error ?? new Error("Failed to clear run cache"));
  });
}

async function writeMeta(meta: RunMeta) {
  const db = await openDb();
  await new Promise<void>((resolve, reject) => {
    const tx = db.transaction(META, "readwrite");
    tx.objectStore(META).put(meta, "current");
    tx.oncomplete = () => resolve();
    tx.onerror = () => reject(tx.error ?? new Error("Failed to write run meta"));
  });
}

async function flushPending(): Promise<void> {
  if (flushing || pending.length === 0) return;
  flushing = true;
  const batch = pending;
  pending = [];
  try {
    const db = await openDb();
    await new Promise<void>((resolve, reject) => {
      const tx = db.transaction(STORE, "readwrite");
      const store = tx.objectStore(STORE);
      for (const entry of batch) {
        store.add(entry);
      }
      tx.oncomplete = () => resolve();
      tx.onerror = () => reject(tx.error ?? new Error("Failed to flush run cache"));
    });
  } catch {
    pending = batch.concat(pending);
  } finally {
    flushing = false;
    if (pending.length >= FLUSH_EVERY) {
      await flushPending();
    }
  }
}

export async function startRun(meta: Omit<RunMeta, "runId" | "startedAt">) {
  runMeta = {
    runId: Date.now(),
    startedAt: Date.now(),
    ...meta,
  };
  index = 0;
  stepAccumulator = 0;
  pending = [];
  memory = [];
  try {
    await clearDb();
    await writeMeta(runMeta);
  } catch {
    // Fallback to in-memory only.
  }
}

export function addSnapshot(entry: Omit<SnapshotEntry, "runId" | "index" | "t">) {
  if (!runMeta) return;
  if (captureEverySteps <= 0) return;
  stepAccumulator += Math.max(0, entry.stepsDelta);
  if (stepAccumulator < captureEverySteps) return;
  stepAccumulator = stepAccumulator % captureEverySteps;

  const snapshot: SnapshotEntry = {
    runId: runMeta.runId,
    index,
    t: performance.now(),
    ...entry,
    positions: new Float32Array(entry.positions),
    bonds: new Uint32Array(entry.bonds),
    counters: new Int16Array(entry.counters),
    apparatus: new Uint16Array(entry.apparatus),
    field: new Uint8Array(entry.field),
  };
  index += 1;
  pending.push(snapshot);
  memory.push(snapshot);
  if (memory.length > MEMORY_KEEP) {
    memory.shift();
  }
  if (pending.length >= FLUSH_EVERY) {
    void flushPending();
  }
}

export function setCaptureEverySteps(steps: number) {
  const v = Math.max(1, Math.floor(steps));
  captureEverySteps = v;
  if (runMeta) {
    runMeta.captureEverySteps = v;
  }
}

export function getMemory(): SnapshotEntry[] {
  return memory.slice();
}

export function getMeta(): RunMeta | null {
  return runMeta;
}

export function attachToWindow() {
  (window as any).__ratchetRunCache = {
    getMeta,
    getMemory,
    exportRun,
  };
}

export async function exportRun() {
  try {
    await flushPending();
  } catch {
    // Ignore flush errors; we'll fall back to in-memory data.
  }

  let db: IDBDatabase | null = null;
  try {
    db = await openDb();
  } catch {
    db = null;
  }

  const meta = await new Promise<RunMeta | null>((resolve, reject) => {
    if (!db) {
      resolve(runMeta);
      return;
    }
    const tx = db.transaction(META, "readonly");
    const req = tx.objectStore(META).get("current");
    req.onsuccess = () => resolve((req.result as RunMeta) ?? null);
    req.onerror = () => reject(req.error ?? new Error("Failed to read run meta"));
  });

  const parts: BlobPart[] = [];
  if (meta) {
    parts.push(`${JSON.stringify({ type: "meta", ...meta })}\n`);
  }

  let wroteSnapshots = false;
  if (db) {
    await new Promise<void>((resolve, reject) => {
      const tx = db!.transaction(STORE, "readonly");
      const store = tx.objectStore(STORE);
      const req = store.openCursor();
      req.onsuccess = () => {
        const cursor = req.result;
        if (!cursor) {
          resolve();
          return;
        }
        const entry = cursor.value as SnapshotEntry;
        parts.push(`${JSON.stringify(serializeSnapshot(entry))}\n`);
        wroteSnapshots = true;
        cursor.continue();
      };
      req.onerror = () => reject(req.error ?? new Error("Failed to read run snapshots"));
    });
  }

  if (!wroteSnapshots) {
    const fallback = pending.concat(memory);
    for (const entry of fallback) {
      parts.push(`${JSON.stringify(serializeSnapshot(entry))}\n`);
    }
  }

  const blob = new Blob(parts, { type: "application/jsonl" });
  const url = URL.createObjectURL(blob);
  const a = document.createElement("a");
  const ts = meta?.startedAt ? new Date(meta.startedAt).toISOString().replace(/[:.]/g, "-") : "run";
  a.href = url;
  a.download = `ratchet-run-${ts}.jsonl`;
  a.click();
  URL.revokeObjectURL(url);
}

function serializeSnapshot(entry: SnapshotEntry) {
  return {
    type: "snapshot",
    runId: entry.runId,
    index: entry.index,
    t: entry.t,
    n: entry.n,
    energy: entry.energy,
    diagnostics: entry.diagnostics,
    graphStats: entry.graphStats,
    positions: Array.from(entry.positions),
    bonds: Array.from(entry.bonds),
    counters: Array.from(entry.counters),
    apparatus: Array.from(entry.apparatus),
    field: Array.from(entry.field),
    stepsDelta: entry.stepsDelta,
  };
}
</file>

<file path="apps/web/tsconfig.tsbuildinfo">
{"root":["./src/App.tsx","./src/main.tsx","./src/vite-env.d.ts","./src/sim/runCache.ts","./src/sim/sim.worker.ts","./src/sim/workerClient.ts","./src/sim/workerMessages.ts","./src/wasm/sim_core/sim_core.d.ts","./src/wasm/sim_core/sim_core_bg.wasm.d.ts"],"version":"5.9.3"}
</file>

<file path="scripts/params/clock_code/code_null.json">
{
  "beta": 2.0,
  "stepSize": 0.01,
  "p3On": 0,
  "p6On": 0,
  "pWrite": 0,
  "pNWrite": 0,
  "pAWrite": 0,
  "pSWrite": 1,
  "muHigh": 1.0,
  "muLow": 1.0,
  "kappaRep": 1.0,
  "r0": 0.25,
  "kappaBond": 0.0,
  "rStar": 0.22,
  "lambdaW": 0.0,
  "lW": 4,
  "lambdaN": 0.0,
  "lN": 6,
  "lambdaA": 0.0,
  "lA": 6,
  "lambdaS": 0.0,
  "lS": 20,
  "gridSize": 8,
  "rPropose": 0.12,
  "metaLayers": 2,
  "eta": 0.0,
  "etaDrive": 1.0,
  "clockOn": 0,
  "clockK": 8,
  "clockFrac": 0.0,
  "clockUsesP6": 1,
  "repairClockGated": 0
}
</file>

<file path="scripts/params/clock_code/code_p6_clock_gated_random.json">
{
  "beta": 2.0,
  "stepSize": 0.01,
  "p3On": 0,
  "p6On": 1,
  "p6SFactor": 0.0,
  "pWrite": 0,
  "pNWrite": 0.1,
  "pAWrite": 0,
  "pSWrite": 0.7,
  "muHigh": 1.0,
  "muLow": 1.0,
  "kappaRep": 1.0,
  "r0": 0.25,
  "kappaBond": 0.0,
  "rStar": 0.22,
  "lambdaW": 0.0,
  "lW": 4,
  "lambdaN": 0.0,
  "lN": 6,
  "lambdaA": 0.0,
  "lA": 6,
  "lambdaS": 0.5,
  "lS": 20,
  "gridSize": 8,
  "rPropose": 0.12,
  "metaLayers": 2,
  "eta": 0.0,
  "etaDrive": 1.0,
  "clockOn": 1,
  "clockK": 8,
  "clockFrac": 1.0,
  "clockUsesP6": 0,
  "repairClockGated": 1
}
</file>

<file path="scripts/params/clock_code/code_p6_clock_gated_static.json">
{
  "beta": 2.0,
  "stepSize": 0.01,
  "p3On": 0,
  "p6On": 1,
  "p6SFactor": 0.0,
  "pWrite": 0,
  "pNWrite": 0.1,
  "pAWrite": 0,
  "pSWrite": 0.7,
  "muHigh": 1.0,
  "muLow": 1.0,
  "kappaRep": 1.0,
  "r0": 0.25,
  "kappaBond": 0.0,
  "rStar": 0.22,
  "lambdaW": 0.0,
  "lW": 4,
  "lambdaN": 0.0,
  "lN": 6,
  "lambdaA": 0.0,
  "lA": 6,
  "lambdaS": 0.5,
  "lS": 20,
  "gridSize": 8,
  "rPropose": 0.12,
  "metaLayers": 2,
  "eta": 0.0,
  "etaDrive": 1.0,
  "clockOn": 0,
  "clockK": 8,
  "clockFrac": 1.0,
  "clockUsesP6": 1,
  "repairClockGated": 1
}
</file>

<file path="scripts/params/clock_code/code_p6_drive.json">
{
  "beta": 2.0,
  "stepSize": 0.01,
  "p3On": 0,
  "p6On": 1,
  "p6SFactor": 0.0,
  "pWrite": 0,
  "pNWrite": 0,
  "pAWrite": 0,
  "pSWrite": 1,
  "muHigh": 1.0,
  "muLow": 1.0,
  "kappaRep": 1.0,
  "r0": 0.25,
  "kappaBond": 0.0,
  "rStar": 0.22,
  "lambdaW": 0.0,
  "lW": 4,
  "lambdaN": 0.0,
  "lN": 6,
  "lambdaA": 0.0,
  "lA": 6,
  "lambdaS": 0.0,
  "lS": 20,
  "gridSize": 8,
  "rPropose": 0.12,
  "metaLayers": 2,
  "eta": 0.0,
  "etaDrive": 1.0,
  "clockOn": 0,
  "clockK": 8,
  "clockFrac": 0.0,
  "clockUsesP6": 1,
  "repairClockGated": 0
}
</file>

<file path="scripts/bootstrap.sh">
#!/usr/bin/env bash
set -euo pipefail

repo_root="$(cd -- "$(dirname -- "${BASH_SOURCE[0]}")/.." && pwd)"
cd "$repo_root"

have() { command -v "$1" >/dev/null 2>&1; }

echo "[ratchet-playground] Bootstrapping dev dependencies (WSL/Ubuntu)."

if ! have curl; then
  echo "Missing dependency: curl"
  echo "Install it with: sudo apt-get update && sudo apt-get install -y curl"
  exit 1
fi

if ! have rustup; then
  echo "Installing rustup..."
  curl --proto '=https' --tlsv1.2 -sSf https://sh.rustup.rs | sh -s -- -y
fi

if [[ -f "$HOME/.cargo/env" ]]; then
  # shellcheck disable=SC1091
  source "$HOME/.cargo/env"
fi

echo "Ensuring wasm32 target..."
rustup target add wasm32-unknown-unknown

if ! have wasm-pack; then
  echo "Installing wasm-pack..."
  cargo install wasm-pack
fi

if ! have wasm-bindgen; then
  echo "Installing wasm-bindgen-cli..."
  cargo install wasm-bindgen-cli
fi

if ! have nvm; then
  echo "Installing nvm..."
  curl -o- https://raw.githubusercontent.com/nvm-sh/nvm/v0.40.1/install.sh | bash
fi

export NVM_DIR="${NVM_DIR:-$HOME/.nvm}"
if [[ -s "$NVM_DIR/nvm.sh" ]]; then
  # shellcheck disable=SC1091
  source "$NVM_DIR/nvm.sh"
else
  echo "nvm installed but not found at $NVM_DIR/nvm.sh"
  echo "Open a new shell and re-run: bash scripts/bootstrap.sh"
  exit 1
fi

echo "Installing Node.js LTS..."
nvm install --lts

echo
echo "Done."
echo "Next: run 'make dev' from the repo root."
</file>

<file path="scripts/run-clock-tur-sweep.mjs">
#!/usr/bin/env node
import fs from "node:fs";
import path from "node:path";
import { fileURLToPath, pathToFileURL } from "node:url";

const __filename = fileURLToPath(import.meta.url);
const __dirname = path.dirname(__filename);
const rootDir = path.resolve(__dirname, "..");
const outDir = path.resolve(rootDir, ".tmp", "clock_code");

const wasmDir = path.resolve(rootDir, "apps/web/src/wasm/sim_core");
const wasmJs = pathToFileURL(path.join(wasmDir, "sim_core.js")).href;
const wasmBytes = fs.readFileSync(path.join(wasmDir, "sim_core_bg.wasm"));
const mod = await import(wasmJs);
mod.initSync({ module: wasmBytes });

const steps = 1_000_000;
const seeds = Array.from({ length: 10 }, (_, i) => i + 1);
const mus = [0.2, 0.4, 0.6, 0.8, 1.0, 1.4];

function ensureDir(dir) {
  fs.mkdirSync(dir, { recursive: true });
}

function mean(values) {
  return values.reduce((acc, v) => acc + v, 0) / values.length;
}

function variance(values, meanVal) {
  return values.reduce((acc, v) => acc + (v - meanVal) ** 2, 0) / values.length;
}

function median(values) {
  const sorted = [...values].sort((a, b) => a - b);
  const mid = Math.floor(sorted.length / 2);
  if (sorted.length % 2 === 0) {
    return 0.5 * (sorted[mid - 1] + sorted[mid]);
  }
  return sorted[mid];
}

function readJson(filePath) {
  return JSON.parse(fs.readFileSync(filePath, "utf8"));
}

ensureDir(outDir);

const baseParamsPath = path.resolve(rootDir, "scripts/params/clock_code/clock_tur_sweep_base.json");
const baseParams = readJson(baseParamsPath);

const rawPath = path.join(outDir, "clock_tur_raw.jsonl");
const summaryPath = path.join(outDir, "clock_tur_summary.csv");
const rawLines = [];
const summaryRows = [];

for (const mu of mus) {
  const qs = [];
  const sigmas = [];
  for (const seed of seeds) {
    const sim = new mod.Sim(50, seed);
    sim.set_params({ ...baseParams, muHigh: mu, muLow: mu });
    sim.step(steps);
    const q = Number(sim.clock_q());
    const sigma = sim.ep_exact_total();
    qs.push(q);
    sigmas.push(sigma);
    rawLines.push(
      JSON.stringify({
        mu,
        seed,
        steps,
        clockQ: q,
        epTotal: sigma,
      }),
    );
  }
  const meanQ = mean(qs);
  const varQ = variance(qs, meanQ);
  const meanSigma = mean(sigmas);
  const relVar = meanQ !== 0 ? varQ / (meanQ * meanQ) : Infinity;
  const R = relVar * meanSigma / 2;
  summaryRows.push({ mu, meanQ, varQ, meanSigma, R });
  console.log(
    `mu ${mu.toFixed(2)} | meanQ ${meanQ.toFixed(2)} | varQ ${varQ.toFixed(2)} | meanSigma ${meanSigma.toFixed(4)} | R ${R.toFixed(3)}`,
  );
}

fs.writeFileSync(rawPath, rawLines.join("\n"));
const header = "mu,meanQ,varQ,meanSigma,R";
const csvLines = [
  header,
  ...summaryRows.map((row) =>
    [row.mu, row.meanQ, row.varQ, row.meanSigma, row.R].join(","),
  ),
];
fs.writeFileSync(summaryPath, csvLines.join("\n"));

const qMono = summaryRows.reduce((acc, row, i) => {
  if (i === 0) return 0;
  return acc + (row.meanQ >= summaryRows[i - 1].meanQ ? 1 : 0);
}, 0);
const sigmaMono = summaryRows.reduce((acc, row, i) => {
  if (i === 0) return 0;
  return acc + (row.meanSigma >= summaryRows[i - 1].meanSigma ? 1 : 0);
}, 0);

if (qMono < 4) {
  throw new Error(`meanQ monotonicity failed (count=${qMono})`);
}
if (sigmaMono < 4) {
  throw new Error(`meanSigma monotonicity failed (count=${sigmaMono})`);
}

const Rs = summaryRows.map((row) => row.R);
const medR = median(Rs);
for (const row of summaryRows) {
  if (!(row.R >= 0.6)) {
    throw new Error(`TUR ratio too low for mu=${row.mu}: R=${row.R}`);
  }
}
if (!(medR >= 1.0)) {
  throw new Error(`Median TUR ratio too low: ${medR}`);
}

console.log(`TUR sweep complete. Summary saved to ${summaryPath}`);
</file>

<file path="scripts/test-clock-current.mjs">
#!/usr/bin/env node
import assert from "node:assert/strict";
import fs from "node:fs";
import path from "node:path";
import { fileURLToPath, pathToFileURL } from "node:url";

const __filename = fileURLToPath(import.meta.url);
const __dirname = path.dirname(__filename);
const rootDir = path.resolve(__dirname, "..");

const wasmDir = path.resolve(rootDir, "apps/web/src/wasm/sim_core");
const wasmJs = pathToFileURL(path.join(wasmDir, "sim_core.js")).href;
const wasmBytes = fs.readFileSync(path.join(wasmDir, "sim_core_bg.wasm"));
const mod = await import(wasmJs);
mod.initSync({ module: wasmBytes });

const seeds = [1, 2, 3, 4, 5];
const steps = 2_000_000;
const clockFrac = 0.01;

function runCase(label, params) {
  const rows = [];
  for (const seed of seeds) {
    const sim = new mod.Sim(50, seed);
    sim.set_params(params);
    sim.step(steps);
    const clockQ = Number(sim.clock_q());
    const clockFwd = Number(sim.clock_fwd());
    const clockBwd = Number(sim.clock_bwd());
    const clockSteps = clockFwd + clockBwd;
    const drift = steps > 0 ? clockQ / steps : 0;
    const epTotal = sim.ep_exact_total();
    const epRate = steps > 0 ? epTotal / steps : 0;
    rows.push({ seed, clockQ, clockSteps, drift, epRate });
  }
  console.log(`\n${label}`);
  for (const row of rows) {
    console.log(
      `seed ${row.seed} | Q ${row.clockQ} | steps ${row.clockSteps} | drift ${row.drift.toExponential(3)} | epRate ${row.epRate.toExponential(3)}`,
    );
  }
  return rows;
}

const baseParams = {
  beta: 1.0,
  stepSize: 0.01,
  p3On: 0,
  p6On: 0,
  pWrite: 0,
  pNWrite: 1,
  pAWrite: 0,
  pSWrite: 0,
  muHigh: 1.0,
  muLow: 1.0,
  kappaRep: 1.0,
  r0: 0.25,
  kappaBond: 0.0,
  rStar: 0.22,
  lambdaW: 0.0,
  lW: 4,
  lambdaN: 0.0,
  lN: 6,
  lambdaA: 0.0,
  lA: 6,
  lambdaS: 0.0,
  lS: 6,
  gridSize: 12,
  rPropose: 0.12,
  metaLayers: 0,
  eta: 0.0,
  etaDrive: 0.0,
  clockOn: 1,
  clockK: 8,
  clockFrac,
  clockUsesP6: 1,
  repairClockGated: 0,
};

const nullRows = runCase("Case A (null)", { ...baseParams, p6On: 0 });
for (const row of nullRows) {
  assert.ok(Math.abs(row.clockQ) <= 5 * Math.sqrt(row.clockSteps));
  assert.ok(Math.abs(row.drift) <= 1e-4);
  assert.ok(Math.abs(row.epRate) <= 5e-4);
}

const driveRows = runCase("Case B (P6 drive)", { ...baseParams, p6On: 1 });
const signs = driveRows
  .map((row) => Math.sign(row.clockQ))
  .filter((v) => v !== 0);
if (signs.length > 0) {
  const expected = signs[0];
  for (const s of signs) {
    assert.strictEqual(s, expected);
  }
}
for (const row of driveRows) {
  assert.ok(row.drift >= 1e-3);
  assert.ok(row.epRate > 1e-4);
}

console.log("\nClock current tests passed.");
</file>

<file path="scripts/test-clock-traversal-necessity.mjs">
#!/usr/bin/env node
import assert from "node:assert/strict";
import fs from "node:fs";
import path from "node:path";
import { fileURLToPath, pathToFileURL } from "node:url";

const __filename = fileURLToPath(import.meta.url);
const __dirname = path.dirname(__filename);
const rootDir = path.resolve(__dirname, "..");
const outDir = path.resolve(rootDir, ".tmp", "clock_code");

const wasmDir = path.resolve(rootDir, "apps/web/src/wasm/sim_core");
const wasmJs = pathToFileURL(path.join(wasmDir, "sim_core.js")).href;
const wasmBytes = fs.readFileSync(path.join(wasmDir, "sim_core_bg.wasm"));
const mod = await import(wasmJs);
mod.initSync({ module: wasmBytes });

const seeds = [1, 2, 3, 4, 5];
const steps = 1_000_000;
const reportEvery = 100_000;
const perturbStep = 500_000;

function ensureDir(dir) {
  fs.mkdirSync(dir, { recursive: true });
}

function readJson(filePath) {
  return JSON.parse(fs.readFileSync(filePath, "utf8"));
}

function mean(values) {
  return values.reduce((acc, v) => acc + v, 0) / values.length;
}

function percentile(values, p) {
  const nums = values.filter((v) => Number.isFinite(v)).sort((a, b) => a - b);
  if (nums.length === 0) return null;
  const idx = Math.min(nums.length - 1, Math.floor(p * (nums.length - 1)));
  return nums[idx];
}

function meanAbsDiff(a, b) {
  let sum = 0;
  for (let i = 0; i < a.length; i += 1) {
    sum += Math.abs(a[i] - b[i]);
  }
  return a.length === 0 ? 0 : sum / a.length;
}

function quadrantIndex(idx, g) {
  const x = idx % g;
  const y = Math.floor(idx / g);
  const qx = x < g / 2 ? 0 : 1;
  const qy = y < g / 2 ? 0 : 1;
  return qy * 2 + qx;
}

function logicalBitsFromField(field, g, lS, mask) {
  const sums = [0, 0, 0, 0];
  const counts = [0, 0, 0, 0];
  for (let i = 0; i < field.length; i += 1) {
    if (mask && !mask[i]) continue;
    const q = quadrantIndex(i, g);
    sums[q] += field[i];
    counts[q] += 1;
  }
  const threshold = lS / 2;
  return sums.map((sum, i) => {
    const meanVal = counts[i] > 0 ? sum / counts[i] : 0;
    return meanVal >= threshold ? 1 : 0;
  });
}

function errorRate(bitsA, bitsB) {
  let mismatches = 0;
  for (let i = 0; i < bitsA.length; i += 1) {
    if (bitsA[i] !== bitsB[i]) mismatches += 1;
  }
  return mismatches / bitsA.length;
}

function makeMask(seed, size, frac) {
  let x = seed >>> 0;
  if (x === 0) x = 1;
  const mask = new Array(size);
  for (let i = 0; i < size; i += 1) {
    x ^= x << 13;
    x ^= x >>> 17;
    x ^= x << 5;
    const r = (x >>> 8) / (1 << 24);
    mask[i] = r < frac;
  }
  return mask;
}

function errF05(baseBits, metaField, g, lS, seed) {
  const trials = 20;
  let acc = 0;
  for (let t = 0; t < trials; t += 1) {
    const mask = makeMask(seed + t * 101, metaField.length, 0.5);
    const bits = logicalBitsFromField(metaField, g, lS, mask);
    acc += errorRate(bits, baseBits);
  }
  return acc / trials;
}

function runPreset(id, paramsPath) {
  const params = readJson(path.resolve(rootDir, paramsPath));
  const runs = [];
  for (const seed of seeds) {
    const sim = new mod.Sim(50, seed);
    sim.set_params(params);
    let preSamples = [];
    let baselineMean = null;
    let baselineTarget = null;
    let recoverySteps = null;
    let perturbApplied = false;

    for (let step = reportEvery; step <= steps; step += reportEvery) {
      sim.step(reportEvery);
      const baseS = sim.base_s_field();
      const metaS = sim.meta_field();
      const cells = baseS.length;
      const meta0 = metaS.subarray(0, cells);
      const sdiffBase = meanAbsDiff(baseS, meta0);
      if (!perturbApplied) {
        if (step <= perturbStep) preSamples.push(sdiffBase);
        if (step >= perturbStep) {
          baselineMean = mean(preSamples.slice(-3));
          baselineTarget = Math.max(baselineMean, 0.5);
          sim.apply_perturbation({
            target: "metaS",
            layer: 0,
            frac: 0.3,
            mode: "randomize",
            seed: seed * 1000 + 9,
          });
          perturbApplied = true;
        }
      } else if (recoverySteps === null && baselineTarget !== null) {
        if (sdiffBase <= baselineTarget * 1.1) {
          recoverySteps = step - perturbStep;
        }
      }
    }

    const baseS = sim.base_s_field();
    const metaS = sim.meta_field();
    const cells = baseS.length;
    const meta0 = metaS.subarray(0, cells);
    const lS = params.lS ?? 1;
    const baseBits = logicalBitsFromField(baseS, params.gridSize, lS);
    const err = errF05(baseBits, meta0, params.gridSize, lS, seed + 4000);
    const sdiffBase = meanAbsDiff(baseS, meta0);

    runs.push({
      id,
      seed,
      errF05: err,
      sdiffBase,
      recoverySteps: recoverySteps ?? Infinity,
    });
  }
  return runs;
}

ensureDir(outDir);

const presets = [
  { id: "A_ungated", file: "scripts/params/clock_code/code_p6_drive.json" },
  { id: "B_gated_clock", file: "scripts/params/clock_code/code_p6_clock_gated.json" },
  { id: "C_gated_static", file: "scripts/params/clock_code/code_p6_clock_gated_static.json" },
  { id: "D_gated_random", file: "scripts/params/clock_code/code_p6_clock_gated_random.json" },
];

const rawLines = [];
const summaries = [];

for (const preset of presets) {
  const runs = runPreset(preset.id, preset.file);
  for (const row of runs) rawLines.push(JSON.stringify(row));
  const err = runs.map((r) => r.errF05);
  const sdiff = runs.map((r) => r.sdiffBase);
  const recovery = runs.map((r) => r.recoverySteps);
  summaries.push({
    id: preset.id,
    errMean: mean(err),
    sdiffMean: mean(sdiff),
    recoveryMean: mean(recovery.filter((v) => Number.isFinite(v))),
    recoveryP95: percentile(recovery, 0.95),
    recoveryFinite: recovery.filter((v) => Number.isFinite(v)).length,
    errPass: err.filter((v) => v <= 0.1).length,
    errFail: err.filter((v) => v >= 0.25).length,
    recovery,
    err,
  });
}

const summaryPath = path.join(outDir, "clock_traversal_necessity_summary.csv");
const rawPath = path.join(outDir, "clock_traversal_necessity_raw.jsonl");
fs.writeFileSync(rawPath, rawLines.join("\n"));

const header = [
  "preset",
  "errF05Mean",
  "sdiffMean",
  "recoveryMean",
  "recoveryP95",
  "recoveryFinite",
  "errPassCount",
  "errFailCount",
];
const csvLines = [
  header.join(","),
  ...summaries.map((s) =>
    [
      s.id,
      s.errMean,
      s.sdiffMean,
      Number.isFinite(s.recoveryMean) ? s.recoveryMean : "",
      s.recoveryP95 ?? "",
      s.recoveryFinite,
      s.errPass,
      s.errFail,
    ].join(","),
  ),
];
fs.writeFileSync(summaryPath, csvLines.join("\n"));

const gated = summaries.find((s) => s.id === "B_gated_clock");
const staticCtrl = summaries.find((s) => s.id === "C_gated_static");

assert.ok(gated.errPass >= 4);
assert.ok(gated.recoveryFinite >= 4);

const gatedRecoveryMean = gated.recoveryMean;
const staticFailByErr = staticCtrl.errFail >= 4;
const staticFailByRecovery = staticCtrl.recovery.filter((v) => !Number.isFinite(v) || v > 3 * gatedRecoveryMean).length >= 4;
assert.ok(staticFailByErr || staticFailByRecovery);

console.log("Clock traversal necessity summary:");
for (const row of summaries) {
  console.log(
    `${row.id} | err(f=0.5) ${row.errMean.toFixed(3)} | sdiff ${row.sdiffMean.toFixed(3)} | recovery mean ${row.recoveryMean.toFixed(1)} | err<=0.1 count ${row.errPass}`,
  );
}
</file>

<file path="NOTE-TO-SELF.md">
# NOTE TO SELF — Ratchet Playground Status (handoff)

## What we built

We scaffolded a web app and implemented the first MVP simulation loop for the Ratchet Playground:

- **UI:** `apps/web` (Vite + React + TypeScript)
- **Sim core:** `crates/sim-core` (Rust compiled to WASM via `wasm-pack`)
- **Runtime model:** the simulation runs in a **WebWorker** and streams snapshots to the UI.
- **Rendering:** Canvas2D draws particles and P₁ bond edges.
- **Docs source of truth:** `docs/` (ratchet primitives P₁–P₆, null regime, deliverables A–D).

## Key constraints (must preserve)

- **Null regime requirement:** with P₃=OFF and P₆=OFF, the Markov chain must satisfy **detailed balance** w.r.t. an explicit stationary measure (Deliverable A).
- Only P₃ and P₆ may break reversibility. (We are currently implementing only the null regime and P₁.)
- This is a scientific instrument: **what is drawn must equal what the physics is**. We removed a prior UI-only bond filter; the UI now draws exactly the bond list returned by the sim, with torus-aware segment drawing.

## Current simulation (MVP)

State:
- Particles on a 2D torus: `x_i ∈ [0,1)^2`
- P₁ bond weights: `w_ij ∈ {0..L_w}` stored in an upper-triangular array
- P₄ counters: `n_i ∈ {-L_n..L_n}` (per-particle counters)

Dynamics (null regime):
- Mixture kernel per step:
  - **X move**: pick particle `i`, symmetric proposal `x_i' = x_i + δ (mod 1)`, Metropolis accept using `ΔE`
  - **P₁ write**: propose `w_ij → w_ij ± 1` with symmetric proposals, Metropolis accept using `ΔE`
- **P₄ write**: propose `n_i → n_i ± 1` with symmetric proposals, Metropolis accept using `ΔE`
- Energy `E(Z)` includes repulsion and a bond geometry coupling plus quadratic penalties on `w` (matches Deliverable A template for X+P1).
- Energy now also includes quadratic penalties on `n` (P₄) per Deliverable A.
- **Neighbor locality for P₁ proposals:** write proposals are restricted to pairs with torus distance `r_ij ≤ r_propose` (aligned with `docs/12_browser_impl_notes.md` guidance about sparse graphs). Bonds can still persist even if particles later move apart.

Files:
- Sim core: `crates/sim-core/src/lib.rs`
- Worker bridge: `apps/web/src/sim/sim.worker.ts`
- UI: `apps/web/src/App.tsx`
 - Run cache: `apps/web/src/sim/runCache.ts`

## UI/Worker/WASM wiring status

Worker setup was initially broken; fixes applied:
- Vite worker import uses `?worker` (required for correct TS worker bundling).
- Message queuing implemented in `SimWorkerClient` so early worker messages aren’t lost before React effects attach handlers.
- React StrictMode double-mount issue avoided by using a module-level singleton `SimWorkerClient` in `App.tsx`.

Current behavior:
- `Init` constructs the WASM `Sim`, posts `ready` and an immediate snapshot.
- `Run` loops and posts snapshots; `Step` advances fixed steps.
- Each snapshot now includes diagnostics and step count.

## Tunable parameters (now exposed)

We exposed key null-regime params to the UI and plumbed them into WASM:

- UI maintains **draft vs applied** parameters, with a clear “Apply params” button (draft edits do not silently affect runs).
- Worker sends `config` with `{ bondThreshold, params }` and calls `sim.set_params(params)` (or caches params if config arrives before init).
- WASM provides `Sim.set_params(JsValue)` with safe clamps and optional field handling.

Types:
- `apps/web/src/sim/workerMessages.ts` defines `SimParams`
- `apps/web/src/App.tsx` provides presets (`sparse`, `balanced`, `dense`)

P₄ parameters added:
- `pNWrite`, `lambdaN`, `lN`

Important: For clean scientific runs, prefer **Apply params + Init** whenever changing physics parameters.

## Diagnostics + charts (Deliverable D MVP)

We implemented minimal null‑regime diagnostics and time‑series charts:
- **P₁ flux/affinity:** windowed counts of accepted `w+`/`w-`, with `Jw`, `Aw`, and `Σ_mem` (should ~0 in null regime).
- **Graph stats:** edges, component count, largest component size and fraction.
- **Charts panel:** time‑series sparklines for `Jw`, `Aw`, `Σ_mem`, edges, largest component fraction, total energy.
- **Histograms:** bond weight histogram (from WASM) and component size histogram (from UI).
- **Step counter** shown in UI.

Charts are data‑driven (`CHARTS`/`HISTOGRAMS` lists in `apps/web/src/App.tsx`) so adding new metrics is straightforward.

## Torus consistency audit

- Sim-core uses torus wrap for positions and torus minimal-image distance for all distance-dependent terms.
- Rendering draws torus-consistent shortest wrapped segments for each bond; no UI-only filtering remains.
- We fixed a sign bug in the bond wrap drawing (wrong image shift sign caused “curtain” artifacts). Now the “curtains” are gone; remaining density is physics/parameters.

## Build / tooling notes

- Root `Makefile` has `make dev`, `make wasm`, etc.
- `make wasm` has been hardened to use a repo-local temp dir `.tmp/` because `wasm-pack` sometimes fails creating temp dirs for `cargo install wasm-bindgen` on this setup.
- `.gitignore` includes `.tmp/`.
- `scripts/bootstrap.sh` installs `wasm-pack` and also `wasm-bindgen-cli` to reduce wasm-pack download attempts.

## Known issues / open questions

1) **Dense bond graphs** can still occur depending on params and equilibrium regime; after fixing the torus-drawing sign bug, remaining density is not a rendering artifact. Use UI params + `bondThreshold` to explore regimes; add diagnostics next (see below).
2) The P₁ proposal selection currently scans all pairs to choose a random neighbor pair (reservoir sampling). This is O(N²) per write attempt and will not scale; docs recommend spatial hashing / neighbor lists.
3) The “Sparse” preset may still percolate into a single connected component at `bondThreshold=3` (often becomes sparser if threshold is raised to `Lw`). This indicates the chosen null-regime parameters still admit a dense-bond equilibrium for that threshold; needs empirical tuning.
4) Snapshot cadence vs “record every steps” is limited by the worker step chunk (`sim.step(500)`), so recording interval ≤ 500 still yields one sample per snapshot.

## Where we want to go next

Priority roadmap (stay aligned with docs):

1) **Deliverable D (null regime sanity diagnostics)**
   - ✅ Implemented minimal diagnostics + charts + histograms.

2) **Performance correctness: neighbor lists**
   - Replace O(N²) neighbor selection with spatial hashing buckets and symmetric proposal accounting (as in `docs/12_browser_impl_notes.md`).

3) **Expand primitives**
   - Implement P₂/P₄/P₅ as additional reversible channels (still null regime).
   - Only later add P₃ and/or P₆ with the strict constraint that they are the only irreversibility sources (Deliverables B/C).

4) **Experiment management**
   - Add “preset export/import” (JSON) and a run metadata panel (params, seed, timestamp).
   - Make “init required after physics change” more explicit (e.g., disable Run until re-init).
   - **Run cache + export implemented:** `runCache.ts` stores snapshots in IndexedDB + memory and supports JSONL export via UI button. Cache resets on Init. Record interval default is 2000 steps (UI).
</file>

<file path="README.md">
# Ratchet Playground

Interactive browser sandbox for exploring **ratchet primitives P₁–P₆** under a strict **null regime** (P₃=OFF, P₆=OFF ⇒ reversible / detailed balance), plus diagnostics for irreversibility (cycle affinities) and structure.

- Theory/spec: `docs/README.md`
- UI app: `apps/web`
- Simulation core: `crates/sim-core`

## Prereqs (WSL Ubuntu 22.04)

You’ll need Node.js + npm and Rust + wasm tooling.

### One-shot bootstrap (recommended)

```bash
bash scripts/bootstrap.sh
```

### Install Rust toolchain

```bash
curl --proto '=https' --tlsv1.2 -sSf https://sh.rustup.rs | sh
source "$HOME/.cargo/env"
rustup target add wasm32-unknown-unknown
cargo install wasm-pack
```

### Install Node.js (one option: nvm)

```bash
curl -o- https://raw.githubusercontent.com/nvm-sh/nvm/v0.40.1/install.sh | bash
source "$HOME/.nvm/nvm.sh"
nvm install --lts
```

## Getting started

From the repo root:

```bash
# Recommended: use the Makefile targets (handles temp-dir quirks for wasm-pack)
make dev
```

This will:
- compile the Rust/WASM package into `apps/web/src/wasm/sim_core`
- install `apps/web` npm dependencies
- start the Vite dev server (it may use port 5174+ if 5173 is busy)

### Manual compile/run (no Makefile)

```bash
# 1) Build the WASM package into the web app source tree
mkdir -p .tmp
TMPDIR="$PWD/.tmp" CARGO_TARGET_DIR="$PWD/.tmp/cargo-target" \
  cd crates/sim-core && wasm-pack build --target web --out-dir ../../apps/web/src/wasm/sim_core --out-name sim_core

# 2) Install web deps and run dev server
cd apps/web
npm install
npm run dev
```

### Production build

```bash
make wasm
cd apps/web
npm run build
npm run preview
```
</file>

<file path="apps/web/src/style.css">
:root {
  color-scheme: dark;
  --bg: #0b0f14;
  --panel: #111826;
  --text: #e6edf3;
  --muted: #97a3b3;
  --border: rgba(255, 255, 255, 0.12);
  --accent: #6aa9ff;
}

html,
body {
  height: 100%;
}

body {
  margin: 0;
  background: var(--bg);
  color: var(--text);
  font-family: ui-sans-serif, system-ui, -apple-system, Segoe UI, Roboto, Helvetica,
    Arial, "Apple Color Emoji", "Segoe UI Emoji";
}

.app {
  display: grid;
  grid-template-columns: 320px 1fr;
  height: 100vh;
}

.panel {
  padding: 16px;
  border-right: 1px solid var(--border);
  background: var(--panel);
}

.panel h1 {
  font-size: 16px;
  margin: 0 0 8px 0;
}

.panel p {
  margin: 0 0 12px 0;
  color: var(--muted);
  font-size: 13px;
  line-height: 1.35;
}

.row {
  display: grid;
  grid-template-columns: 1fr 1fr;
  gap: 8px;
  margin-bottom: 10px;
}

.row.twoCol {
  grid-template-columns: 1fr 1fr;
}

.panel label {
  display: block;
  font-size: 12px;
  color: var(--muted);
  margin-bottom: 6px;
}

.panel input {
  width: 100%;
  box-sizing: border-box;
  padding: 8px 10px;
  border-radius: 8px;
  border: 1px solid var(--border);
  background: rgba(255, 255, 255, 0.04);
  color: var(--text);
}

.panel select {
  width: 100%;
  box-sizing: border-box;
  padding: 8px 10px;
  border-radius: 8px;
  border: 1px solid var(--border);
  background: rgba(255, 255, 255, 0.04);
  color: var(--text);
}

.panel button {
  padding: 10px 12px;
  border-radius: 10px;
  border: 1px solid var(--border);
  background: rgba(255, 255, 255, 0.06);
  color: var(--text);
  cursor: pointer;
}

.panel button.primary {
  border-color: rgba(106, 169, 255, 0.55);
  background: rgba(106, 169, 255, 0.18);
}

.panel button:disabled {
  opacity: 0.5;
  cursor: not-allowed;
}

.primitiveToggle {
  display: grid;
  gap: 8px;
  margin-top: 6px;
}

.primitiveToggle label {
  display: grid;
  grid-template-columns: 18px 12px 1fr;
  align-items: center;
  column-gap: 8px;
  font-size: 13px;
  color: var(--text);
}

.primitiveLabel {
  white-space: nowrap;
}

.primitiveToggle input[type="checkbox"] {
  accent-color: var(--accent);
  justify-self: center;
}

.legendSwatch {
  width: 10px;
  height: 10px;
  border-radius: 3px;
  border: 1px solid rgba(255, 255, 255, 0.35);
  display: inline-block;
}

.legendSwatch--P1 {
  background: rgba(80, 170, 255, 0.9);
}

.legendSwatch--P2 {
  background: rgba(255, 170, 80, 0.9);
}

.legendSwatch--P4 {
  background: rgba(120, 230, 120, 0.9);
}

.legendSwatch--P5 {
  background: rgba(220, 120, 230, 0.9);
}

.legendSwatch--P3 {
  background: rgba(255, 230, 80, 0.9);
}

.legendSwatch--P6 {
  background: rgba(255, 120, 120, 0.9);
}

.accordion {
  margin-bottom: 12px;
  padding: 10px 12px 6px;
  border: 1px solid var(--border);
  border-radius: 10px;
  background: rgba(255, 255, 255, 0.03);
}

.accordionTitle {
  font-size: 12px;
  text-transform: uppercase;
  letter-spacing: 0.06em;
  color: var(--muted);
  margin-bottom: 0;
  padding: 4px 0;
  display: flex;
  align-items: center;
  gap: 6px;
  transition: color 0.2s;
}

.accordionTitle:hover {
  color: var(--text);
}

.accordionTitle span {
  font-size: 10px;
  transition: transform 0.2s;
}

.accordionContent {
  margin-top: 12px;
}

.main {
  display: grid;
  grid-template-rows: 1fr auto;
  height: 100vh;
  overflow: hidden;
}

.canvasWrap {
  position: relative;
  display: grid;
  place-items: center;
  min-height: 0;
  padding: 16px;
}

canvas {
  width: min(92vmin, 980px);
  height: auto;
  aspect-ratio: 1 / 1;
  max-width: 100%;
  max-height: 100%;
  border-radius: 16px;
  border: 1px solid var(--border);
  background: rgba(255, 255, 255, 0.02);
}

.chartsPanel {
  padding: 12px 16px 14px;
  background: linear-gradient(180deg, rgba(11, 15, 20, 0.1), rgba(11, 15, 20, 0.85));
  border-top: 1px solid var(--border);
  backdrop-filter: blur(6px);
  display: grid;
  grid-template-columns: repeat(auto-fill, minmax(190px, 1fr));
  gap: 12px;
}

.chartCard {
  background: rgba(255, 255, 255, 0.03);
  border: 1px solid var(--border);
  border-radius: 10px;
  padding: 8px 10px;
  display: grid;
  grid-template-rows: auto 1fr;
  gap: 6px;
  min-height: 86px;
}

.chartCard--P1 {
  background: rgba(106, 169, 255, 0.08);
}

.chartCard--P2 {
  background: rgba(140, 200, 255, 0.08);
}

.chartCard--P4 {
  background: rgba(120, 210, 180, 0.08);
}

.chartCard--P5 {
  background: rgba(160, 220, 140, 0.08);
}

.chartCard--P3 {
  background: rgba(240, 210, 80, 0.1);
}

.chartCard--System {
  background: rgba(255, 205, 120, 0.08);
}

.chartCard--Graph {
  background: rgba(255, 170, 160, 0.08);
}

.chartLabel {
  font-size: 11px;
  color: var(--muted);
  letter-spacing: 0.02em;
  text-transform: uppercase;
}

.chartCard canvas {
  width: 100%;
  height: 58px;
  border: none;
  border-radius: 6px;
  background: rgba(255, 255, 255, 0.02);
}
</file>

<file path="scripts/params/clock_code/code_p6_clock_gated.json">
{
  "beta": 2.0,
  "stepSize": 0.01,
  "p3On": 0,
  "p6On": 1,
  "p6SFactor": 0.0,
  "pWrite": 0,
  "pNWrite": 0.1,
  "pAWrite": 0,
  "pSWrite": 0.7,
  "muHigh": 1.0,
  "muLow": 1.0,
  "kappaRep": 1.0,
  "r0": 0.25,
  "kappaBond": 0.0,
  "rStar": 0.22,
  "lambdaW": 0.0,
  "lW": 4,
  "lambdaN": 0.0,
  "lN": 6,
  "lambdaA": 0.0,
  "lA": 6,
  "lambdaS": 0.5,
  "lS": 20,
  "gridSize": 8,
  "rPropose": 0.12,
  "metaLayers": 2,
  "eta": 0.0,
  "etaDrive": 1.0,
  "clockOn": 1,
  "clockK": 8,
  "clockFrac": 1.0,
  "clockUsesP6": 1,
  "repairClockGated": 1
}
</file>

<file path="scripts/deadline-event-utils.mjs">
import fs from "node:fs";
import path from "node:path";
import { fileURLToPath, pathToFileURL } from "node:url";

const __filename = fileURLToPath(import.meta.url);
const __dirname = path.dirname(__filename);
const rootDir = path.resolve(__dirname, "..");

let wasmMod = null;
let wasmInit = false;

export async function loadWasm() {
  if (wasmInit) return wasmMod;
  const wasmDir = path.resolve(rootDir, "apps/web/src/wasm/sim_core");
  const wasmJs = pathToFileURL(path.join(wasmDir, "sim_core.js")).href;
  const wasmBytes = fs.readFileSync(path.join(wasmDir, "sim_core_bg.wasm"));
  const mod = await import(wasmJs);
  mod.initSync({ module: wasmBytes });
  wasmMod = mod;
  wasmInit = true;
  return wasmMod;
}

export function readJson(filePath) {
  return JSON.parse(fs.readFileSync(filePath, "utf8"));
}

export function mean(values) {
  return values.reduce((acc, v) => acc + v, 0) / values.length;
}

export function std(values) {
  const m = mean(values);
  const variance = values.reduce((acc, v) => acc + (v - m) ** 2, 0) / values.length;
  return Math.sqrt(variance);
}

export function percentile(values, p) {
  const nums = values.filter((v) => Number.isFinite(v)).sort((a, b) => a - b);
  if (nums.length === 0) return null;
  const idx = Math.min(nums.length - 1, Math.floor(p * (nums.length - 1)));
  return nums[idx];
}

export function parseSeedList(seedArg) {
  if (!seedArg) return [1, 2, 3];
  return seedArg
    .split(",")
    .map((s) => Number(s.trim()))
    .filter((v) => Number.isFinite(v) && v > 0);
}

export function quadrantIndex(idx, g) {
  const x = idx % g;
  const y = Math.floor(idx / g);
  const qx = x < g / 2 ? 0 : 1;
  const qy = y < g / 2 ? 0 : 1;
  return qy * 2 + qx;
}

export function quadrantMeans(field, g, mask) {
  const sums = [0, 0, 0, 0];
  const counts = [0, 0, 0, 0];
  for (let i = 0; i < field.length; i += 1) {
    if (mask && !mask[i]) continue;
    const q = quadrantIndex(i, g);
    sums[q] += field[i];
    counts[q] += 1;
  }
  return sums.map((sum, i) => (counts[i] > 0 ? sum / counts[i] : 0));
}

export function logicalBitsFromField(field, g, lS, mask) {
  const sums = [0, 0, 0, 0];
  const counts = [0, 0, 0, 0];
  for (let i = 0; i < field.length; i += 1) {
    if (mask && !mask[i]) continue;
    const q = quadrantIndex(i, g);
    sums[q] += field[i];
    counts[q] += 1;
  }
  const threshold = lS / 2;
  return sums.map((sum, i) => {
    const meanVal = counts[i] > 0 ? sum / counts[i] : 0;
    return meanVal >= threshold ? 1 : 0;
  });
}

export function errorRate(bitsA, bitsB) {
  let mismatches = 0;
  for (let i = 0; i < bitsA.length; i += 1) {
    if (bitsA[i] !== bitsB[i]) mismatches += 1;
  }
  return mismatches / bitsA.length;
}

export function errRegionBits(baseField, metaField, g, lS, regionMask) {
  const baseBits = logicalBitsFromField(baseField, g, lS, regionMask);
  const metaBits = logicalBitsFromField(metaField, g, lS, regionMask);
  return errorRate(metaBits, baseBits);
}

export function errQuadrantMean(baseField, metaField, g, lS, regionMask) {
  const denom = lS > 0 ? lS : 1;
  const baseMeans = quadrantMeans(baseField, g, regionMask);
  const metaMeans = quadrantMeans(metaField, g, regionMask);
  let acc = 0;
  for (let i = 0; i < 4; i += 1) {
    acc += Math.abs(baseMeans[i] - metaMeans[i]);
  }
  return acc / (4 * denom);
}

export function makeMask(seed, size, frac) {
  let x = seed >>> 0;
  if (x === 0) x = 1;
  const mask = new Array(size);
  for (let i = 0; i < size; i += 1) {
    x ^= x << 13;
    x ^= x >>> 17;
    x ^= x << 5;
    const r = (x >>> 8) / (1 << 24);
    mask[i] = r < frac;
  }
  return mask;
}

export function errF05(baseBits, metaField, g, lS, seed) {
  const trials = 20;
  let acc = 0;
  for (let t = 0; t < trials; t += 1) {
    const mask = makeMask(seed + t * 101, metaField.length, 0.5);
    const bits = logicalBitsFromField(metaField, g, lS, mask);
    acc += errorRate(bits, baseBits);
  }
  return acc / trials;
}

export function errF05Region(baseField, metaField, g, lS, seed, regionMask) {
  const trials = 20;
  let acc = 0;
  for (let t = 0; t < trials; t += 1) {
    const mask = makeMask(seed + t * 101, metaField.length, 0.5);
    if (regionMask) {
      for (let i = 0; i < mask.length; i += 1) {
        mask[i] = mask[i] && regionMask[i];
      }
    }
    const baseBits = logicalBitsFromField(baseField, g, lS, mask);
    const metaBits = logicalBitsFromField(metaField, g, lS, mask);
    acc += errorRate(metaBits, baseBits);
  }
  return acc / trials;
}

function stripeIndex(idx, g, bins) {
  const x = idx % g;
  const fx = x / g;
  return Math.min(bins - 1, Math.floor(fx * bins));
}

function regionMask(g, regionType, regionIndex, span, bins) {
  const cells = g * g;
  const mask = new Array(cells);
  if (regionType === "stripe") {
    const s = Math.max(1, span);
    for (let i = 0; i < cells; i += 1) {
      const stripe = stripeIndex(i, g, bins);
      let ok = false;
      for (let k = 0; k < s; k += 1) {
        if ((regionIndex + k) % bins === stripe) {
          ok = true;
          break;
        }
      }
      mask[i] = ok;
    }
  } else {
    for (let i = 0; i < cells; i += 1) {
      mask[i] = quadrantIndex(i, g) === regionIndex;
    }
  }
  return mask;
}

export function meanAbsDiffRegion(a, b, mask) {
  let sum = 0;
  let count = 0;
  for (let i = 0; i < a.length; i += 1) {
    if (!mask[i]) continue;
    sum += Math.abs(a[i] - b[i]);
    count += 1;
  }
  return count === 0 ? 0 : sum / count;
}

function gateAllowsRegion(params, active, regionType, regionIndex, gateSpan, bins) {
  if (!params.repairClockGated) return true;
  if (regionType === "stripe") {
    const span = Math.min(bins, Math.max(1, gateSpan ?? 1));
    const activeBin = active % bins;
    for (let i = 0; i < span; i += 1) {
      if ((activeBin + i) % bins === regionIndex) return true;
    }
    return false;
  }
  return (active % 4) === regionIndex;
}

export async function calibrateGateGaps({
  presetPath,
  presetParams,
  variant,
  steps,
  reportEvery,
  regionType,
  regionIndex,
  gateSpan,
}) {
  const mod = await loadWasm();
  const baseParams = presetParams ?? readJson(path.resolve(rootDir, presetPath));
  const params = { ...baseParams };
  params.codeNoiseRate = 0.0;
  params.codeNoiseBatch = params.codeNoiseBatch ?? 1;
  params.codeNoiseLayer = params.codeNoiseLayer ?? 0;
  params.repairClockGated = 1;
  if (variant === "drift") {
    params.clockOn = 1;
    params.clockUsesP6 = 1;
  } else if (variant === "random") {
    params.clockOn = 1;
    params.clockUsesP6 = 0;
  } else {
    params.clockOn = 0;
    params.clockUsesP6 = 1;
  }
  if (gateSpan) params.repairGateSpan = gateSpan;

  const bins = params.clockK ?? 8;
  const effectiveSpan = gateSpan ?? params.repairGateSpan ?? 1;
  const sim = new mod.Sim(50, 1);
  sim.set_params(params);

  const gaps = [];
  let lastAllowed = null;
  for (let t = reportEvery; t <= steps; t += reportEvery) {
    sim.step(reportEvery);
    const active = sim.clock_state();
    if (gateAllowsRegion(params, active, regionType, regionIndex, effectiveSpan, bins)) {
      if (lastAllowed !== null) {
        gaps.push(t - lastAllowed);
      }
      lastAllowed = t;
    }
  }

  return {
    gaps,
    gapP50: percentile(gaps, 0.5),
    gapP95: percentile(gaps, 0.95),
    gapMax: gaps.length ? Math.max(...gaps) : null,
  };
}

export async function runDeadlineEvents({
  presetPath,
  presetParams,
  variant,
  seeds,
  steps,
  reportEvery,
  eventEvery,
  deadline,
  regionType,
  regionIndex,
  gateSpan,
  corruptFrac,
  errGood,
  sdiffGood,
  tailWindow,
  includeEvents = false,
}) {
  const mod = await loadWasm();
  const baseParams = presetParams ?? readJson(path.resolve(rootDir, presetPath));
  const params = { ...baseParams, epDebug: 1 };
  if (variant === "drift") {
    params.clockOn = 1;
    params.clockUsesP6 = 1;
  } else if (variant === "random") {
    params.clockOn = 1;
    params.clockUsesP6 = 0;
  } else {
    params.clockOn = 0;
    params.clockUsesP6 = 1;
  }
  if (gateSpan) params.repairGateSpan = gateSpan;

  const bins = params.clockK ?? 8;
  const effectiveSpan = gateSpan ?? params.repairGateSpan ?? 1;
  const regionMaskArr = regionMask(params.gridSize, regionType, regionIndex, effectiveSpan, bins);
  const tailWindowSteps = tailWindow ?? 200_000;
  const graceWindow = Math.max(0, Math.floor(0.2 * deadline));

  const runs = [];
  const MOVE_P5_META = 8;
  const MOVE_OPK = 9;
  const MOVE_CLOCK = 10;
  const eventsPerRun = Math.floor((steps - deadline) / eventEvery);
  const eventTimes = [];
  for (let t = eventEvery; t + deadline <= steps; t += eventEvery) {
    eventTimes.push(t);
  }

  for (const seed of seeds) {
    const sim = new mod.Sim(50, seed);
    sim.set_params(params);
    const readCounts = () => {
      const stats = sim.ep_q_stats();
      const counts = stats.count;
      return Array.from(counts);
    };
    let lastCounts = readCounts();
    const events = eventTimes.map((t) => ({ tEvent: t, recovered: false, recovery: null, miss: false }));
    let eventIdx = 0;
    let lastEventTime = null;
    let misses = 0;
    let recoveries = [];
    let p5MetaRecovered = [];
    let opkRecovered = [];
    let uptimeGood = 0;
    let sampleCount = 0;
    const sampleRecords = [];
    const baselineSamples = [];
    let errFloor = null;

    for (let t = reportEvery; t <= steps; t += reportEvery) {
      sim.step(reportEvery);
      const countsNow = readCounts();

      if (eventIdx < events.length && t >= events[eventIdx].tEvent) {
        for (; eventIdx < events.length && events[eventIdx].tEvent <= t; eventIdx += 1) {
          const event = events[eventIdx];
          const perturb = {
            target: "metaS",
            layer: 0,
            frac: corruptFrac ?? 1.0,
            mode: "randomize",
          };
          if (regionType === "stripe") {
            perturb.region = "stripe";
            perturb.bins = bins;
            perturb.span = gateSpan ?? 1;
            perturb.bin = regionIndex;
          } else {
            perturb.region = "quadrant";
            perturb.quadrant = regionIndex;
          }
          perturb.seed = seed * 1000 + event.tEvent;
          sim.apply_perturbation(perturb);
          lastEventTime = event.tEvent;
          event.startCounts = {
            p5Meta: countsNow[MOVE_P5_META] ?? 0,
            opk: countsNow[MOVE_OPK] ?? 0,
            clock: countsNow[MOVE_CLOCK] ?? 0,
            total: countsNow.reduce((acc, v) => acc + v, 0),
          };
        }
      }

      const baseS = sim.base_s_field();
      const metaS = sim.meta_field();
      const cells = baseS.length;
      const meta0 = metaS.subarray(0, cells);
      const sdiff = meanAbsDiffRegion(baseS, meta0, regionMaskArr);
      const lS = params.lS ?? 1;
      const errSample = errRegionBits(baseS, meta0, params.gridSize, lS, regionMaskArr);
      if (t < eventEvery) {
        baselineSamples.push(errSample);
      } else if (errFloor === null) {
        errFloor = baselineSamples.length > 0 ? mean(baselineSamples) : 0;
      }
      const errAdj = errFloor === null ? 0 : Math.max(0, errSample - errFloor);
      const good = sdiff <= sdiffGood && errAdj <= errGood;
      uptimeGood += good ? 1 : 0;
      sampleCount += 1;
      const sinceEvent = lastEventTime === null ? Number.POSITIVE_INFINITY : t - lastEventTime;
      sampleRecords.push({ t, err: errAdj, sdiff, good, sinceEvent });

      for (const event of events) {
        if (event.recovered || event.miss) continue;
        if (t < event.tEvent) continue;
        const elapsed = t - event.tEvent;
        if (elapsed > deadline) {
          event.miss = true;
          misses += 1;
          if (event.startCounts) {
            event.windowCounts = {
              p5Meta: (countsNow[MOVE_P5_META] ?? 0) - event.startCounts.p5Meta,
              opk: (countsNow[MOVE_OPK] ?? 0) - event.startCounts.opk,
              clock: (countsNow[MOVE_CLOCK] ?? 0) - event.startCounts.clock,
              total: countsNow.reduce((acc, v) => acc + v, 0) - event.startCounts.total,
            };
            event.p5MetaAcceptedToOutcome = event.windowCounts.p5Meta;
            event.opkAcceptedToOutcome = event.windowCounts.opk;
            event.clockAcceptedToOutcome = event.windowCounts.clock;
            event.stepsToOutcome = elapsed;
          }
          continue;
        }
        if (good) {
          event.recovered = true;
          event.recovery = elapsed;
          recoveries.push(elapsed);
          if (event.startCounts) {
            event.windowCounts = {
              p5Meta: (countsNow[MOVE_P5_META] ?? 0) - event.startCounts.p5Meta,
              opk: (countsNow[MOVE_OPK] ?? 0) - event.startCounts.opk,
              clock: (countsNow[MOVE_CLOCK] ?? 0) - event.startCounts.clock,
              total: countsNow.reduce((acc, v) => acc + v, 0) - event.startCounts.total,
            };
            event.p5MetaAcceptedToOutcome = event.windowCounts.p5Meta;
            event.opkAcceptedToOutcome = event.windowCounts.opk;
            event.clockAcceptedToOutcome = event.windowCounts.clock;
            event.stepsToOutcome = elapsed;
            p5MetaRecovered.push(event.windowCounts.p5Meta);
            opkRecovered.push(event.windowCounts.opk);
          }
        }
      }

      lastCounts = countsNow;
    }

    for (const event of events) {
      if (!event.recovered && !event.miss) {
        event.miss = true;
        misses += 1;
      }
    }


    const baseS = sim.base_s_field();
    const metaS = sim.meta_field();
    const cells = baseS.length;
    const meta0 = metaS.subarray(0, cells);
    const lS = params.lS ?? 1;
    const baseBits = logicalBitsFromField(baseS, params.gridSize, lS, regionMaskArr);
    const errEnd = errorRate(baseBits, logicalBitsFromField(meta0, params.gridSize, lS, regionMaskArr));
    const sdiffEnd = meanAbsDiffRegion(baseS, meta0, regionMaskArr);

    const tailStart = Math.max(0, steps - tailWindowSteps);
    const tailSamples = sampleRecords.filter(
      (s) => s.t >= tailStart && s.sinceEvent >= graceWindow,
    );
    const tailUptime = tailSamples.length
      ? tailSamples.filter((s) => s.good).length / tailSamples.length
      : 0;
    const tailErrMean = tailSamples.length ? mean(tailSamples.map((s) => s.err)) : 0;
    const tailSdiffMean = tailSamples.length ? mean(tailSamples.map((s) => s.sdiff)) : 0;
    const errP95 = percentile(sampleRecords.map((s) => s.err), 0.95);

    const p5MetaSuccess = [];
    const p5MetaMiss = [];
    const opkSuccess = [];
    const opkMiss = [];
    const clockSuccess = [];
    const clockMiss = [];
    const stepsToRecover = [];
    const stepsToMiss = [];
    const repairEfficiency = [];

    for (const event of events) {
      if (event.recovered && event.windowCounts) {
        p5MetaSuccess.push(event.windowCounts.p5Meta ?? 0);
        opkSuccess.push(event.windowCounts.opk ?? 0);
        clockSuccess.push(event.windowCounts.clock ?? 0);
        stepsToRecover.push(event.stepsToOutcome ?? 0);
        if ((event.windowCounts.p5Meta ?? 0) > 0 && event.stepsToOutcome != null) {
          repairEfficiency.push(event.stepsToOutcome / event.windowCounts.p5Meta);
        }
      } else if (event.miss && event.windowCounts) {
        p5MetaMiss.push(event.windowCounts.p5Meta ?? 0);
        opkMiss.push(event.windowCounts.opk ?? 0);
        clockMiss.push(event.windowCounts.clock ?? 0);
        stepsToMiss.push(event.stepsToOutcome ?? deadline);
      }
    }

    const epTotal = sim.ep_exact_total();
    const epByMove = sim.ep_exact_by_move();
    const epClock = epByMove[10] ?? 0;
    const epRepair = (epByMove[7] ?? 0) + (epByMove[8] ?? 0);
    const epOpK = epByMove[9] ?? 0;
    const epNoise = 0;
    const epOther = epTotal - epClock - epRepair - epOpK;
    const finalCounts = readCounts();
    const repairRate = steps > 0 ? (finalCounts[MOVE_P5_META] ?? 0) / steps : 0;
    const opkRate = steps > 0 ? (finalCounts[MOVE_OPK] ?? 0) / steps : 0;

    runs.push({
      seed,
      missFrac: events.length > 0 ? misses / events.length : 0,
      recoveryMean: recoveries.length > 0 ? mean(recoveries) : null,
      recoveryP95: percentile(recoveries, 0.95),
      recoveryMax: recoveries.length > 0 ? Math.max(...recoveries) : null,
      p5MetaToRecoverMean: p5MetaRecovered.length > 0 ? mean(p5MetaRecovered) : null,
      p5MetaToRecoverP95: percentile(p5MetaRecovered, 0.95),
      opkToRecoverMean: opkRecovered.length > 0 ? mean(opkRecovered) : null,
      opkToRecoverP95: percentile(opkRecovered, 0.95),
      p5MetaToRecoverSuccessMean: p5MetaSuccess.length > 0 ? mean(p5MetaSuccess) : null,
      p5MetaToRecoverSuccessP95: percentile(p5MetaSuccess, 0.95),
      p5MetaBeforeMissMean: p5MetaMiss.length > 0 ? mean(p5MetaMiss) : null,
      p5MetaBeforeMissP95: percentile(p5MetaMiss, 0.95),
      opkToRecoverSuccessMean: opkSuccess.length > 0 ? mean(opkSuccess) : null,
      opkToRecoverSuccessP95: percentile(opkSuccess, 0.95),
      opkBeforeMissMean: opkMiss.length > 0 ? mean(opkMiss) : null,
      opkBeforeMissP95: percentile(opkMiss, 0.95),
      clockToRecoverSuccessMean: clockSuccess.length > 0 ? mean(clockSuccess) : null,
      clockToRecoverSuccessP95: percentile(clockSuccess, 0.95),
      clockBeforeMissMean: clockMiss.length > 0 ? mean(clockMiss) : null,
      clockBeforeMissP95: percentile(clockMiss, 0.95),
      recoveriesCount: p5MetaSuccess.length,
      missesCount: p5MetaMiss.length,
      repairEfficiencySuccessMean: repairEfficiency.length > 0 ? mean(repairEfficiency) : null,
      repairEfficiencySuccessMedian:
        repairEfficiency.length > 0 ? percentile(repairEfficiency, 0.5) : null,
      repairRate,
      opkRate,
      uptime: sampleCount > 0 ? uptimeGood / sampleCount : 0,
      uptimeTail: tailUptime,
      errTailMean: tailErrMean,
      sdiffTailMean: tailSdiffMean,
      errP95,
      errEnd,
      sdiffEnd,
      epTotal,
      epClock,
      epRepair,
      epOpK,
      epNoise,
      epOther,
      epTotalRate: steps > 0 ? epTotal / steps : 0,
      epClockRate: steps > 0 ? epClock / steps : 0,
      epRepairRate: steps > 0 ? epRepair / steps : 0,
      epOpKRate: steps > 0 ? epOpK / steps : 0,
      epNoiseRate: 0,
      epOtherRate: steps > 0 ? epOther / steps : 0,
      events: events.length,
      eventOutcomes: includeEvents
        ? events.map((event) => ({
            success: !!event.recovered,
            miss: !!event.miss,
            repairsUsed:
              event.p5MetaAcceptedToOutcome == null
                ? Number.POSITIVE_INFINITY
                : event.p5MetaAcceptedToOutcome,
            stepsUsed: event.stepsToOutcome ?? null,
          }))
        : null,
    });
  }

  return { params, runs };
}
</file>

<file path="scripts/run-clock-code-joint-sweep.mjs">
#!/usr/bin/env node
import fs from "node:fs";
import path from "node:path";
import { fileURLToPath, pathToFileURL } from "node:url";

const __filename = fileURLToPath(import.meta.url);
const __dirname = path.dirname(__filename);
const rootDir = path.resolve(__dirname, "..");
const outDir = path.resolve(rootDir, ".tmp", "clock_code");

const wasmDir = path.resolve(rootDir, "apps/web/src/wasm/sim_core");
const wasmJs = pathToFileURL(path.join(wasmDir, "sim_core.js")).href;
const wasmBytes = fs.readFileSync(path.join(wasmDir, "sim_core_bg.wasm"));
const mod = await import(wasmJs);
mod.initSync({ module: wasmBytes });

const steps = 1_500_000;
const reportEvery = 100_000;
const perturbStep = 750_000;
const seeds = Array.from({ length: 10 }, (_, i) => i + 1);
const mus = [0.2, 0.4, 0.6, 0.8, 1.0, 1.4];
const MOVE_P5_BASE = 7;
const MOVE_P5_META = 8;
const MOVE_CLOCK = 10;

function ensureDir(dir) {
  fs.mkdirSync(dir, { recursive: true });
}

function mean(values) {
  return values.reduce((acc, v) => acc + v, 0) / values.length;
}

function variance(values, meanVal) {
  return values.reduce((acc, v) => acc + (v - meanVal) ** 2, 0) / values.length;
}

function median(values) {
  const sorted = [...values].sort((a, b) => a - b);
  const mid = Math.floor(sorted.length / 2);
  if (sorted.length % 2 === 0) {
    return 0.5 * (sorted[mid - 1] + sorted[mid]);
  }
  return sorted[mid];
}

function percentile(values, p) {
  const nums = values.filter((v) => Number.isFinite(v)).sort((a, b) => a - b);
  if (nums.length === 0) return null;
  const idx = Math.min(nums.length - 1, Math.floor(p * (nums.length - 1)));
  return nums[idx];
}

function meanAbsDiff(a, b) {
  let sum = 0;
  for (let i = 0; i < a.length; i += 1) {
    sum += Math.abs(a[i] - b[i]);
  }
  return a.length === 0 ? 0 : sum / a.length;
}

function quadrantIndex(idx, g) {
  const x = idx % g;
  const y = Math.floor(idx / g);
  const qx = x < g / 2 ? 0 : 1;
  const qy = y < g / 2 ? 0 : 1;
  return qy * 2 + qx;
}

function logicalBitsFromField(field, g, lS, mask) {
  const sums = [0, 0, 0, 0];
  const counts = [0, 0, 0, 0];
  for (let i = 0; i < field.length; i += 1) {
    if (mask && !mask[i]) continue;
    const q = quadrantIndex(i, g);
    sums[q] += field[i];
    counts[q] += 1;
  }
  const threshold = lS / 2;
  return sums.map((sum, i) => {
    const meanVal = counts[i] > 0 ? sum / counts[i] : 0;
    return meanVal >= threshold ? 1 : 0;
  });
}

function errorRate(bitsA, bitsB) {
  let mismatches = 0;
  for (let i = 0; i < bitsA.length; i += 1) {
    if (bitsA[i] !== bitsB[i]) mismatches += 1;
  }
  return mismatches / bitsA.length;
}

function makeMask(seed, size, frac) {
  let x = seed >>> 0;
  if (x === 0) x = 1;
  const mask = new Array(size);
  for (let i = 0; i < size; i += 1) {
    x ^= x << 13;
    x ^= x >>> 17;
    x ^= x << 5;
    const r = (x >>> 8) / (1 << 24);
    mask[i] = r < frac;
  }
  return mask;
}

function errF05(baseBits, metaField, g, lS, seed) {
  const trials = 20;
  let acc = 0;
  for (let t = 0; t < trials; t += 1) {
    const mask = makeMask(seed + t * 101, metaField.length, 0.5);
    const bits = logicalBitsFromField(metaField, g, lS, mask);
    acc += errorRate(bits, baseBits);
  }
  return acc / trials;
}

function spearman(xs, ys) {
  const rank = (arr) => {
    const sorted = arr
      .map((v, i) => ({ v, i }))
      .sort((a, b) => a.v - b.v);
    const ranks = Array(arr.length);
    for (let i = 0; i < sorted.length; i += 1) {
      ranks[sorted[i].i] = i + 1;
    }
    return ranks;
  };
  const rx = rank(xs);
  const ry = rank(ys);
  const meanRx = mean(rx);
  const meanRy = mean(ry);
  let num = 0;
  let denx = 0;
  let deny = 0;
  for (let i = 0; i < rx.length; i += 1) {
    const dx = rx[i] - meanRx;
    const dy = ry[i] - meanRy;
    num += dx * dy;
    denx += dx * dx;
    deny += dy * dy;
  }
  return num / Math.sqrt(denx * deny);
}

ensureDir(outDir);

const rawPath = path.join(outDir, "clock_code_joint_raw.jsonl");
const summaryPath = path.join(outDir, "clock_code_joint_summary.csv");
const rawLines = [];
const summaryRows = [];

const baseParams = {
  beta: 2.0,
  stepSize: 0.01,
  p3On: 0,
  p6On: 1,
  p6SFactor: 0.0,
  pWrite: 0,
  pNWrite: 0.1,
  pAWrite: 0,
  pSWrite: 0.9,
  muHigh: 1.0,
  muLow: 1.0,
  kappaRep: 1.0,
  r0: 0.25,
  kappaBond: 0.0,
  rStar: 0.22,
  lambdaW: 0.0,
  lW: 4,
  lambdaN: 0.0,
  lN: 6,
  lambdaA: 0.0,
  lA: 6,
  lambdaS: 0.0,
  lS: 20,
  gridSize: 8,
  rPropose: 0.12,
  metaLayers: 2,
  eta: 0.0,
  etaDrive: 1.0,
  clockOn: 1,
  clockK: 8,
  clockFrac: 1.0,
  clockUsesP6: 1,
  repairClockGated: 1,
};

for (const mu of mus) {
  const qs = [];
  const sigmas = [];
  const relVars = [];
  const errs = [];
  const recoveries = [];
  const windowRates = [];
  const epClocks = [];
  const epRepairs = [];
  for (const seed of seeds) {
    const sim = new mod.Sim(50, seed);
    sim.set_params({ ...baseParams, muHigh: mu, muLow: mu });
    let lastEpExact = sim.ep_exact_total();
    let epExactWindow = 0;
    let preSamples = [];
    let baselineMean = null;
    let baselineTarget = null;
    let recoverySteps = null;
    let perturbApplied = false;

    for (let step = reportEvery; step <= steps; step += reportEvery) {
      sim.step(reportEvery);
      const baseS = sim.base_s_field();
      const metaS = sim.meta_field();
      const cells = baseS.length;
      const meta0 = metaS.subarray(0, cells);
      const sdiffBase = meanAbsDiff(baseS, meta0);
      if (!perturbApplied) {
        if (step <= perturbStep) preSamples.push(sdiffBase);
        if (step >= perturbStep) {
          baselineMean = mean(preSamples.slice(-3));
          baselineTarget = Math.max(baselineMean, 0.5);
          sim.apply_perturbation({
            target: "metaS",
            layer: 0,
            frac: 0.3,
            mode: "randomize",
            seed: seed * 1000 + 11,
          });
          perturbApplied = true;
        }
      } else if (recoverySteps === null && baselineTarget !== null) {
        if (sdiffBase <= baselineTarget * 1.1) {
          recoverySteps = step - perturbStep;
        }
      }

      const epExact = sim.ep_exact_total();
      epExactWindow = (epExact - lastEpExact) / reportEvery;
      lastEpExact = epExact;
    }

    const q = Number(sim.clock_q());
    const sigma = sim.ep_exact_total();
    const epByMove = sim.ep_exact_by_move();
    const epClock = epByMove[MOVE_CLOCK] ?? 0;
    const epRepair = (epByMove[MOVE_P5_BASE] ?? 0) + (epByMove[MOVE_P5_META] ?? 0);
    const baseS = sim.base_s_field();
    const metaS = sim.meta_field();
    const cells = baseS.length;
    const meta0 = metaS.subarray(0, cells);
    const baseBits = logicalBitsFromField(baseS, baseParams.gridSize, baseParams.lS);
    const err = errF05(baseBits, meta0, baseParams.gridSize, baseParams.lS, seed + 7000);

    qs.push(q);
    sigmas.push(sigma);
    errs.push(err);
    recoveries.push(recoverySteps ?? Infinity);
    windowRates.push(epExactWindow);
    epClocks.push(epClock);
    epRepairs.push(epRepair);

    rawLines.push(
      JSON.stringify({
        mu,
        seed,
        steps,
        clockQ: q,
        epExactTotal: sigma,
        epExactRateWindow: epExactWindow,
        epClock,
        epRepair,
        errF05: err,
        recoverySteps: Number.isFinite(recoverySteps) ? recoverySteps : null,
      }),
    );
  }

  const meanQ = mean(qs);
  const varQ = variance(qs, meanQ);
  const meanSigma = mean(sigmas);
  const relVar = meanQ !== 0 ? varQ / (meanQ * meanQ) : Infinity;
  const R = relVar * meanSigma / 2;
  const errMed = median(errs);
  const recMed = median(recoveries);
  const recP95 = percentile(recoveries, 0.95);
  const meanWindowRate = mean(windowRates);
  const meanEpClock = mean(epClocks);
  const meanEpRepair = mean(epRepairs);

  summaryRows.push({
    mu,
    meanSigma,
    meanWindowRate,
    meanQ,
    varQ,
    relVar,
    R,
    errMed,
    recMed,
    recP95,
    meanEpClock,
    meanEpRepair,
  });

}

fs.writeFileSync(rawPath, rawLines.join("\n"));
const header = [
  "mu",
  "meanSigma",
  "epExactRateWindowLast",
  "meanQ",
  "varQ",
  "relVar",
  "R",
  "errF05Median",
  "recoveryMedian",
  "recoveryP95",
  "meanEpClock",
  "meanEpRepair",
];
const csvLines = [
  header.join(","),
  ...summaryRows.map((row) =>
    [
      row.mu,
      row.meanSigma,
      row.meanWindowRate,
      row.meanQ,
      row.varQ,
      row.relVar,
      row.R,
      row.errMed,
      row.recMed,
      row.recP95 ?? "",
      row.meanEpClock,
      row.meanEpRepair,
    ].join(","),
  ),
];
fs.writeFileSync(summaryPath, csvLines.join("\n"));

const sigmaMono = summaryRows.reduce((acc, row, i) => {
  if (i === 0) return 0;
  return acc + (row.meanSigma >= summaryRows[i - 1].meanSigma ? 1 : 0);
}, 0);
const relVarMono = summaryRows.reduce((acc, row, i) => {
  if (i === 0) return 0;
  return acc + (row.relVar <= summaryRows[i - 1].relVar ? 1 : 0);
}, 0);
const errMono = summaryRows.reduce((acc, row, i) => {
  if (i === 0) return 0;
  return acc + (row.errMed <= summaryRows[i - 1].errMed ? 1 : 0);
}, 0);
const recMono = summaryRows.reduce((acc, row, i) => {
  if (i === 0) return 0;
  return acc + (row.recMed <= summaryRows[i - 1].recMed ? 1 : 0);
}, 0);

if (sigmaMono < 4) {
  throw new Error(`meanSigma monotonicity failed (count=${sigmaMono})`);
}
if (relVarMono < 4) {
  const eps = summaryRows.map((row) => row.meanSigma);
  const invRelVar = summaryRows.map((row) => 1 / row.relVar);
  const corrPrecision = spearman(eps, invRelVar);
  if (corrPrecision <= 0.7) {
    throw new Error(
      `relVar monotonicity failed (count=${relVarMono}), corr=${corrPrecision.toFixed(3)}`,
    );
  }
}
if (errMono < 4 && recMono < 4) {
  const eps = summaryRows.map((row) => row.meanSigma);
  const invRelVar = summaryRows.map((row) => 1 / row.relVar);
  const invErr = summaryRows.map((row) => 1 / row.errMed);
  const corrPrecision = spearman(eps, invRelVar);
  const corrCode = spearman(eps, invErr);
  if (!(corrPrecision > 0.7 && corrCode > 0.5)) {
    throw new Error(`Spearman correlations failed: precision ${corrPrecision}, code ${corrCode}`);
  }
}

console.log(`Joint sweep complete. Summary saved to ${summaryPath}`);
</file>

<file path="apps/web/src/sim/sim.worker.ts">
import type { SimMessage, SimRequest } from "./workerMessages";

let wasmMod: any | null = null;
let sim: any | null = null;
let running = false;
let bondThreshold = 2;
let pendingParams: any | null = null;

function post(message: SimMessage) {
  postMessage(message);
}

function debug(message: string) {
  post({ type: "debug", message });
}

debug("Worker bootstrap (WASM).");

async function ensureSim() {
  if (wasmMod) return wasmMod;
  debug("Loading WASM JS module…");
  const mod = await import("../wasm/sim_core/sim_core.js");
  debug("Initializing WASM module…");
  await mod.default();
  debug("WASM ready.");
  wasmMod = mod;
  return wasmMod;
}

async function tick(mod: any) {
  if (!running || !sim) return;
  const steps = 500;
  sim.step(steps);
  const snapshot = {
    n: sim.n(),
    positions: sim.positions(),
    bonds: sim.bonds(bondThreshold),
    counters: sim.counters(),
    apparatus: sim.apparatus(),
    field: sim.field(),
    metaLayers: sim.meta_layers(),
    metaField: sim.meta_field(),
    baseSField: sim.base_s_field(),
    metaNField: sim.meta_n_field(),
    metaAField: sim.meta_a_field(),
    metaWEdges: sim.meta_w_edges(),
    energy: sim.energy_breakdown(),
    diagnostics: sim.diagnostics(),
    steps,
  };
  post({ type: "snapshot", snapshot });
  setTimeout(() => void tick(mod), 16);
}

self.onmessage = async (ev: MessageEvent<SimRequest>) => {
  try {
    const req = ev.data;
    debug(`received: ${req.type}`);

    const mod = await ensureSim();

    if (req.type === "init") {
      debug("Constructing Sim…");
      sim = new mod.Sim(req.n, req.seed);
      if (pendingParams) {
        debug("Applying pending params…");
        sim.set_params(pendingParams);
        pendingParams = null;
      }
      debug("Sim constructed; posting ready + initial snapshot.");
      post({ type: "ready" });
      const snapshot = {
        n: sim.n(),
        positions: sim.positions(),
        bonds: sim.bonds(bondThreshold),
        counters: sim.counters(),
        apparatus: sim.apparatus(),
        field: sim.field(),
        metaLayers: sim.meta_layers(),
        metaField: sim.meta_field(),
        baseSField: sim.base_s_field(),
        metaNField: sim.meta_n_field(),
        metaAField: sim.meta_a_field(),
        metaWEdges: sim.meta_w_edges(),
        energy: sim.energy_breakdown(),
        diagnostics: sim.diagnostics(),
        steps: 0,
      };
      post({ type: "snapshot", snapshot });
      return;
    }

    if (req.type === "config") {
      bondThreshold = Math.max(0, Math.min(255, Math.floor(req.bondThreshold)));
      if (!sim) {
        pendingParams = req.params;
        debug("Stored config params (will apply on init).");
        return;
      }
      sim.set_params(req.params);
      debug(`Updated bondThreshold=${bondThreshold}`);
      const snapshot = {
        n: sim.n(),
        positions: sim.positions(),
        bonds: sim.bonds(bondThreshold),
        counters: sim.counters(),
        apparatus: sim.apparatus(),
        field: sim.field(),
        metaLayers: sim.meta_layers(),
        metaField: sim.meta_field(),
        baseSField: sim.base_s_field(),
        metaNField: sim.meta_n_field(),
        metaAField: sim.meta_a_field(),
        metaWEdges: sim.meta_w_edges(),
        energy: sim.energy_breakdown(),
        diagnostics: sim.diagnostics(),
        steps: 0,
      };
      post({ type: "snapshot", snapshot });
      return;
    }

    if (!sim) throw new Error("Simulation not initialized; send {type:'init'} first.");

    if (req.type === "step") {
      sim.step(req.steps);
      const snapshot = {
        n: sim.n(),
        positions: sim.positions(),
        bonds: sim.bonds(bondThreshold),
        counters: sim.counters(),
        apparatus: sim.apparatus(),
        field: sim.field(),
        metaLayers: sim.meta_layers(),
        metaField: sim.meta_field(),
        baseSField: sim.base_s_field(),
        metaNField: sim.meta_n_field(),
        metaAField: sim.meta_a_field(),
        metaWEdges: sim.meta_w_edges(),
        energy: sim.energy_breakdown(),
        diagnostics: sim.diagnostics(),
        steps: req.steps,
      };
      post({ type: "snapshot", snapshot });
      return;
    }

    if (req.type === "resume") {
      if (!running) {
        running = true;
        void tick(mod);
      }
      return;
    }

    if (req.type === "pause") {
      running = false;
      return;
    }
  } catch (err) {
    const message = err instanceof Error ? err.message : String(err);
    post({ type: "error", message });
  }
};
</file>

<file path="apps/web/src/sim/workerMessages.ts">
export type SimInitRequest = { type: "init"; n: number; seed: number };
export type SimStepRequest = { type: "step"; steps: number };
export type SimParams = {
  beta: number;
  stepSize: number;
  p3On: number;
  p6On: number;
  pWrite: number;
  pNWrite: number;
  pAWrite: number;
  pSWrite: number;
  muHigh: number;
  muLow: number;
  kappaRep: number;
  r0: number;
  kappaBond: number;
  rStar: number;
  lambdaW: number;
  lW: number;
  lambdaN: number;
  lN: number;
  lambdaA: number;
  lA: number;
  lambdaS: number;
  lS: number;
  gridSize: number;
  rPropose: number;
  metaLayers: number;
  eta: number;
};

export type SimConfigRequest = { type: "config"; bondThreshold: number; params: SimParams };
export type SimPauseRequest = { type: "pause" };
export type SimResumeRequest = { type: "resume" };
export type SimRequest =
  | SimInitRequest
  | SimStepRequest
  | SimConfigRequest
  | SimPauseRequest
  | SimResumeRequest;

export type EnergyBreakdown = {
  total: number;
  uRep: number;
  uBond: number;
  eW: number;
  eN: number;
  eA: number;
  eS: number;
};

export type Diagnostics = {
  wPlus: number;
  wMinus: number;
  nPlus: number;
  nMinus: number;
  aPlus: number;
  aMinus: number;
  sPlus: number;
  sMinus: number;
  window: number;
  jW: number;
  aW: number;
  jN: number;
  aN: number;
  jA: number;
  aA: number;
  jS: number;
  aS: number;
  p3CycleLen: number;
  p3DispX: number;
  p3DispY: number;
  p3DispMag: number;
  p3LoopArea: number;
  aM6W: number;
  aM6N: number;
  aM6A: number;
  aM6S: number;
  sigmaMem: number;
  wHist: Uint32Array;
  sHist: Uint32Array;
};

export type SimSnapshot = {
  n: number;
  positions: Float32Array;
  bonds: Uint32Array; // [i0,j0,i1,j1,...] for w_ij >= threshold
  counters: Int16Array;
  apparatus: Uint16Array;
  field: Uint8Array;
  metaLayers: number;
  metaField: Uint8Array;
  baseSField: Uint8Array;
  metaNField: Int16Array;
  metaAField: Uint16Array;
  metaWEdges: Uint8Array;
  energy: EnergyBreakdown;
  diagnostics: Diagnostics;
  steps: number;
};
export type SimSnapshotMessage = { type: "snapshot"; snapshot: SimSnapshot };
export type SimReadyMessage = { type: "ready" };
export type SimErrorMessage = { type: "error"; message: string };
export type SimDebugMessage = { type: "debug"; message: string };
export type SimMessage = SimSnapshotMessage | SimReadyMessage | SimErrorMessage | SimDebugMessage;
</file>

<file path="apps/web/src/App.tsx">
import React, { useEffect, useRef, useState } from "react";
import { SimWorkerClient } from "./sim/workerClient";
import {
  addSnapshot,
  attachToWindow,
  exportRun,
  setCaptureEverySteps,
  startRun,
} from "./sim/runCache";
import type { Diagnostics, EnergyBreakdown, SimParams } from "./sim/workerMessages";

// Create worker as module-level singleton to avoid React StrictMode double-creation
let sharedClient: SimWorkerClient | null = null;
function getClient(): SimWorkerClient {
  if (!sharedClient) {
    sharedClient = new SimWorkerClient();
  }
  return sharedClient;
}

function drawFrame(
  canvas: HTMLCanvasElement,
  positions: Float32Array,
  bonds: Uint32Array,
  counters?: Int16Array | Uint16Array,
  field?: ArrayLike<number>,
  fieldMax?: number
) {
  const dpr = Math.max(1, Math.floor(window.devicePixelRatio || 1));
  const size = Math.min(canvas.clientWidth, canvas.clientHeight);
  const px = Math.max(1, Math.floor(size * dpr));
  if (canvas.width !== px || canvas.height !== px) {
    canvas.width = px;
    canvas.height = px;
  }

  const ctx = canvas.getContext("2d");
  if (!ctx) return;

  ctx.clearRect(0, 0, canvas.width, canvas.height);

  // Bonds (P1) underlay: draw exactly the bonds returned by the simulation.
  ctx.lineWidth = Math.max(1, Math.floor(1 * dpr));
  ctx.strokeStyle = "rgba(160, 190, 255, 0.22)";
  ctx.beginPath();
  for (let k = 0; k < bonds.length; k += 2) {
    const i = bonds[k]!;
    const j = bonds[k + 1]!;
    let dx = positions[2 * i]! - positions[2 * j]!;
    let dy = positions[2 * i + 1]! - positions[2 * j + 1]!;
    let shiftX = 0;
    let shiftY = 0;
    if (dx > 0.5) {
      dx -= 1.0;
      shiftX = 1;
    } else if (dx < -0.5) {
      dx += 1.0;
      shiftX = -1;
    }
    if (dy > 0.5) {
      dy -= 1.0;
      shiftY = 1;
    } else if (dy < -0.5) {
      dy += 1.0;
      shiftY = -1;
    }
    const xi = positions[2 * i]!;
    const yi = positions[2 * i + 1]!;
    const xj = positions[2 * j]!;
    const yj = positions[2 * j + 1]!;
    // Draw the shortest wrapped segment (may extend outside canvas).
    ctx.moveTo(xi * canvas.width, yi * canvas.height);
    ctx.lineTo((xj + shiftX) * canvas.width, (yj + shiftY) * canvas.height);
    // If wrapped, also draw the complementary segment on the opposite side.
    if (shiftX !== 0 || shiftY !== 0) {
      ctx.moveTo((xi - shiftX) * canvas.width, (yi - shiftY) * canvas.height);
      ctx.lineTo(xj * canvas.width, yj * canvas.height);
    }
  }
  ctx.stroke();

  if (field && field.length > 0) {
    const g = Math.round(Math.sqrt(field.length));
    if (g * g === field.length) {
      const max = Math.max(1, fieldMax ?? 1);
      const cellW = canvas.width / g;
      const cellH = canvas.height / g;
      for (let y = 0; y < g; y++) {
        for (let x = 0; x < g; x++) {
          const idx = y * g + x;
          const v = field[idx] ?? 0;
          if (v === 0) continue;
          const t = Math.min(1, v / max);
          ctx.fillStyle = `rgba(120, 200, 120, ${0.08 + 0.22 * t})`;
          ctx.fillRect(x * cellW, y * cellH, cellW, cellH);
        }
      }
    }
  }

  const r = Math.max(2, Math.floor(2 * dpr));
  let maxAbs = 0;
  if (counters && counters.length > 0) {
    for (let i = 0; i < counters.length; i++) {
      const v = Math.abs(Number(counters[i]!));
      if (v > maxAbs) maxAbs = v;
    }
  }
  const hasCounters = counters && counters.length * 2 === positions.length && maxAbs > 0;
  const negColor = [90, 160, 255];
  const zeroColor = [210, 225, 255];
  const posColor = [255, 150, 100];
  const overlayIsUnsigned = counters instanceof Uint16Array;

  for (let i = 0; i < positions.length; i += 2) {
    const x = positions[i] * canvas.width;
    const y = positions[i + 1] * canvas.height;
    if (hasCounters && counters) {
      const n = Number(counters[i / 2]!);
      const t = Math.max(-1, Math.min(1, n / maxAbs));
      let rC = zeroColor[0]!;
      let gC = zeroColor[1]!;
      let bC = zeroColor[2]!;
      if (overlayIsUnsigned || t >= 0) {
        const k = Math.abs(t);
        rC = Math.round(zeroColor[0]! * (1 - k) + posColor[0]! * k);
        gC = Math.round(zeroColor[1]! * (1 - k) + posColor[1]! * k);
        bC = Math.round(zeroColor[2]! * (1 - k) + posColor[2]! * k);
      } else if (t < 0) {
        const k = -t;
        rC = Math.round(zeroColor[0]! * (1 - k) + negColor[0]! * k);
        gC = Math.round(zeroColor[1]! * (1 - k) + negColor[1]! * k);
        bC = Math.round(zeroColor[2]! * (1 - k) + negColor[2]! * k);
      }
      ctx.fillStyle = `rgba(${rC}, ${gC}, ${bC}, 0.9)`;
    } else {
      ctx.fillStyle = "rgba(106, 169, 255, 0.85)";
    }
    ctx.beginPath();
    ctx.arc(x, y, r, 0, Math.PI * 2);
    ctx.fill();
  }
}

function normalizeUnsignedField(source: Uint16Array, denom: number): Float32Array {
  const d = Math.max(1, denom);
  const out = new Float32Array(source.length);
  for (let i = 0; i < source.length; i++) {
    out[i] = source[i]! / d;
  }
  return out;
}

function normalizeSignedField(source: Int16Array, denom: number): Float32Array {
  const d = Math.max(1, denom);
  const out = new Float32Array(source.length);
  for (let i = 0; i < source.length; i++) {
    out[i] = (source[i]! + d) / (2 * d);
  }
  return out;
}

function metaWEdgesToCells(source: Uint8Array, grid: number): Float32Array {
  const cells = grid * grid;
  const out = new Float32Array(cells);
  for (let y = 0; y < grid; y++) {
    const upY = (y + grid - 1) % grid;
    for (let x = 0; x < grid; x++) {
      const leftX = (x + grid - 1) % grid;
      const q = y * grid + x;
      const left = y * grid + leftX;
      const up = upY * grid + x;
      const h0 = source[q] ?? 0;
      const h1 = source[left] ?? 0;
      const v0 = source[cells + q] ?? 0;
      const v1 = source[cells + up] ?? 0;
      out[q] = (h0 + h1 + v0 + v1) / 4;
    }
  }
  return out;
}

type GraphStats = {
  edges: number;
  components: number;
  largest: number;
  sizes: number[];
};

type History = Record<string, number[]>;

type ChartConfig = {
  id: string;
  label: string;
  color: string;
  group: "P1" | "P2" | "P3" | "P4" | "P5" | "System" | "Graph";
  centerZero?: boolean;
  value: (ctx: ChartContext) => number;
};

type ChartContext = {
  diagnostics: Diagnostics;
  energy: EnergyBreakdown;
  stats: GraphStats;
  safeSet: SafeSetStats;
  n: number;
};

type HistogramConfig = {
  id: string;
  label: string;
  color: string;
  group: "P1" | "P2" | "P3" | "P4" | "P5" | "System" | "Graph";
  bins: (ctx: ChartContext) => number[];
};

const CHARTS: ChartConfig[] = [
  {
    id: "jw",
    label: "Jw (flux)",
    color: "rgba(122, 187, 255, 0.85)",
    group: "P1",
    centerZero: true,
    value: (ctx) => ctx.diagnostics.jW,
  },
  {
    id: "aw",
    label: "Aw (affinity)",
    color: "rgba(180, 210, 255, 0.85)",
    group: "P1",
    centerZero: true,
    value: (ctx) => ctx.diagnostics.aW,
  },
  {
    id: "ja",
    label: "Ja (flux)",
    color: "rgba(150, 200, 255, 0.85)",
    group: "P2",
    centerZero: true,
    value: (ctx) => ctx.diagnostics.jA,
  },
  {
    id: "aa",
    label: "Aa (affinity)",
    color: "rgba(190, 220, 255, 0.85)",
    group: "P2",
    centerZero: true,
    value: (ctx) => ctx.diagnostics.aA,
  },
  {
    id: "jn",
    label: "Jn (flux)",
    color: "rgba(140, 220, 200, 0.85)",
    group: "P4",
    centerZero: true,
    value: (ctx) => ctx.diagnostics.jN,
  },
  {
    id: "an",
    label: "An (affinity)",
    color: "rgba(180, 230, 210, 0.85)",
    group: "P4",
    centerZero: true,
    value: (ctx) => ctx.diagnostics.aN,
  },
  {
    id: "js",
    label: "Js (flux)",
    color: "rgba(170, 200, 140, 0.85)",
    group: "P5",
    centerZero: true,
    value: (ctx) => ctx.diagnostics.jS,
  },
  {
    id: "as",
    label: "As (affinity)",
    color: "rgba(200, 220, 160, 0.85)",
    group: "P5",
    centerZero: true,
    value: (ctx) => ctx.diagnostics.aS,
  },
  {
    id: "p3Disp",
    label: "P3 disp",
    color: "rgba(240, 210, 80, 0.85)",
    group: "P3",
    value: (ctx) => ctx.diagnostics.p3DispMag,
  },
  {
    id: "p3Loop",
    label: "P3 loop",
    color: "rgba(240, 190, 60, 0.85)",
    group: "P3",
    centerZero: true,
    value: (ctx) => ctx.diagnostics.p3LoopArea,
  },
  {
    id: "m6w",
    label: "M6 W",
    color: "rgba(255, 130, 130, 0.85)",
    group: "System",
    centerZero: true,
    value: (ctx) => ctx.diagnostics.aM6W,
  },
  {
    id: "m6n",
    label: "M6 N",
    color: "rgba(255, 150, 150, 0.85)",
    group: "System",
    centerZero: true,
    value: (ctx) => ctx.diagnostics.aM6N,
  },
  {
    id: "m6a",
    label: "M6 A",
    color: "rgba(255, 170, 170, 0.85)",
    group: "System",
    centerZero: true,
    value: (ctx) => ctx.diagnostics.aM6A,
  },
  {
    id: "m6s",
    label: "M6 S",
    color: "rgba(255, 190, 190, 0.85)",
    group: "System",
    centerZero: true,
    value: (ctx) => ctx.diagnostics.aM6S,
  },
  {
    id: "safeFrac",
    label: "Safe area",
    color: "rgba(120, 210, 160, 0.85)",
    group: "P5",
    value: (ctx) => ctx.safeSet.fraction,
  },
  {
    id: "safeLargest",
    label: "Safe largest",
    color: "rgba(160, 230, 120, 0.85)",
    group: "P5",
    value: (ctx) => ctx.safeSet.largestFrac,
  },
  {
    id: "safeCount",
    label: "Safe components",
    color: "rgba(200, 220, 120, 0.85)",
    group: "P5",
    value: (ctx) => ctx.safeSet.components,
  },
  {
    id: "sigma",
    label: "Sigma (mem)",
    color: "rgba(255, 203, 120, 0.85)",
    group: "System",
    centerZero: true,
    value: (ctx) => ctx.diagnostics.sigmaMem,
  },
  {
    id: "edges",
    label: "Edges",
    color: "rgba(140, 245, 200, 0.85)",
    group: "Graph",
    value: (ctx) => ctx.stats.edges,
  },
  {
    id: "largest",
    label: "Largest frac",
    color: "rgba(255, 160, 160, 0.85)",
    group: "Graph",
    value: (ctx) => ctx.stats.largest / ctx.n,
  },
  {
    id: "energy",
    label: "Energy",
    color: "rgba(200, 180, 255, 0.85)",
    group: "System",
    value: (ctx) => ctx.energy.total,
  },
];

const HISTOGRAMS: HistogramConfig[] = [
  {
    id: "wHist",
    label: "Bond weights",
    color: "rgba(120, 170, 255, 0.85)",
    group: "P1",
    bins: (ctx) => Array.from(ctx.diagnostics.wHist),
  },
  {
    id: "compHist",
    label: "Component sizes",
    color: "rgba(255, 180, 140, 0.85)",
    group: "Graph",
    bins: (ctx) => componentSizeHistogram(ctx.stats.sizes, ctx.n, 12),
  },
  {
    id: "sHist",
    label: "Field levels",
    color: "rgba(170, 200, 140, 0.85)",
    group: "P5",
    bins: (ctx) => Array.from(ctx.diagnostics.sHist),
  },
];

type SafeSetStats = {
  fraction: number;
  components: number;
  largestFrac: number;
};

function pushHistory(series: number[], value: number) {
  series.push(value);
}

function drawSparkline(
  canvas: HTMLCanvasElement,
  series: number[],
  color: string,
  options: { centerZero?: boolean } = {}
) {
  const dpr = Math.max(1, Math.floor(window.devicePixelRatio || 1));
  const w = Math.max(1, Math.floor(canvas.clientWidth * dpr));
  const h = Math.max(1, Math.floor(canvas.clientHeight * dpr));
  if (canvas.width !== w || canvas.height !== h) {
    canvas.width = w;
    canvas.height = h;
  }
  const ctx = canvas.getContext("2d");
  if (!ctx) return;

  ctx.clearRect(0, 0, w, h);
  ctx.strokeStyle = "rgba(255,255,255,0.08)";
  ctx.lineWidth = 1;
  ctx.beginPath();
  ctx.moveTo(0, h - 1);
  ctx.lineTo(w, h - 1);
  ctx.stroke();

  if (series.length < 2) {
    return;
  }

  let min = Infinity;
  let max = -Infinity;
  for (const v of series) {
    if (v < min) min = v;
    if (v > max) max = v;
  }
  if (options.centerZero) {
    const abs = Math.max(Math.abs(min), Math.abs(max), 1e-6);
    min = -abs;
    max = abs;
  }
  if (Math.abs(max - min) < 1e-9) {
    max = min + 1e-6;
  }

  ctx.strokeStyle = color;
  ctx.lineWidth = Math.max(1, Math.floor(1 * dpr));
  ctx.beginPath();
  const count = series.length;
  const cols = Math.max(1, w);
  const samples = Math.min(count, cols);
  ctx.beginPath();
  if (count <= cols) {
    for (let i = 0; i < count; i++) {
      const t = series[i]!;
      const x = (i / Math.max(1, count - 1)) * (w - 1);
      const y = h - 1 - ((t - min) / (max - min)) * (h - 2);
      if (i === 0) {
        ctx.moveTo(x, y);
      } else {
        ctx.lineTo(x, y);
      }
    }
  } else {
    for (let s = 0; s < samples; s++) {
      const start = Math.floor((s * count) / samples);
      const end = Math.max(start + 1, Math.floor(((s + 1) * count) / samples));
      let sum = 0;
      for (let i = start; i < end; i++) {
        sum += series[i]!;
      }
      const avg = sum / (end - start);
      const x = (s / Math.max(1, samples - 1)) * (w - 1);
      const y = h - 1 - ((avg - min) / (max - min)) * (h - 2);
      if (s === 0) {
        ctx.moveTo(x, y);
      } else {
        ctx.lineTo(x, y);
      }
    }
  }
  ctx.stroke();

  if (options.centerZero && min < 0 && max > 0) {
    const y0 = h - 1 - ((0 - min) / (max - min)) * (h - 2);
    ctx.strokeStyle = "rgba(255,255,255,0.15)";
    ctx.lineWidth = 1;
    ctx.beginPath();
    ctx.moveTo(0, y0);
    ctx.lineTo(w, y0);
    ctx.stroke();
  }
}

function drawHistogram(canvas: HTMLCanvasElement, bins: number[], color: string) {
  const dpr = Math.max(1, Math.floor(window.devicePixelRatio || 1));
  const w = Math.max(1, Math.floor(canvas.clientWidth * dpr));
  const h = Math.max(1, Math.floor(canvas.clientHeight * dpr));
  if (canvas.width !== w || canvas.height !== h) {
    canvas.width = w;
    canvas.height = h;
  }
  const ctx = canvas.getContext("2d");
  if (!ctx) return;

  ctx.clearRect(0, 0, w, h);
  if (bins.length === 0) return;

  let max = 0;
  for (const v of bins) {
    if (v > max) max = v;
  }
  if (max <= 0) return;

  const barW = w / bins.length;
  ctx.fillStyle = color;
  for (let i = 0; i < bins.length; i++) {
    const v = bins[i]!;
    const barH = (v / max) * (h - 2);
    const x = i * barW;
    const y = h - 1 - barH;
    ctx.fillRect(x + 0.5, y, Math.max(1, barW - 1), barH);
  }
  ctx.strokeStyle = "rgba(255,255,255,0.08)";
  ctx.lineWidth = 1;
  ctx.beginPath();
  ctx.moveTo(0, h - 1);
  ctx.lineTo(w, h - 1);
  ctx.stroke();
}

function componentSizeHistogram(sizes: number[], n: number, bins: number): number[] {
  const out = new Array(bins).fill(0);
  if (n <= 0 || bins <= 0) return out;
  for (const size of sizes) {
    const frac = size / n;
    const idx = Math.min(bins - 1, Math.floor(frac * bins));
    out[idx] += 1;
  }
  return out;
}

function computeSafeSetStats(field: Uint8Array, threshold: number): SafeSetStats {
  const total = field.length;
  if (total === 0) {
    return { fraction: 0, components: 0, largestFrac: 0 };
  }
  const g = Math.round(Math.sqrt(total));
  if (g * g !== total) {
    return { fraction: 0, components: 0, largestFrac: 0 };
  }
  const visited = new Uint8Array(total);
  let safeCount = 0;
  let components = 0;
  let largest = 0;

  const queueX = new Int16Array(total);
  const queueY = new Int16Array(total);

  const inBounds = (x: number, y: number) => x >= 0 && y >= 0 && x < g && y < g;

  for (let y = 0; y < g; y++) {
    for (let x = 0; x < g; x++) {
      const idx = y * g + x;
      if (field[idx]! < threshold) continue;
      safeCount += 1;
      if (visited[idx]) continue;
      components += 1;
      let head = 0;
      let tail = 0;
      queueX[tail] = x;
      queueY[tail] = y;
      tail += 1;
      visited[idx] = 1;
      let compSize = 0;
      while (head < tail) {
        const cx = queueX[head]!;
        const cy = queueY[head]!;
        head += 1;
        compSize += 1;
        const neighbors = [
          [cx + 1, cy],
          [cx - 1, cy],
          [cx, cy + 1],
          [cx, cy - 1],
        ];
        for (const [nx, ny] of neighbors) {
          if (!inBounds(nx, ny)) continue;
          const nidx = ny * g + nx;
          if (visited[nidx]) continue;
          if (field[nidx]! < threshold) continue;
          visited[nidx] = 1;
          queueX[tail] = nx;
          queueY[tail] = ny;
          tail += 1;
        }
      }
      if (compSize > largest) {
        largest = compSize;
      }
    }
  }

  return {
    fraction: safeCount / total,
    components,
    largestFrac: total > 0 ? largest / total : 0,
  };
}

function computeGraphStats(n: number, bonds: Uint32Array): GraphStats {
  const parent = new Int32Array(n);
  const size = new Int32Array(n);
  for (let i = 0; i < n; i++) {
    parent[i] = i;
    size[i] = 1;
  }

  const find = (x: number): number => {
    let p = parent[x]!;
    if (p !== x) {
      parent[x] = find(p);
    }
    return parent[x]!;
  };

  const union = (a: number, b: number) => {
    let ra = find(a);
    let rb = find(b);
    if (ra === rb) return;
    if (size[ra]! < size[rb]!) {
      [ra, rb] = [rb, ra];
    }
    parent[rb] = ra;
    size[ra] += size[rb]!;
  };

  for (let k = 0; k < bonds.length; k += 2) {
    union(bonds[k]!, bonds[k + 1]!);
  }

  let components = 0;
  let largest = 0;
  const sizes: number[] = [];
  for (let i = 0; i < n; i++) {
    if (parent[i] === i) {
      components += 1;
      const s = size[i]!;
      sizes.push(s);
      if (s > largest) {
        largest = s;
      }
    }
  }

  return { edges: bonds.length / 2, components, largest, sizes };
}

const DEFAULT_PARAMS: SimParams = {
  beta: 1.0,
  stepSize: 0.01,
  p3On: 0,
  p6On: 0,
  pWrite: 0.1,
  pNWrite: 0.05,
  pAWrite: 0.05,
  pSWrite: 0.05,
  muHigh: 0.6,
  muLow: -0.6,
  kappaRep: 500.0,
  r0: 0.25,
  kappaBond: 1.2,
  rStar: 0.22,
  lambdaW: 0.3,
  lW: 4,
  lambdaN: 0.5,
  lN: 6,
  lambdaA: 0.5,
  lA: 6,
  lambdaS: 0.5,
  lS: 6,
  gridSize: 16,
  rPropose: 0.12,
  metaLayers: 0,
  eta: 0.0,
};

export default function App() {
  const [n, setN] = useState(200);
  const [seed, setSeed] = useState(1);
  const [bondThreshold, setBondThreshold] = useState(3);
  const [energy, setEnergy] = useState<EnergyBreakdown | null>(null);
  const [diagnostics, setDiagnostics] = useState<Diagnostics | null>(null);
  const [graphStats, setGraphStats] = useState<GraphStats | null>(null);
  const [graphStatsN, setGraphStatsN] = useState<number | null>(null);
  const [recordEverySteps, setRecordEverySteps] = useState(2000);
  const [totalSteps, setTotalSteps] = useState(0);
  const [p1Enabled, setP1Enabled] = useState(true);
  const [p2Enabled, setP2Enabled] = useState(true);
  const [p4Enabled, setP4Enabled] = useState(true);
  const [p5Enabled, setP5Enabled] = useState(true);
  const [p3Enabled, setP3Enabled] = useState(false);
  const [p6Enabled, setP6Enabled] = useState(false);
  const [safeThreshold, setSafeThreshold] = useState(3);
  const [colorSource, setColorSource] = useState<"none" | "p4" | "p2">("p4");
  const [overlayChannel, setOverlayChannel] = useState<
    "none" | "baseS" | "metaS" | "metaN" | "metaA" | "metaW"
  >("none");
  const [overlayLayerIndex, setOverlayLayerIndex] = useState(0);
  const [status, setStatus] = useState<"idle" | "initializing" | "ready" | "running">("idle");
  const [error, setError] = useState<string | null>(null);
  const [initSlow, setInitSlow] = useState(false);
  const [metaSnapshot, setMetaSnapshot] = useState<{ layers: number; length: number } | null>(null);
  const [paramsDraft, setParamsDraft] = useState<SimParams>(DEFAULT_PARAMS);
  const [paramsApplied, setParamsApplied] = useState<SimParams>(DEFAULT_PARAMS);
  const [preset, setPreset] = useState<"sparse" | "balanced" | "dense" | "nullBaseline" | "p3p6Full">(
    "balanced",
  );
  const [expandedPanels, setExpandedPanels] = useState<Record<string, boolean>>({
    motion: false,
    p1: false,
    p2: false,
    p4: false,
    p5: false,
    p6: false,
    meta: false,
  });

  const canvasRef = useRef<HTMLCanvasElement | null>(null);
  const chartRefs = useRef<Record<string, HTMLCanvasElement | null>>({});
  const histRefs = useRef<Record<string, HTMLCanvasElement | null>>({});
  const historyRef = useRef<History>({});
  const chartStepRef = useRef(0);
  const chartGroups = useRef<Array<ChartConfig["group"]>>(["P1", "P2", "P3", "P4", "P5", "System", "Graph"]);

  const effectiveParams = (base: SimParams): SimParams => ({
    ...base,
    p3On: p3Enabled ? 1 : 0,
    p6On: p6Enabled ? 1 : 0,
    pWrite: p1Enabled ? base.pWrite : 0,
    pAWrite: p2Enabled ? base.pAWrite : 0,
    pNWrite: p4Enabled ? base.pNWrite : 0,
    pSWrite: p5Enabled ? base.pSWrite : 0,
  });

  // Use module-level singleton to avoid React StrictMode double-creation
  const client = getClient();

  useEffect(() => {
    attachToWindow();
    const offReady = client.onReady(() => {
      setStatus((s) => (s === "running" ? "running" : "ready"));
    });
    const offErr = client.onError((m) => setError(m));
    const offSnap = client.onSnapshot((s) => {
      const canvas = canvasRef.current;
      if (!canvas) return;
      const overlay =
        colorSource === "p2" ? s.apparatus : colorSource === "p4" ? s.counters : undefined;
      const grid = paramsApplied.gridSize;
      const cells = grid * grid;
      const canUseMeta = s.metaLayers > 0 && overlayLayerIndex >= 0 && overlayLayerIndex < s.metaLayers;
      let overlayField: ArrayLike<number> | undefined;
      let overlayMax: number | undefined;
      if (overlayChannel === "baseS") {
        overlayField = s.baseSField;
        overlayMax = paramsApplied.lS;
      } else if (overlayChannel === "metaS" && canUseMeta) {
        const start = overlayLayerIndex * cells;
        overlayField = s.metaField.subarray(start, start + cells);
        overlayMax = paramsApplied.lS;
      } else if (overlayChannel === "metaA" && canUseMeta) {
        const start = overlayLayerIndex * cells;
        overlayField = normalizeUnsignedField(s.metaAField.subarray(start, start + cells), paramsApplied.lA);
        overlayMax = 1;
      } else if (overlayChannel === "metaN" && canUseMeta) {
        const start = overlayLayerIndex * cells;
        overlayField = normalizeSignedField(s.metaNField.subarray(start, start + cells), paramsApplied.lN);
        overlayMax = 1;
      } else if (overlayChannel === "metaW" && canUseMeta) {
        const edges = 2 * cells;
        const start = overlayLayerIndex * edges;
        const slice = s.metaWEdges.subarray(start, start + edges);
        overlayField = metaWEdgesToCells(slice, grid);
        overlayMax = paramsApplied.lW;
      }
      drawFrame(
        canvas,
        s.positions,
        s.bonds,
        overlay,
        overlayField,
        overlayMax
      );
      setEnergy(s.energy);
      setDiagnostics(s.diagnostics);
      const stats = computeGraphStats(s.n, s.bonds);
      setGraphStats(stats);
      setGraphStatsN(s.n);
      const safeSet = computeSafeSetStats(s.baseSField, safeThreshold);
      setMetaSnapshot({ layers: s.metaLayers, length: s.metaField.length });
      if (s.steps > 0) {
        setTotalSteps((prev) => prev + s.steps);
      }

      chartStepRef.current += Math.max(0, s.steps);
      if (chartStepRef.current >= recordEverySteps) {
        chartStepRef.current = chartStepRef.current % recordEverySteps;
        const ctx: ChartContext = {
          diagnostics: s.diagnostics,
          energy: s.energy,
          stats,
          safeSet,
          n: s.n,
        };
        const history = historyRef.current;
        for (const chart of CHARTS) {
          const series = history[chart.id] ?? (history[chart.id] = []);
          pushHistory(series, chart.value(ctx));
          const chartCanvas = chartRefs.current[chart.id];
          if (chartCanvas) {
            drawSparkline(chartCanvas, series, chart.color, {
              centerZero: chart.centerZero,
            });
          }
        }
        for (const hist of HISTOGRAMS) {
          const bins = hist.bins(ctx);
          const histCanvas = histRefs.current[hist.id];
          if (histCanvas) {
            drawHistogram(histCanvas, bins, hist.color);
          }
        }
      }

      addSnapshot({
        n: s.n,
        energy: s.energy,
        diagnostics: s.diagnostics,
        graphStats: stats,
        positions: s.positions,
        bonds: s.bonds,
        counters: s.counters,
        apparatus: s.apparatus,
        field: s.baseSField,
        stepsDelta: s.steps,
      });
    });
    return () => {
      offReady();
      offErr();
      offSnap();
      // Don't terminate the worker on cleanup - we want it to survive StrictMode remounts
    };
  }, [client, colorSource, overlayChannel, overlayLayerIndex, paramsApplied, recordEverySteps, safeThreshold]);

  useEffect(() => {
    if (status === "initializing") {
      historyRef.current = {};
      chartStepRef.current = 0;
      setTotalSteps(0);
    }
  }, [status]);

  // Note: We don't terminate the singleton worker - it lives for the page lifetime

  useEffect(() => {
    if (status !== "initializing") {
      setInitSlow(false);
      return;
    }
    const t = window.setTimeout(() => setInitSlow(true), 5000);
    return () => window.clearTimeout(t);
  }, [status]);

  useEffect(() => {
    if (status === "idle" || status === "initializing") return;
    client.send({ type: "config", bondThreshold, params: paramsApplied });
  }, [bondThreshold, client, paramsApplied, status]);

  useEffect(() => {
    const layers = metaSnapshot?.layers ?? 0;
    const wantsMeta = overlayChannel.startsWith("meta");
    if (wantsMeta && layers === 0) {
      setOverlayChannel("none");
      setOverlayLayerIndex(0);
      return;
    }
    if (wantsMeta) {
      const maxLayer = Math.max(0, layers - 1);
      if (overlayLayerIndex > maxLayer) {
        setOverlayLayerIndex(maxLayer);
      } else if (overlayLayerIndex < 0) {
        setOverlayLayerIndex(0);
      }
    }
  }, [metaSnapshot, overlayChannel, overlayLayerIndex]);

  const canInit = status === "idle" || status === "ready";
  const canRun = status === "ready";
  const canPause = status === "running";
  const metaLayerCount = metaSnapshot?.layers ?? 0;

  return (
    <div className="app">
      <div className="panel">
        <h1>Ratchet Playground</h1>
        <p>
          Scaffold UI: worker + WASM sim core. Next: implement null-regime detailed balance kernels
          and Deliverable D diagnostics.
        </p>

        <div className="row">
          <div>
            <label>Particles (N)</label>
            <input
              type="number"
              min={10}
              max={5000}
              value={n}
              onChange={(e) => setN(Number(e.target.value))}
              disabled={!canInit}
            />
          </div>
          <div>
            <label>Seed</label>
            <input
              type="number"
              min={1}
              value={seed}
              onChange={(e) => setSeed(Number(e.target.value))}
              disabled={!canInit}
            />
          </div>
        </div>

        <div className="row">
          <button
            className="primary"
            onClick={() => {
              setError(null);
              setStatus("initializing");
              void startRun({
                n,
                seed,
                bondThreshold,
                params: paramsApplied,
                captureEverySteps: recordEverySteps,
              });
              setCaptureEverySteps(recordEverySteps);
              client.send({ type: "init", n, seed });
            }}
            disabled={!canInit}
          >
            Init
          </button>
          <button
            onClick={() => {
              setError(null);
              client.send({ type: "step", steps: 5000 });
            }}
            disabled={status === "idle" || status === "initializing"}
          >
            Step
          </button>
        </div>

        <div className="row">
          <div>
            <label>Bond threshold (w ≥)</label>
            <input
              type="number"
              min={0}
              max={255}
              value={bondThreshold}
              onChange={(e) => setBondThreshold(Number(e.target.value))}
              disabled={status === "idle" || status === "initializing"}
            />
          </div>
          <div />
        </div>

        <div className="row">
          <div>
            <label>Primitives</label>
            <div className="primitiveToggle">
              <label>
                <input
                  type="checkbox"
                  checked={p1Enabled}
                  onChange={(e) => setP1Enabled(e.target.checked)}
                  disabled={status === "initializing"}
                />
                <span className="legendSwatch legendSwatch--P1" aria-hidden />
                <span className="primitiveLabel">P1 bond write</span>
              </label>
              <label>
                <input
                  type="checkbox"
                  checked={p2Enabled}
                  onChange={(e) => setP2Enabled(e.target.checked)}
                  disabled={status === "initializing"}
                />
                <span className="legendSwatch legendSwatch--P2" aria-hidden />
                <span className="primitiveLabel">P2 apparatus</span>
              </label>
              <label>
                <input
                  type="checkbox"
                  checked={p3Enabled}
                  onChange={(e) => setP3Enabled(e.target.checked)}
                  disabled={status === "initializing"}
                />
                <span className="legendSwatch legendSwatch--P3" aria-hidden />
                <span className="primitiveLabel">P3 protocol</span>
              </label>
              <label>
                <input
                  type="checkbox"
                  checked={p6Enabled}
                  onChange={(e) => setP6Enabled(e.target.checked)}
                  disabled={status === "initializing"}
                />
                <span className="legendSwatch legendSwatch--P6" aria-hidden />
                <span className="primitiveLabel">P6 resource</span>
              </label>
              <label>
                <input
                  type="checkbox"
                  checked={p4Enabled}
                  onChange={(e) => setP4Enabled(e.target.checked)}
                  disabled={status === "initializing"}
                />
                <span className="legendSwatch legendSwatch--P4" aria-hidden />
                <span className="primitiveLabel">P4 counters</span>
              </label>
              <label>
                <input
                  type="checkbox"
                  checked={p5Enabled}
                  onChange={(e) => setP5Enabled(e.target.checked)}
                  disabled={status === "initializing"}
                />
                <span className="legendSwatch legendSwatch--P5" aria-hidden />
                <span className="primitiveLabel">P5 field</span>
              </label>
            </div>
          </div>
          <div />
        </div>

        <div className="row twoCol">
          <div>
            <label>Record every (steps)</label>
            <input
              type="number"
              min={1}
              max={100000}
              value={recordEverySteps}
              onChange={(e) => {
                const v = Math.max(1, Math.floor(Number(e.target.value)));
                setRecordEverySteps(v);
                setCaptureEverySteps(v);
              }}
              disabled={status === "initializing"}
            />
          </div>
          <div>
            <label>Color overlay</label>
            <select
              value={colorSource}
              onChange={(e) => setColorSource(e.target.value as "none" | "p4" | "p2")}
              disabled={status === "initializing"}
            >
              <option value="none">None</option>
              <option value="p4">P4 counters</option>
              <option value="p2">P2 apparatus</option>
            </select>
          </div>
        </div>

        <div className="row twoCol">
          <div>
            <label>Overlay channel</label>
            <select
              value={overlayChannel}
              onChange={(e) =>
                setOverlayChannel(
                  e.target.value as "none" | "baseS" | "metaS" | "metaN" | "metaA" | "metaW"
                )
              }
              disabled={status === "initializing"}
            >
              <option value="none">None</option>
              <option value="baseS">Base S</option>
              <option value="metaS" disabled={metaLayerCount === 0}>
                Meta S
              </option>
              <option value="metaN" disabled={metaLayerCount === 0}>
                Meta N
              </option>
              <option value="metaA" disabled={metaLayerCount === 0}>
                Meta A
              </option>
              <option value="metaW" disabled={metaLayerCount === 0}>
                Meta W
              </option>
            </select>
          </div>
          <div>
            <label>Preset</label>
            <select
              value={preset}
              onChange={(e) => {
                const v = e.target.value as "sparse" | "balanced" | "dense" | "nullBaseline" | "p3p6Full";
                setPreset(v);
                if (v === "sparse") {
                  setParamsDraft((p) => ({
                    ...p,
                    p3On: 0,
                    p6On: 0,
                    pWrite: 0.12,
                    pNWrite: 0.05,
                    pAWrite: 0.05,
                    pSWrite: 0.05,
                    muHigh: 1.0,
                    muLow: -1.0,
                    lambdaW: 0.25,
                    kappaBond: 1.2,
                    rStar: 0.08,
                    lW: 6,
                    lambdaN: 0.5,
                    lN: 6,
                    lambdaA: 0.5,
                    lA: 6,
                    lambdaS: 0.5,
                    lS: 6,
                    gridSize: 16,
                    rPropose: 0.08,
                  }));
                  setBondThreshold(3);
                } else if (v === "dense") {
                  setParamsDraft((p) => ({
                    ...p,
                    p3On: 0,
                    p6On: 0,
                    pWrite: 0.35,
                    pNWrite: 0.05,
                    pAWrite: 0.05,
                    pSWrite: 0.05,
                    muHigh: 1.0,
                    muLow: -1.0,
                    lambdaW: 0.08,
                    kappaBond: 4.0,
                    lW: 6,
                    lambdaN: 0.5,
                    lN: 6,
                    lambdaA: 0.5,
                    lA: 6,
                    lambdaS: 0.5,
                    lS: 6,
                    gridSize: 16,
                    rPropose: 0.25,
                  }));
                  setBondThreshold(2);
                } else if (v === "nullBaseline") {
                  setP1Enabled(true);
                  setP2Enabled(true);
                  setP4Enabled(true);
                  setP5Enabled(true);
                  setP3Enabled(false);
                  setP6Enabled(false);
                  setParamsDraft((p) => ({
                    ...p,
                    beta: 1.0,
                    stepSize: 0.01,
                    p3On: 0,
                    p6On: 0,
                    pWrite: 0.05,
                    pNWrite: 0.05,
                    pAWrite: 0.05,
                    pSWrite: 0.05,
                    muHigh: 0.6,
                    muLow: -0.6,
                    kappaRep: 500.0,
                    r0: 0.25,
                    kappaBond: 1.2,
                    rStar: 0.22,
                    lambdaW: 0.5,
                    lW: 4,
                    lambdaN: 0.5,
                    lN: 6,
                    lambdaA: 0.5,
                    lA: 6,
                    lambdaS: 0.5,
                    lS: 6,
                    gridSize: 16,
                    rPropose: 0.05,
                  }));
                  setBondThreshold(3);
                } else if (v === "p3p6Full") {
                  setP1Enabled(true);
                  setP2Enabled(true);
                  setP4Enabled(true);
                  setP5Enabled(true);
                  setP3Enabled(true);
                  setP6Enabled(true);
                  setParamsDraft((p) => ({
                    ...p,
                    beta: 1.0,
                    stepSize: 0.02,
                    p3On: 1,
                    p6On: 1,
                    pWrite: 0.05,
                    pNWrite: 0.05,
                    pAWrite: 0.05,
                    pSWrite: 0.05,
                    muHigh: 1.0,
                    muLow: -1.0,
                    kappaRep: 500.0,
                    r0: 0.25,
                    kappaBond: 1.2,
                    rStar: 0.22,
                    lambdaW: 0.5,
                    lW: 4,
                    lambdaN: 0.5,
                    lN: 6,
                    lambdaA: 0.5,
                    lA: 6,
                    lambdaS: 0.5,
                    lS: 6,
                    gridSize: 16,
                    rPropose: 0.05,
                  }));
                  setBondThreshold(3);
                } else {
                  setParamsDraft((p) => ({
                    ...p,
                    p3On: 0,
                    p6On: 0,
                    pWrite: 0.1,
                    pNWrite: 0.05,
                    pAWrite: 0.05,
                    pSWrite: 0.05,
                    muHigh: 0.6,
                    muLow: -0.6,
                    kappaRep: 250.0,
                    r0: 0.15,
                    lambdaW: 0.3,
                    kappaBond: 1.2,
                    rStar: 0.22,
                    lW: 4,
                    lambdaN: 0.5,
                    lN: 6,
                    lambdaA: 0.5,
                    lA: 6,
                    lambdaS: 0.5,
                    lS: 6,
                    gridSize: 16,
                    rPropose: 0.12,
                  }));
                  setBondThreshold(3);
                }
              }}
              disabled={status === "idle" || status === "initializing"}
              style={{
                width: "100%",
                boxSizing: "border-box",
                padding: "8px 10px",
                borderRadius: 8,
                border: "1px solid rgba(255,255,255,0.12)",
                background: "rgba(255,255,255,0.04)",
                color: "var(--text)",
              }}
            >
              <option value="sparse">Sparse</option>
              <option value="balanced">Balanced</option>
              <option value="nullBaseline">Null baseline</option>
              <option value="p3p6Full">P3+P6 full</option>
              <option value="dense">Dense</option>
            </select>
          </div>
        </div>

        {overlayChannel.startsWith("meta") ? (
          <div className="row twoCol">
            <div>
              <label>Overlay layer</label>
              <input
                type="number"
                step={1}
                min={0}
                max={Math.max(0, metaLayerCount - 1)}
                value={overlayLayerIndex}
                onChange={(e) => setOverlayLayerIndex(Math.max(0, Math.floor(Number(e.target.value))))}
                disabled={status === "initializing" || metaLayerCount === 0}
              />
            </div>
            <div />
          </div>
        ) : null}

        <div className="accordion">
          <div 
            className="accordionTitle"
            onClick={() => setExpandedPanels(prev => ({ ...prev, motion: !prev.motion }))}
            style={{ cursor: 'pointer', userSelect: 'none' }}
          >
            <span>{expandedPanels.motion ? '▼' : '▶'}</span> X — motion
          </div>
          {expandedPanels.motion && (
          <div className="accordionContent">
          <div className="row">
            <div>
              <label>beta</label>
              <input
                type="number"
                step={0.1}
                min={0}
                value={paramsDraft.beta}
                onChange={(e) => setParamsDraft((p) => ({ ...p, beta: Number(e.target.value) }))}
                disabled={status === "idle" || status === "initializing"}
              />
            </div>
            <div>
              <label>stepSize</label>
              <input
                type="number"
                step={0.001}
                min={0}
                value={paramsDraft.stepSize}
                onChange={(e) => setParamsDraft((p) => ({ ...p, stepSize: Number(e.target.value) }))}
                disabled={status === "idle" || status === "initializing"}
              />
            </div>
          </div>
          <div className="row">
            <div>
              <label>kappaRep</label>
              <input
                type="number"
                step={1}
                min={0}
                value={paramsDraft.kappaRep}
                onChange={(e) => setParamsDraft((p) => ({ ...p, kappaRep: Number(e.target.value) }))}
                disabled={status === "idle" || status === "initializing"}
              />
            </div>
            <div>
              <label>r0</label>
              <input
                type="number"
                step={0.01}
                min={0}
                max={0.5}
                value={paramsDraft.r0}
                onChange={(e) => setParamsDraft((p) => ({ ...p, r0: Number(e.target.value) }))}
                disabled={status === "idle" || status === "initializing"}
              />
            </div>
          </div>
          </div>
          )}
        </div>

        {p1Enabled ? (
          <div className="accordion">
            <div 
              className="accordionTitle"
              onClick={() => setExpandedPanels(prev => ({ ...prev, p1: !prev.p1 }))}
              style={{ cursor: 'pointer', userSelect: 'none' }}
            >
              <span>{expandedPanels.p1 ? '▼' : '▶'}</span> P1 — bond write
            </div>
            {expandedPanels.p1 && (
            <div className="accordionContent">
            <div className="row">
              <div>
                <label>pWrite (P1 rate)</label>
                <input
                  type="number"
                  step={0.01}
                  min={0}
                  max={1}
                  value={paramsDraft.pWrite}
                  onChange={(e) => setParamsDraft((p) => ({ ...p, pWrite: Number(e.target.value) }))}
                  disabled={status === "idle" || status === "initializing"}
                />
              </div>
              <div>
                <label>rPropose</label>
                <input
                  type="number"
                  step={0.01}
                  min={0}
                  max={0.5}
                  value={paramsDraft.rPropose}
                  onChange={(e) => setParamsDraft((p) => ({ ...p, rPropose: Number(e.target.value) }))}
                  disabled={status === "idle" || status === "initializing"}
                />
              </div>
            </div>

            <div className="row">
              <div>
                <label>lambdaW</label>
                <input
                  type="number"
                  step={0.01}
                  min={0}
                  value={paramsDraft.lambdaW}
                  onChange={(e) => setParamsDraft((p) => ({ ...p, lambdaW: Number(e.target.value) }))}
                  disabled={status === "idle" || status === "initializing"}
                />
              </div>
              <div>
                <label>kappaBond</label>
                <input
                  type="number"
                  step={0.1}
                  min={0}
                  value={paramsDraft.kappaBond}
                  onChange={(e) => setParamsDraft((p) => ({ ...p, kappaBond: Number(e.target.value) }))}
                  disabled={status === "idle" || status === "initializing"}
                />
              </div>
            </div>

            <div className="row">
              <div>
                <label>rStar</label>
                <input
                  type="number"
                  step={0.01}
                  min={0}
                  max={0.5}
                  value={paramsDraft.rStar}
                  onChange={(e) => setParamsDraft((p) => ({ ...p, rStar: Number(e.target.value) }))}
                  disabled={status === "idle" || status === "initializing"}
                />
              </div>
              <div>
                <label>Lw (max w)</label>
                <input
                  type="number"
                  step={1}
                  min={1}
                  max={255}
                  value={paramsDraft.lW}
                  onChange={(e) => setParamsDraft((p) => ({ ...p, lW: Number(e.target.value) }))}
                  disabled={status === "idle" || status === "initializing"}
                />
              </div>
            </div>
            </div>
            )}
          </div>
        ) : null}

        {p4Enabled ? (
          <div className="accordion">
            <div 
              className="accordionTitle"
              onClick={() => setExpandedPanels(prev => ({ ...prev, p4: !prev.p4 }))}
              style={{ cursor: 'pointer', userSelect: 'none' }}
            >
              <span>{expandedPanels.p4 ? '▼' : '▶'}</span> P4 — counters
            </div>
            {expandedPanels.p4 && (
            <div className="accordionContent">
            <div className="row">
              <div>
                <label>pNWrite (P4 rate)</label>
                <input
                  type="number"
                  step={0.01}
                  min={0}
                  max={1}
                  value={paramsDraft.pNWrite}
                  onChange={(e) => setParamsDraft((p) => ({ ...p, pNWrite: Number(e.target.value) }))}
                  disabled={status === "idle" || status === "initializing"}
                />
              </div>
              <div>
                <label>lambdaN</label>
                <input
                  type="number"
                  step={0.01}
                  min={0}
                  value={paramsDraft.lambdaN}
                  onChange={(e) => setParamsDraft((p) => ({ ...p, lambdaN: Number(e.target.value) }))}
                  disabled={status === "idle" || status === "initializing"}
                />
              </div>
            </div>
            <div className="row">
              <div>
                <label>Ln (max |n|)</label>
                <input
                  type="number"
                  step={1}
                  min={1}
                  max={32767}
                  value={paramsDraft.lN}
                  onChange={(e) => setParamsDraft((p) => ({ ...p, lN: Number(e.target.value) }))}
                  disabled={status === "idle" || status === "initializing"}
                />
              </div>
              <div />
            </div>
            </div>
            )}
          </div>
        ) : null}

        {p2Enabled ? (
          <div className="accordion">
            <div 
              className="accordionTitle"
              onClick={() => setExpandedPanels(prev => ({ ...prev, p2: !prev.p2 }))}
              style={{ cursor: 'pointer', userSelect: 'none' }}
            >
              <span>{expandedPanels.p2 ? '▼' : '▶'}</span> P2 — apparatus
            </div>
            {expandedPanels.p2 && (
            <div className="accordionContent">
            <div className="row">
              <div>
                <label>pAWrite (P2 rate)</label>
                <input
                  type="number"
                  step={0.01}
                  min={0}
                  max={1}
                  value={paramsDraft.pAWrite}
                  onChange={(e) => setParamsDraft((p) => ({ ...p, pAWrite: Number(e.target.value) }))}
                  disabled={status === "idle" || status === "initializing"}
                />
              </div>
              <div>
                <label>lambdaA</label>
                <input
                  type="number"
                  step={0.01}
                  min={0}
                  value={paramsDraft.lambdaA}
                  onChange={(e) => setParamsDraft((p) => ({ ...p, lambdaA: Number(e.target.value) }))}
                  disabled={status === "idle" || status === "initializing"}
                />
              </div>
            </div>
            <div className="row">
              <div>
                <label>La (max |a|)</label>
                <input
                  type="number"
                  step={1}
                  min={1}
                  max={32767}
                  value={paramsDraft.lA}
                  onChange={(e) => setParamsDraft((p) => ({ ...p, lA: Number(e.target.value) }))}
                  disabled={status === "idle" || status === "initializing"}
                />
              </div>
              <div />
            </div>
            </div>
            )}
          </div>
        ) : null}

        {p5Enabled ? (
          <div className="accordion">
            <div 
              className="accordionTitle"
              onClick={() => setExpandedPanels(prev => ({ ...prev, p5: !prev.p5 }))}
              style={{ cursor: 'pointer', userSelect: 'none' }}
            >
              <span>{expandedPanels.p5 ? '▼' : '▶'}</span> P5 — field
            </div>
            {expandedPanels.p5 && (
            <div className="accordionContent">
            <div className="row">
              <div>
                <label>pSWrite (P5 rate)</label>
                <input
                  type="number"
                  step={0.01}
                  min={0}
                  max={1}
                  value={paramsDraft.pSWrite}
                  onChange={(e) => setParamsDraft((p) => ({ ...p, pSWrite: Number(e.target.value) }))}
                  disabled={status === "idle" || status === "initializing"}
                />
              </div>
              <div>
                <label>lambdaS</label>
                <input
                  type="number"
                  step={0.01}
                  min={0}
                  value={paramsDraft.lambdaS}
                  onChange={(e) => setParamsDraft((p) => ({ ...p, lambdaS: Number(e.target.value) }))}
                  disabled={status === "idle" || status === "initializing"}
                />
              </div>
            </div>
            <div className="row">
              <div>
                <label>Ls (max S)</label>
                <input
                  type="number"
                  step={1}
                  min={1}
                  max={255}
                  value={paramsDraft.lS}
                  onChange={(e) => setParamsDraft((p) => ({ ...p, lS: Number(e.target.value) }))}
                  disabled={status === "idle" || status === "initializing"}
                />
              </div>
              <div>
                <label>Grid size</label>
                <input
                  type="number"
                  step={1}
                  min={2}
                  max={256}
                  value={paramsDraft.gridSize}
                  onChange={(e) => setParamsDraft((p) => ({ ...p, gridSize: Number(e.target.value) }))}
                  disabled={status === "idle" || status === "initializing"}
                />
              </div>
            </div>
            <div className="row">
              <div>
                <label>Safe threshold (S ≥)</label>
                <input
                  type="number"
                  step={1}
                  min={1}
                  max={255}
                  value={safeThreshold}
                  onChange={(e) => setSafeThreshold(Math.max(1, Math.floor(Number(e.target.value))))}
                  disabled={status === "initializing"}
                />
              </div>
              <div />
            </div>
            </div>
            )}
          </div>
        ) : null}

        <div className="accordion">
          <div 
            className="accordionTitle"
            onClick={() => setExpandedPanels(prev => ({ ...prev, meta: !prev.meta }))}
            style={{ cursor: 'pointer', userSelect: 'none' }}
          >
            <span>{expandedPanels.meta ? '▼' : '▶'}</span> Meta
          </div>
          {expandedPanels.meta && (
          <div className="accordionContent">
          <div className="row">
            <div>
              <label>Meta layers</label>
              <input
                type="number"
                step={1}
                min={0}
                max={16}
                value={paramsDraft.metaLayers}
                onChange={(e) => setParamsDraft((p) => ({ ...p, metaLayers: Number(e.target.value) }))}
                disabled={status === "idle" || status === "initializing"}
              />
            </div>
            <div>
              <label>eta</label>
              <input
                type="number"
                step={0.01}
                min={0}
                max={1}
                value={paramsDraft.eta}
                onChange={(e) => setParamsDraft((p) => ({ ...p, eta: Number(e.target.value) }))}
                disabled={status === "idle" || status === "initializing"}
              />
            </div>
          </div>
          <div className="row">
            <div>
              <label>eta slider</label>
              <input
                type="range"
                step={0.01}
                min={0}
                max={1}
                value={paramsDraft.eta}
                onChange={(e) => setParamsDraft((p) => ({ ...p, eta: Number(e.target.value) }))}
                disabled={status === "idle" || status === "initializing"}
              />
            </div>
            <div />
          </div>
          </div>
          )}
        </div>

        {p6Enabled ? (
          <div className="accordion">
            <div 
              className="accordionTitle"
              onClick={() => setExpandedPanels(prev => ({ ...prev, p6: !prev.p6 }))}
              style={{ cursor: 'pointer', userSelect: 'none' }}
            >
              <span>{expandedPanels.p6 ? '▼' : '▶'}</span> P6 — resource
            </div>
            {expandedPanels.p6 && (
            <div className="accordionContent">
            <div className="row">
              <div>
                <label>muHigh</label>
                <input
                  type="number"
                  step={0.1}
                  value={paramsDraft.muHigh}
                  onChange={(e) => setParamsDraft((p) => ({ ...p, muHigh: Number(e.target.value) }))}
                  disabled={status === "idle" || status === "initializing"}
                />
              </div>
              <div>
                <label>muLow</label>
                <input
                  type="number"
                  step={0.1}
                  value={paramsDraft.muLow}
                  onChange={(e) => setParamsDraft((p) => ({ ...p, muLow: Number(e.target.value) }))}
                  disabled={status === "idle" || status === "initializing"}
                />
              </div>
            </div>
            </div>
            )}
          </div>
        ) : null}

        <div className="row">
          <button
            onClick={() => {
              setError(null);
              const applied = effectiveParams(paramsDraft);
              setParamsApplied(applied);
              if (status !== "idle" && status !== "initializing") {
                client.send({ type: "config", bondThreshold, params: applied });
              }
            }}
            disabled={status === "initializing" || shallowEqual(effectiveParams(paramsDraft), paramsApplied)}
          >
            Apply params
          </button>
          <div />
        </div>

        <div className="row">
          <button
            className="primary"
            onClick={() => {
              setError(null);
              setStatus("running");
              client.send({ type: "resume" });
            }}
            disabled={!canRun}
          >
            Run
          </button>
          <button
            onClick={() => {
              setStatus("ready");
              client.send({ type: "pause" });
            }}
            disabled={!canPause}
          >
            Pause
          </button>
        </div>

        <p>Status: {status}</p>
        {!shallowEqual(effectiveParams(paramsDraft), paramsApplied) ? (
          <p style={{ color: "#97a3b3" }}>Params changed (not applied). For scientific runs, prefer Apply params + Init.</p>
        ) : null}
        {status === "initializing" && initSlow ? (
          <p style={{ color: "#97a3b3" }}>
            Still initializing… if this persists, the worker likely failed to load the WASM module; check the Error
            line below.
          </p>
        ) : null}
        {energy ? (
          <p style={{ marginTop: 8, fontFamily: "ui-monospace, SFMono-Regular, Menlo, monospace" }}>
            E={energy.total.toFixed(3)} (Urep {energy.uRep.toFixed(3)}, Ubond {energy.uBond.toFixed(3)}, Ew{" "}
            {energy.eW.toFixed(3)}, En {energy.eN.toFixed(3)}, Ea {energy.eA.toFixed(3)}, Es{" "}
            {energy.eS.toFixed(3)})
          </p>
        ) : null}
        <p style={{ marginTop: 6, fontFamily: "ui-monospace, SFMono-Regular, Menlo, monospace" }}>
          Steps: {totalSteps}
        </p>
        <p style={{ marginTop: 6, fontFamily: "ui-monospace, SFMono-Regular, Menlo, monospace" }}>
          Meta layers {paramsApplied.metaLayers} | metaField.length {metaSnapshot ? metaSnapshot.length : 0}
        </p>
        {diagnostics ? (
          <p style={{ marginTop: 6, fontFamily: "ui-monospace, SFMono-Regular, Menlo, monospace" }}>
            P1 steps {diagnostics.window} | N+ {diagnostics.wPlus} N- {diagnostics.wMinus} | Jw{" "}
            {diagnostics.jW.toFixed(4)} Aw {diagnostics.aW.toFixed(4)} Σmem {diagnostics.sigmaMem.toFixed(4)}
          </p>
        ) : null}
        {diagnostics ? (
          <p style={{ marginTop: 6, fontFamily: "ui-monospace, SFMono-Regular, Menlo, monospace" }}>
            P2 steps {diagnostics.window} | N+ {diagnostics.aPlus} N- {diagnostics.aMinus} | Ja{" "}
            {diagnostics.jA.toFixed(4)} Aa {diagnostics.aA.toFixed(4)}
          </p>
        ) : null}
        {diagnostics ? (
          <p style={{ marginTop: 6, fontFamily: "ui-monospace, SFMono-Regular, Menlo, monospace" }}>
            P4 steps {diagnostics.window} | N+ {diagnostics.nPlus} N- {diagnostics.nMinus} | Jn{" "}
            {diagnostics.jN.toFixed(4)} An {diagnostics.aN.toFixed(4)}
          </p>
        ) : null}
        {diagnostics ? (
          <p style={{ marginTop: 6, fontFamily: "ui-monospace, SFMono-Regular, Menlo, monospace" }}>
            P5 steps {diagnostics.window} | N+ {diagnostics.sPlus} N- {diagnostics.sMinus} | Js{" "}
            {diagnostics.jS.toFixed(4)} As {diagnostics.aS.toFixed(4)}
          </p>
        ) : null}
        {diagnostics && p3Enabled ? (
          <p style={{ marginTop: 6, fontFamily: "ui-monospace, SFMono-Regular, Menlo, monospace" }}>
            P3 cycle {diagnostics.p3CycleLen} | disp {diagnostics.p3DispMag.toFixed(4)} | loop{" "}
            {diagnostics.p3LoopArea.toFixed(4)}
          </p>
        ) : null}
        {diagnostics && p6Enabled ? (
          <p style={{ marginTop: 6, fontFamily: "ui-monospace, SFMono-Regular, Menlo, monospace" }}>
            P6 M6 | W {diagnostics.aM6W.toFixed(4)} N {diagnostics.aM6N.toFixed(4)} A{" "}
            {diagnostics.aM6A.toFixed(4)} S {diagnostics.aM6S.toFixed(4)}
          </p>
        ) : null}
        {graphStats && graphStatsN ? (
          <p style={{ marginTop: 6, fontFamily: "ui-monospace, SFMono-Regular, Menlo, monospace" }}>
            Graph edges {graphStats.edges} | components {graphStats.components} | largest{" "}
            {graphStats.largest}/{graphStatsN} ({(graphStats.largest / graphStatsN).toFixed(2)})
          </p>
        ) : null}
        <button
          style={{ marginTop: 8 }}
          onClick={async () => {
            try {
              await exportRun();
            } catch (err) {
              const message = err instanceof Error ? err.message : String(err);
              setError(message);
            }
          }}
          disabled={status === "idle" || status === "initializing"}
        >
          Export run
        </button>
        {error ? <p style={{ color: "#ff6a6a" }}>Error: {error}</p> : null}
        <p style={{ marginTop: 14 }}>
          Build WASM into the UI at: <code>apps/web/src/wasm/sim_core</code>
        </p>
      </div>

      <div className="main">
        <div className="canvasWrap">
          <canvas ref={canvasRef} />
        </div>
        <div className="chartsPanel">
          {chartGroups.current.flatMap((group) => {
            if (group === "P1" && !p1Enabled) return [];
            if (group === "P2" && !p2Enabled) return [];
            if (group === "P3" && !p3Enabled) return [];
            if (group === "P4" && !p4Enabled) return [];
            if (group === "P5" && !p5Enabled) return [];
            const charts = CHARTS.filter((c) => c.group === group).map((chart) => (
              <div className={`chartCard chartCard--${group}`} key={chart.id}>
                <div className="chartLabel">{chart.label}</div>
                <canvas ref={(el) => (chartRefs.current[chart.id] = el)} />
              </div>
            ));
            const hists = HISTOGRAMS.filter((h) => h.group === group).map((hist) => (
              <div className={`chartCard chartCard--${group}`} key={hist.id}>
                <div className="chartLabel">{hist.label}</div>
                <canvas ref={(el) => (histRefs.current[hist.id] = el)} />
              </div>
            ));
            return charts.concat(hists);
          })}
        </div>
      </div>
    </div>
  );
}

function shallowEqual(a: SimParams, b: SimParams): boolean {
  const keys: Array<keyof SimParams> = [
    "beta",
    "stepSize",
    "p3On",
    "p6On",
    "pWrite",
    "pNWrite",
    "pAWrite",
    "pSWrite",
    "muHigh",
    "muLow",
    "kappaRep",
    "r0",
    "kappaBond",
    "rStar",
    "lambdaW",
    "lW",
    "lambdaN",
    "lN",
    "lambdaA",
    "lA",
    "lambdaS",
    "lS",
    "gridSize",
    "rPropose",
    "metaLayers",
    "eta",
  ];
  return keys.every((k) => a[k] === b[k]);
}
</file>

<file path="scripts/run-code-maintenance.mjs">
#!/usr/bin/env node
import fs from "node:fs";
import path from "node:path";
import { fileURLToPath, pathToFileURL } from "node:url";

const __filename = fileURLToPath(import.meta.url);
const __dirname = path.dirname(__filename);
const rootDir = path.resolve(__dirname, "..");
const outDir = path.resolve(rootDir, ".tmp", "clock_code");

const wasmDir = path.resolve(rootDir, "apps/web/src/wasm/sim_core");
const wasmJs = pathToFileURL(path.join(wasmDir, "sim_core.js")).href;
const wasmBytes = fs.readFileSync(path.join(wasmDir, "sim_core_bg.wasm"));
const mod = await import(wasmJs);
mod.initSync({ module: wasmBytes });

const steps = 1_000_000;
const reportEvery = 100_000;
const perturbStep = 500_000;
const seeds = [1, 2, 3, 4, 5];
const fractions = [0.25, 0.5, 0.75, 1.0];
const MOVE_P5_BASE = 7;
const MOVE_P5_META = 8;
const MOVE_CLOCK = 10;

function ensureDir(dir) {
  fs.mkdirSync(dir, { recursive: true });
}

function readJson(filePath) {
  return JSON.parse(fs.readFileSync(filePath, "utf8"));
}

function mean(values) {
  return values.reduce((acc, v) => acc + v, 0) / values.length;
}

function meanStd(values) {
  const nums = values.filter((v) => Number.isFinite(v));
  if (nums.length === 0) return { mean: null, std: null };
  const m = mean(nums);
  const variance = nums.reduce((acc, v) => acc + (v - m) ** 2, 0) / nums.length;
  return { mean: m, std: Math.sqrt(variance) };
}

function percentile(values, p) {
  const nums = values.filter((v) => Number.isFinite(v)).sort((a, b) => a - b);
  if (nums.length === 0) return null;
  const idx = Math.min(nums.length - 1, Math.floor(p * (nums.length - 1)));
  return nums[idx];
}

function makeRng(seed) {
  let x = seed >>> 0;
  if (x === 0) x = 1;
  return () => {
    x ^= x << 13;
    x ^= x >>> 17;
    x ^= x << 5;
    return (x >>> 8) / (1 << 24);
  };
}

function meanAbsDiff(a, b) {
  let sum = 0;
  for (let i = 0; i < a.length; i += 1) {
    sum += Math.abs(a[i] - b[i]);
  }
  return a.length === 0 ? 0 : sum / a.length;
}

function quadrantIndex(idx, g) {
  const x = idx % g;
  const y = Math.floor(idx / g);
  const qx = x < g / 2 ? 0 : 1;
  const qy = y < g / 2 ? 0 : 1;
  return qy * 2 + qx;
}

function logicalBitsFromField(field, g, lS, mask) {
  const sums = [0, 0, 0, 0];
  const counts = [0, 0, 0, 0];
  for (let i = 0; i < field.length; i += 1) {
    if (mask && !mask[i]) continue;
    const q = quadrantIndex(i, g);
    sums[q] += field[i];
    counts[q] += 1;
  }
  const threshold = lS / 2;
  return sums.map((sum, i) => {
    const meanVal = counts[i] > 0 ? sum / counts[i] : 0;
    return meanVal >= threshold ? 1 : 0;
  });
}

function errorRate(bitsA, bitsB) {
  let mismatches = 0;
  for (let i = 0; i < bitsA.length; i += 1) {
    if (bitsA[i] !== bitsB[i]) mismatches += 1;
  }
  return mismatches / bitsA.length;
}

function reconstructibilityErrors(baseBits, metaField, g, lS, seed) {
  const rng = makeRng(seed);
  const errors = {};
  for (const frac of fractions) {
    const trials = 20;
    let acc = 0;
    for (let t = 0; t < trials; t += 1) {
      const mask = new Array(metaField.length);
      for (let i = 0; i < metaField.length; i += 1) {
        mask[i] = rng() < frac;
      }
      const bits = logicalBitsFromField(metaField, g, lS, mask);
      acc += errorRate(bits, baseBits);
    }
    errors[frac] = acc / trials;
  }
  return errors;
}

function runCondition(id, params) {
  const runs = [];
  for (const seed of seeds) {
    const sim = new mod.Sim(50, seed);
    sim.set_params(params);
    const timeSeries = [];
    const preSamples = [];
    let baselineMean = null;
    let baselineTarget = null;
    let perturbApplied = false;
    let recoverySteps = null;

    for (let step = reportEvery; step <= steps; step += reportEvery) {
      sim.step(reportEvery);
      const baseS = sim.base_s_field();
      const metaS = sim.meta_field();
      const cells = baseS.length;
      const meta0 = metaS.subarray(0, cells);
      const meta1 = metaS.subarray(cells, 2 * cells);
      const sdiffBase = meanAbsDiff(baseS, meta0);
      const sdiffMeta = meanAbsDiff(meta0, meta1);
      const epTotal = sim.ep_exact_total();
      const clockQ = Number(sim.clock_q());
      timeSeries.push({ step, sdiffBase, sdiffMeta, epTotal, clockQ });

      if (!perturbApplied) {
        if (step <= perturbStep) {
          preSamples.push(sdiffBase);
        }
        if (step >= perturbStep) {
          const take = preSamples.slice(-3);
          baselineMean = mean(take.length > 0 ? take : preSamples);
          baselineTarget = Math.max(baselineMean, 0.5);
          sim.apply_perturbation({
            target: "metaS",
            layer: 0,
            frac: 0.3,
            mode: "randomize",
            seed: seed * 1000 + 7,
          });
          perturbApplied = true;
        }
      } else if (recoverySteps === null && baselineTarget !== null) {
        if (sdiffBase <= baselineTarget * 1.1) {
          recoverySteps = step - perturbStep;
        }
      }
    }

    const baseS = sim.base_s_field();
    const metaS = sim.meta_field();
    const cells = baseS.length;
    const meta0 = metaS.subarray(0, cells);
    const meta1 = metaS.subarray(cells, 2 * cells);
    const lS = params.lS ?? 1;
    const baseBits = logicalBitsFromField(baseS, params.gridSize, lS);
    const errCurve = reconstructibilityErrors(baseBits, meta0, params.gridSize, lS, seed + 5000);
    const finalSdiffBase = meanAbsDiff(baseS, meta0);
    const finalSdiffMeta = meanAbsDiff(meta0, meta1);
    const epTotal = sim.ep_exact_total();
    const epRate = steps > 0 ? epTotal / steps : 0;
    const epByMove = sim.ep_exact_by_move();
    const epClock = epByMove[MOVE_CLOCK] ?? 0;
    const epRepair = (epByMove[MOVE_P5_BASE] ?? 0) + (epByMove[MOVE_P5_META] ?? 0);
    const clockQ = Number(sim.clock_q());
    const clockDrift = steps > 0 ? clockQ / steps : 0;

    runs.push({
      id,
      seed,
      finalSdiffBase,
      finalSdiffMeta,
      errCurve,
      recoverySteps: recoverySteps ?? Infinity,
      epRate,
      epClock,
      epRepair,
      clockDrift,
      timeSeries,
    });
  }
  return runs;
}

function summarizeRuns(id, runs) {
  const sdiffBase = runs.map((r) => r.finalSdiffBase);
  const sdiffMeta = runs.map((r) => r.finalSdiffMeta);
  const errF = runs.map((r) => r.errCurve[0.5]);
  const recovery = runs.map((r) => r.recoverySteps).filter((v) => Number.isFinite(v));
  const epRate = runs.map((r) => r.epRate);
  const epClock = runs.map((r) => r.epClock);
  const epRepair = runs.map((r) => r.epRepair);
  const clockDrift = runs.map((r) => r.clockDrift);
  const recoveryFrac = recovery.length / runs.length;

  return {
    id,
    sdiffBase: meanStd(sdiffBase),
    sdiffMeta: meanStd(sdiffMeta),
    errF: meanStd(errF),
    recoveryMean: recovery.length > 0 ? mean(recovery) : Infinity,
    recoveryP95: percentile(recovery, 0.95),
    recoveryFrac,
    epRate: meanStd(epRate),
    epClock: meanStd(epClock),
    epRepair: meanStd(epRepair),
    clockDrift: meanStd(clockDrift),
  };
}

ensureDir(outDir);

const presets = [
  {
    id: "code_null",
    file: "scripts/params/clock_code/code_null.json",
  },
  {
    id: "code_p6_drive",
    file: "scripts/params/clock_code/code_p6_drive.json",
  },
  {
    id: "code_p6_clock_gated",
    file: "scripts/params/clock_code/code_p6_clock_gated.json",
  },
];

const rawPath = path.join(outDir, "code_maintenance_raw.jsonl");
const summaryPath = path.join(outDir, "code_maintenance_summary.csv");
const rawLines = [];
const summaries = [];

for (const preset of presets) {
  const params = readJson(path.resolve(rootDir, preset.file));
  const runs = runCondition(preset.id, params);
  for (const run of runs) {
    rawLines.push(
      JSON.stringify({
        id: run.id,
        seed: run.seed,
        finalSdiffBase: run.finalSdiffBase,
        finalSdiffMeta: run.finalSdiffMeta,
        errCurve: run.errCurve,
        recoverySteps: Number.isFinite(run.recoverySteps) ? run.recoverySteps : null,
        epRate: run.epRate,
        clockDrift: run.clockDrift,
        timeSeries: run.timeSeries,
      }),
    );
  }
  const summary = summarizeRuns(preset.id, runs);
  summaries.push(summary);
  console.log(
    `${preset.id} | Sdiff ${summary.sdiffBase.mean?.toFixed(3)} | err(0.5) ${summary.errF.mean?.toFixed(3)} | recovery ${summary.recoveryMean.toFixed(1)} | epRate ${summary.epRate.mean?.toFixed(4)}`,
  );
}

fs.writeFileSync(rawPath, rawLines.join("\n"));
const header = [
  "preset",
  "sdiffBaseMean",
  "sdiffBaseStd",
  "sdiffMetaMean",
  "sdiffMetaStd",
  "errF0p5Mean",
  "errF0p5Std",
  "recoveryStepsMean",
  "recoveryStepsP95",
  "recoverySuccessFrac",
  "epRateMean",
  "epRateStd",
  "epClockMean",
  "epClockStd",
  "epRepairMean",
  "epRepairStd",
  "clockDriftMean",
  "clockDriftStd",
];
const csvLines = [
  header.join(","),
  ...summaries.map((s) =>
    [
      s.id,
      s.sdiffBase.mean ?? "",
      s.sdiffBase.std ?? "",
      s.sdiffMeta.mean ?? "",
      s.sdiffMeta.std ?? "",
      s.errF.mean ?? "",
      s.errF.std ?? "",
      Number.isFinite(s.recoveryMean) ? s.recoveryMean : "",
      s.recoveryP95 ?? "",
      s.recoveryFrac.toFixed(2),
      s.epRate.mean ?? "",
      s.epRate.std ?? "",
      s.epClock.mean ?? "",
      s.epClock.std ?? "",
      s.epRepair.mean ?? "",
      s.epRepair.std ?? "",
      s.clockDrift.mean ?? "",
      s.clockDrift.std ?? "",
    ].join(","),
  ),
];
fs.writeFileSync(summaryPath, csvLines.join("\n"));

console.log(`Code maintenance sweep complete. Summary saved to ${summaryPath}`);
</file>

<file path="scripts/ratchet-cli.mjs">
#!/usr/bin/env node
import fs from "node:fs";
import path from "node:path";
import { fileURLToPath, pathToFileURL } from "node:url";

const __filename = fileURLToPath(import.meta.url);
const __dirname = path.dirname(__filename);
const rootDir = path.resolve(__dirname, "..");

const DEFAULT_PARAMS = {
  beta: 1.0,
  stepSize: 0.01,
  p3On: 0,
  p6On: 0,
  p6SFactor: 1.0,
  pWrite: 0.1,
  pNWrite: 0.05,
  pAWrite: 0.05,
  pSWrite: 0.05,
  muHigh: 0.6,
  muLow: -0.6,
  kappaRep: 500.0,
  r0: 0.25,
  kappaBond: 1.2,
  rStar: 0.22,
  lambdaW: 0.3,
  lW: 4,
  lambdaN: 0.5,
  lN: 6,
  lambdaA: 0.5,
  lA: 6,
  lambdaS: 0.5,
  lS: 6,
  gridSize: 16,
  rPropose: 0.12,
  metaLayers: 0,
  eta: 0.0,
  etaDrive: 0.0,
  opCouplingOn: 0,
  opStencil: 0,
  opBudgetK: 16,
  opKTargetWeight: 1.0,
  sCouplingMode: 0,
  opDriveOnK: 1,
  epDebug: 0,
  initRandom: 0,
  codeNoiseRate: 0,
  codeNoiseBatch: 1,
  codeNoiseLayer: 0,
  clockOn: 0,
  clockK: 8,
  clockFrac: 0.2,
  clockUsesP6: 1,
  repairClockGated: 0,
  repairGateMode: 0,
  repairGateSpan: 1,
};

function printHelp() {
  console.log(
    [
      "ratchet-cli.mjs run [options]",
      "",
      "Options:",
      "  --n <int>               particle count (default 200)",
      "  --seed <int>            RNG seed (default 1)",
      "  --steps <int>           steps to run (default 100000)",
      "  --report-every <int>    print summary every N steps (default 0)",
      "  --bond-threshold <int>  bond draw threshold (default 3)",
      "  --params <file.json>    JSON params to merge with defaults",
      "  --set key=value         override param/bondThreshold/recordEverySteps/safeThreshold",
      "",
      "Examples:",
      "  node scripts/ratchet-cli.mjs run --steps 200000",
      "  node scripts/ratchet-cli.mjs run --params ./my-params.json --set p3On=1 --set p6On=1",
    ].join("\n"),
  );
}

function parseArgs(argv) {
  const out = {
    cmd: "run",
    n: 200,
    seed: 1,
    steps: 100000,
    reportEvery: 0,
    bondThreshold: 3,
    paramsPath: null,
    sets: [],
  };
  const args = argv.slice(2);
  if (args.length === 0 || args[0] === "-h" || args[0] === "--help") {
    out.cmd = "help";
    return out;
  }
  out.cmd = args[0] ?? "run";
  for (let i = 1; i < args.length; i += 1) {
    const arg = args[i];
    if (arg === "--n") out.n = Number(args[++i]);
    else if (arg === "--seed") out.seed = Number(args[++i]);
    else if (arg === "--steps") out.steps = Number(args[++i]);
    else if (arg === "--report-every") out.reportEvery = Number(args[++i]);
    else if (arg === "--bond-threshold") out.bondThreshold = Number(args[++i]);
    else if (arg === "--params") out.paramsPath = args[++i] ?? null;
    else if (arg === "--set") out.sets.push(args[++i] ?? "");
  }
  return out;
}

function readJson(filePath) {
  const raw = fs.readFileSync(filePath, "utf8");
  return JSON.parse(raw);
}

function applySets(target, sets) {
  for (const item of sets) {
    if (!item) continue;
    const [key, raw] = item.split("=");
    if (!key) continue;
    const v = Number(raw);
    if (!Number.isFinite(v)) continue;
    target[key.trim()] = v;
  }
}

function graphStats(n, bonds) {
  const parent = Array.from({ length: n }, (_, i) => i);
  const size = Array.from({ length: n }, () => 1);
  const find = (x) => {
    let p = parent[x];
    while (p !== parent[p]) p = parent[p];
    while (x !== p) {
      const next = parent[x];
      parent[x] = p;
      x = next;
    }
    return p;
  };
  const unite = (a, b) => {
    let ra = find(a);
    let rb = find(b);
    if (ra === rb) return;
    if (size[ra] < size[rb]) {
      [ra, rb] = [rb, ra];
    }
    parent[rb] = ra;
    size[ra] += size[rb];
  };
  for (let i = 0; i < bonds.length; i += 2) {
    unite(bonds[i], bonds[i + 1]);
  }
  let components = 0;
  let largest = 0;
  for (let i = 0; i < n; i += 1) {
    if (parent[i] === i) {
      components += 1;
      largest = Math.max(largest, size[i]);
    }
  }
  return { edges: bonds.length / 2, components, largest, n };
}

function formatSummary(totalSteps, energy, diag, graph, extras) {
  const lines = [];
  lines.push(
    `E=${energy.total.toFixed(3)} (Urep ${energy.uRep.toFixed(3)}, Ubond ${energy.uBond.toFixed(3)}, Ew ${energy.eW.toFixed(3)}, En ${energy.eN.toFixed(3)}, Ea ${energy.eA.toFixed(3)}, Es ${energy.eS.toFixed(3)})`,
  );
  lines.push(`Steps: ${totalSteps}`);
  lines.push(
    `P1 steps ${diag.window} | N+ ${diag.wPlus} N- ${diag.wMinus} | Jw ${diag.jW.toFixed(4)} Aw ${diag.aW.toFixed(4)} Σmem ${diag.sigmaMem.toFixed(4)}`,
  );
  lines.push(
    `P2 steps ${diag.window} | N+ ${diag.aPlus} N- ${diag.aMinus} | Ja ${diag.jA.toFixed(4)} Aa ${diag.aA.toFixed(4)}`,
  );
  lines.push(
    `P4 steps ${diag.window} | N+ ${diag.nPlus} N- ${diag.nMinus} | Jn ${diag.jN.toFixed(4)} An ${diag.aN.toFixed(4)}`,
  );
  lines.push(
    `P5 steps ${diag.window} | N+ ${diag.sPlus} N- ${diag.sMinus} | Js ${diag.jS.toFixed(4)} As ${diag.aS.toFixed(4)}`,
  );
  lines.push(`P3 cycle ${diag.p3CycleLen} | disp ${diag.p3DispMag.toFixed(4)} | loop ${diag.p3LoopArea.toFixed(4)}`);
  lines.push(
    `P6 M6 | W ${diag.aM6W.toFixed(4)} N ${diag.aM6N.toFixed(4)} A ${diag.aM6A.toFixed(4)} S ${diag.aM6S.toFixed(4)}`,
  );
  if (extras) {
    lines.push(
      `EP naive ${extras.epNaiveTotal.toFixed(4)} | rate ${extras.epNaiveRate.toFixed(6)}`,
    );
    lines.push(
      `EP exact ${extras.epExactTotal.toFixed(4)} | rate ${extras.epExactRate.toFixed(6)} | window ${extras.epExactWindowRate.toFixed(6)}`,
    );
    lines.push(
      `Clock Q ${extras.clockQ} | fwd ${extras.clockFwd} bwd ${extras.clockBwd} | drift ${extras.clockDrift.toFixed(6)}`,
    );
  }
  lines.push(
    `Graph edges ${graph.edges} | components ${graph.components} | largest ${graph.largest}/${graph.n}`,
  );
  return lines;
}

async function main() {
  const args = parseArgs(process.argv);
  if (args.cmd === "help") {
    printHelp();
    return;
  }
  if (args.cmd !== "run") {
    console.error(`Unknown command: ${args.cmd}`);
    printHelp();
    process.exit(1);
  }

  let params = { ...DEFAULT_PARAMS };
  if (args.paramsPath) {
    const extra = readJson(args.paramsPath);
    params = { ...params, ...extra };
  }
  applySets(params, args.sets);

  const wasmDir = path.resolve(rootDir, "apps/web/src/wasm/sim_core");
  const wasmJs = pathToFileURL(path.join(wasmDir, "sim_core.js")).href;
  const wasmBytes = fs.readFileSync(path.join(wasmDir, "sim_core_bg.wasm"));
  const mod = await import(wasmJs);
  mod.initSync({ module: wasmBytes });

  const sim = new mod.Sim(args.n, args.seed);
  sim.set_params(params);

  let totalSteps = 0;
  let lastEpExact = 0;
  let lastSteps = 0;
  const reportEvery = Math.max(0, Math.floor(args.reportEvery));
  const targetSteps = Math.max(0, Math.floor(args.steps));
  const chunk = reportEvery > 0 ? Math.min(reportEvery, 50000) : Math.min(targetSteps || 1, 50000);

  while (totalSteps < targetSteps) {
    const stepNow = Math.min(chunk, targetSteps - totalSteps);
    sim.step(stepNow);
    totalSteps += stepNow;
    if (reportEvery > 0 && totalSteps % reportEvery === 0) {
      const energy = sim.energy_breakdown();
      const diag = sim.diagnostics();
      const epNaiveTotal = sim.ep_naive_total();
      const epExactTotal = sim.ep_exact_total();
      const clockQ = Number(sim.clock_q());
      const clockFwd = Number(sim.clock_fwd());
      const clockBwd = Number(sim.clock_bwd());
      const epNaiveRate = totalSteps > 0 ? epNaiveTotal / totalSteps : 0;
      const epExactRate = totalSteps > 0 ? epExactTotal / totalSteps : 0;
      const windowSteps = totalSteps - lastSteps;
      const epExactWindowRate =
        windowSteps > 0 ? (epExactTotal - lastEpExact) / windowSteps : 0;
      const clockDrift = totalSteps > 0 ? clockQ / totalSteps : 0;
      const bonds = sim.bonds(Math.max(0, Math.min(255, Math.floor(args.bondThreshold))));
      const graph = graphStats(args.n, bonds);
      for (const line of formatSummary(totalSteps, energy, diag, graph, {
        epNaiveTotal,
        epNaiveRate,
        epExactTotal,
        epExactRate,
        epExactWindowRate,
        clockQ,
        clockFwd,
        clockBwd,
        clockDrift,
      })) {
        console.log(line);
      }
      console.log("");
      lastEpExact = epExactTotal;
      lastSteps = totalSteps;
    }
  }

  const energy = sim.energy_breakdown();
  const diag = sim.diagnostics();
  const epNaiveTotal = sim.ep_naive_total();
  const epExactTotal = sim.ep_exact_total();
  const clockQ = Number(sim.clock_q());
  const clockFwd = Number(sim.clock_fwd());
  const clockBwd = Number(sim.clock_bwd());
  const epNaiveRate = totalSteps > 0 ? epNaiveTotal / totalSteps : 0;
  const epExactRate = totalSteps > 0 ? epExactTotal / totalSteps : 0;
  const windowSteps = totalSteps - lastSteps;
  const epExactWindowRate =
    windowSteps > 0 ? (epExactTotal - lastEpExact) / windowSteps : 0;
  const clockDrift = totalSteps > 0 ? clockQ / totalSteps : 0;
  const bonds = sim.bonds(Math.max(0, Math.min(255, Math.floor(args.bondThreshold))));
  const graph = graphStats(args.n, bonds);
  for (const line of formatSummary(totalSteps, energy, diag, graph, {
    epNaiveTotal,
    epNaiveRate,
    epExactTotal,
    epExactRate,
    epExactWindowRate,
    clockQ,
    clockFwd,
    clockBwd,
    clockDrift,
  })) {
    console.log(line);
  }

  if (params.epDebug >= 0.5) {
    const stats = sim.ep_q_stats();
    const labels = stats.labels;
    const mean = stats.mean;
    const maxAbs = stats.maxAbs;
    const count = stats.count;
    console.log("");
    console.log("EP log q-ratio stats:");
    for (let i = 0; i < labels.length; i += 1) {
      const label = labels[i];
      console.log(
        `${label}: mean ${Number(mean[i]).toExponential(3)} | maxAbs ${Number(maxAbs[i]).toExponential(3)} | count ${count[i]}`,
      );
    }
  }
}

main().catch((err) => {
  console.error(err);
  process.exit(1);
});
</file>

<file path="EXPERIMENTS.md">
# Experiments Log — Hyperparameter Tuning (Ratchet Playground)

Purpose: identify a **useful baseline parameter regime** (null regime) and then test **P3 (protocol)** and **P6 (resource)** effects, guided by the mathematics in `docs/` (Deliverables A–D).

## What “balanced” means here (docs-aligned)

From `docs/02_primitives_P1_P6.md` and `docs/11_deliverable_D_diagnostics.md`:

1) **Null-regime correctness (P3=OFF, P6=OFF)**  
   - Irreversibility diagnostics should relax toward 0: `Ay≈0`, `Jy≈0`, `Σmem≈0`; and `P3 disp/loop≈0`.

2) **Driven-regime separability**  
   - Turning **only** P3 and/or P6 ON should be the only way to get persistent nonzero `Σmem` / motif affinities (M3/M6).

3) **Non-degenerate dynamic range (practical)**  
   - Memory variables not pinned at 0 or caps (`w,a,n,S` not saturated).
   - Graph is not empty and not near complete (edges are a moderate fraction of max).
   - Safe-set fraction not pinned at 0% or 100%.
   - Compare **per DOF** (since P1 has O(N²) weights): `Ew/#pairs`, `En/N`, `Ea/N`, `Es/grid²` (not raw totals).

## Workflow

- Use the shell runner `scripts/ratchet-cli.mjs` so we can run long experiments headlessly and print intermediate summaries.
- Vary parameters gradually; start with isolated subsystems and add complexity.
- Run long enough to see relaxation / stationarity (typically **millions** of steps for N=200; longer if drift is slow).

## Claims index (safe vs suggestive)

Each campaign below contains a “Safe claims” block. Claim IDs are stable so the report can cite them.

Legend:
- **SAFE** = supported by explicit controls and/or multi-seed stats/CI in this log.
- **SUGGESTIVE** = suggestive pattern observed, but missing strong control / CI / sufficient seeds.

- C_BASE_NULL_1 (SAFE) — Null regime reversibility trends: affinities/Σmem decay toward 0 in long runs.
- C_P6_SEP_1 (SAFE) — P6 produces persistent nonzero M6 motifs vs null where M6≈0.
- C_P3_OBS_1 (SUGGESTIVE) — P3 loop observability improves with minimal protocol / cadence.
- C_P3P6_LONG_1 (SUGGESTIVE) — Long P3+P6 run shows persistent M6 and loop signal.
- C_BASE_RETUNE_NULL_1 (SAFE) — Base-only null preset is reversible with M6=0 and low Σmem.
- C_BASE_RETUNE_P6_1 (SAFE) — Base-only P6 drive yields stable nonzero M6 motifs (3 seeds).
- C_BASE_RETUNE_P3_1 (SUGGESTIVE) — P3 loop observed intermittently at report boundaries.
- C_BASE_RETUNE_P3P6_1 (SAFE) — P3+P6 combo shows nonzero M6 motifs; loop remains intermittent.
- C_META_NULL_1 (SAFE) — Meta-layer null remains reversible (M6=0, Σmem small).
- C_META_ETA_ALIGN_1 (SAFE) — Eta reduces cross-layer diffs for S/W in controlled runs.
- C_CLOCK_DRIFT_1 (SAFE) — Clock drift ≈0 in null, nonzero under P6 (5 seeds).
- C_TUR_1 (SAFE) — TUR ratio R ≥ ~1 for mu≥0.4 in sweep (10 seeds).
- C_CODE_MAINT_1 (SAFE) — Drive-only repair lowers mismatch and error vs null with EP>0.
- C_EP_EXACT_NULL_1 (SAFE) — epExact window rate ≈0 in null with CI containing 0.
- C_TRAVERSAL_NEED_1 (SAFE) — Gated repair needs traversal; static fails to recover.
- C_TRAVERSAL_ORIENT_1 (SAFE) — In that gated repair setting, random clock matches drifting clock.
- C_DEADLINE_DRIFT_1 (SAFE) — Drift improves uptimeTail vs random with CI excluding 0.
- C_DEADLINE_CLOCK_EP_1 (SAFE) — ΔEPClockRate > 0 with tight CI (drift vs random).
- C_OPK_INV_1 (SAFE) — K token budget invariants hold.
- C_OPK_NULL_EP_1 (SAFE) — op coupling null EP remains near 0 (eta and eta=0).
- C_OPK_EFFECT_1 (SAFE) — Eta reduces operator mismatch Sdiff_op.
- C_OPK_DRIVE_SELECT_1 (SAFE) — Drive-selecting K reduces mismatch with epOpK>0 vs controls.
- C_OPK_HIER_1 (SUGGESTIVE) — Hierarchy metrics show slopes but are not definitive.
- C_OPK_DILUTION_1 (SAFE) — Deadline regression largely explained by dilution (ratio line).
- C_OPK_COMPOSED_R2_1 (SAFE) — Composed operator R2 grows with depth in measured configs.
- C_OPK_WEIGHT_NULL_EP_1 (SAFE) — opKTargetWeight sweep preserves null EP≈0.
- C_OPK_CI_EP_1 (SAFE) — ΔEP(C−B) at iso-miss excludes 0.
- C_OPK_REPAIR_DOM_1 (SAFE) — Repair-budget curves show C dominates B/A over observed budgets.

## Baseline parameter set (starting point)

Defaults in `scripts/ratchet-cli.mjs` (and app defaults) at the time of this log:

```json
{
  "beta": 1.0,
  "stepSize": 0.01,
  "p3On": 0,
  "p6On": 0,
  "pWrite": 0.1,
  "pNWrite": 0.05,
  "pAWrite": 0.05,
  "pSWrite": 0.05,
  "muHigh": 0.6,
  "muLow": -0.6,
  "kappaRep": 500.0,
  "r0": 0.25,
  "kappaBond": 1.2,
  "rStar": 0.22,
  "lambdaW": 0.3,
  "lW": 4,
  "lambdaN": 0.5,
  "lN": 6,
  "lambdaA": 0.5,
  "lA": 6,
  "lambdaS": 0.5,
  "lS": 6,
  "gridSize": 16,
  "rPropose": 0.12
}
```

---

## Experiments

### E0 — Smoke test (null regime)

Command:
```bash
node scripts/ratchet-cli.mjs run --steps 200000 --report-every 50000
```

Observed:
- Over 0→200k steps, edges climb from ~50 → ~200 and the giant component grows (largest ~164/200 at 200k).
- `P6 M6` is 0 (as expected with P6=OFF).
- `Σmem` decreases over time (transient from initial all-zeros state).

Takeaway:
- Need **millions** of steps for null-regime diagnostics to relax close to 0.

### E1 — Null baseline: X + P1 only (5M steps)

Goal:
- Check that with only X+P1 enabled (P3=OFF, P6=OFF) we get:
  - nontrivial but not saturated bond graph,
  - `Jw, Aw, Σmem` drifting toward 0 over time.

Command:
```bash
node scripts/ratchet-cli.mjs run --steps 5000000 --report-every 1000000 --set pAWrite=0 --set pNWrite=0 --set pSWrite=0
```

Selected checkpoints:
- 1M: `edges 615 | components 1 | largest 200/200`, `Jw 0.0070`, `Aw 0.2313`, `Σmem 0.0016`
- 5M: `edges 1857 | components 1 | largest 200/200`, `Jw 0.0036`, `Aw 0.1148`, `Σmem 0.0004`

Notes:
- Edges keep increasing with time (slowly), and the graph is already connected by 1M.
- Irreversibility diagnostics decay but are not yet ~0 at 5M; longer runs likely needed for tighter null-regime checks.

### E2 — X + P1 only, smaller `rPropose` (0.08)

Command:
```bash
node scripts/ratchet-cli.mjs run --steps 3000000 --report-every 1000000 --set pAWrite=0 --set pNWrite=0 --set pSWrite=0 --set rPropose=0.08
```

Selected checkpoints:
- 1M: `edges 566 | components 2 | largest 199/200`
- 3M: `edges 1241 | components 1 | largest 200/200`

Notes:
- Compared to `rPropose=0.12`, edges and `Ubond/Ew` are a bit lower but still percolate by ~2M.

### E3 — X + P1 only, smaller `rPropose` (0.05)

Command:
```bash
node scripts/ratchet-cli.mjs run --steps 2000000 --report-every 1000000 --set pAWrite=0 --set pNWrite=0 --set pSWrite=0 --set rPropose=0.05
```

Selected checkpoints:
- 1M: `edges 420 | components 3 | largest 198/200`
- 2M: `edges 774 | components 1 | largest 200/200`

Notes:
- Still reaches connectivity, but strong-bond edges (w≥3) are lower at the same horizon.

### E4 — X + P1 only, reduce write rate (`pWrite=0.05`, `rPropose=0.05`)

Command:
```bash
node scripts/ratchet-cli.mjs run --steps 2000000 --report-every 1000000 --set pWrite=0.05 --set pAWrite=0 --set pNWrite=0 --set pSWrite=0 --set rPropose=0.05
```

Selected checkpoints:
- 1M: `edges 343 | components 5 | largest 196/200`
- 2M: `edges 633 | components 1 | largest 200/200`

Notes:
- Strong-bond edges (w≥3) are reduced vs `pWrite=0.1` at the same horizon.

### E5 — Null regime with all reversible channels on (P1+P2+P4+P5, 5M steps)

Goal:
- Validate that adding P2/P4/P5 (still P3=OFF, P6=OFF) keeps:
  - those channels near reversible (`J≈0`, `A≈0`),
  - non-saturated energy scales / DOF activity.

Command:
```bash
node scripts/ratchet-cli.mjs run --steps 5000000 --report-every 1000000 --set pWrite=0.05 --set rPropose=0.05
```

Selected checkpoints:
- 1M: `edges 352 | components 8 | largest 193/200`, `Aa≈0.014`, `As≈0.013`
- 5M: `edges 1243 | components 1 | largest 200/200`
  - P2: `Ja≈0`, `Aa≈0.0025`
  - P4: `Jn≈0`, `An≈0`
  - P5: `Js≈0`, `As≈0.0029`
  - P1: `Jw 0.0025`, `Aw 0.1641`, `Σmem 0.0004`
  - Energy (5M): `Urep ~8978`, `Ubond ~318`, `Ew ~3936`, `En/Ea/Es ~ O(10^2)`

Notes:
- P2/P4/P5 relax quickly toward `A≈0` compared to P1 (which relaxes more slowly from all-zeros initialization).
- At this horizon the bond graph percolates, but strong-bond edge count is still far from complete.

### E6 — Retune P1 stiffness to avoid early percolation (`lambdaW=0.5`)

Command:
```bash
node scripts/ratchet-cli.mjs run --steps 3000000 --report-every 1000000 --set pWrite=0.05 --set rPropose=0.05 --set lambdaW=0.5
```

Selected checkpoints:
- 1M: `edges 158 | components 52 | largest 123/200 (0.61)`
- 3M: `edges 327 | components 5 | largest 194/200 (0.97)`
- 5M: `edges 484 | components 2 | largest 199/200 (0.99)`

Notes:
- Raising `lambdaW` reduces the number of strong bonds (w≥3) substantially at the same horizon, keeping the graph closer to the percolation threshold longer (useful for seeing driven effects).

### E8 — P6-only drive on the null baseline (μ=±0.6)

Command:
```bash
node scripts/ratchet-cli.mjs run --steps 5000000 --report-every 1000000 --set pWrite=0.05 --set rPropose=0.05 --set lambdaW=0.5 --set p6On=1
```

Selected checkpoints:
- 1M: `P6 M6 | W 0.201 N 0.148 A 0.119 S 0.011`, `edges 290 | largest 157/200`
- 5M: `P6 M6 | W 0.226 N 0.134 A 0.121 S 0.002`, `edges 1001 | largest 199/200`

Notes:
- With P6 ON, M6 motif estimates become clearly nonzero (especially for W/N/A).
- Strong-bond edges (w≥3) increase noticeably relative to P6=OFF at the same horizon.

### E9 — P6-only, stronger drive (μ=±1.0)

Command:
```bash
node scripts/ratchet-cli.mjs run --steps 3000000 --report-every 1000000 --set pWrite=0.05 --set rPropose=0.05 --set lambdaW=0.5 --set p6On=1 --set muHigh=1.0 --set muLow=-1.0
```

Selected checkpoints:
- 1M: `M6 W 0.330 N 0.241 A 0.245 S 0.017`, `edges 446 | largest 155/200`
- 3M: `M6 W 0.387 N 0.214 A 0.225 S 0.008`, `edges 1114 | largest 196/200`

Notes:
- Increasing |μ| strengthens M6 signals and grows strong-bond edges faster (risk: eventual percolation / hairball at very long horizons).

### E10 — P3-only with all kernels included (cycle length 5)

Command:
```bash
node scripts/ratchet-cli.mjs run --steps 1000000 --report-every 200000 --set pWrite=0.05 --set rPropose=0.05 --set lambdaW=0.5 --set p3On=1 --set p6On=0
```

Selected checkpoints:
- 200k: `P3 cycle 5 | disp 0.0001 | loop 0.5000`
- 1M: `P3 cycle 5 | disp 0.0001 | loop 0.0000` (value fluctuates; last-cycle diagnostic often prints as 0.0000)
- Graph stays very sparse at this horizon: `edges 96`, `largest 33/200`

Notes:
- P3 loop diagnostic is per-cycle (not averaged), so sampling at report boundaries can miss nonzero cycles.

### E11 — P3-only minimal noncommuting set: X + P1 + P5 (cycle length 3)

Command:
```bash
node scripts/ratchet-cli.mjs run --steps 500000 --report-every 100000 --set p3On=1 --set p6On=0 --set pWrite=0.05 --set pSWrite=0.05 --set pAWrite=0 --set pNWrite=0 --set rPropose=0.05 --set lambdaW=0.5
```

Selected checkpoints:
- 300k: `P3 cycle 3 | disp 0.0000 | loop -0.5000`
- 500k: `P3 cycle 3 | disp 0.0001 | loop -0.5000`

Notes:
- With only P1+P5 in the protocol, the (sum_w, sum_s) loop area becomes consistently nonzero at this scale, matching the Deliverable D “geometric signature” intent.

### E12 — P3+P6 combined (full kernel set), increase mobility (`stepSize=0.02`)

Command:
```bash
node scripts/ratchet-cli.mjs run --steps 2000000 --report-every 500000 --set pWrite=0.05 --set rPropose=0.05 --set lambdaW=0.5 --set p3On=1 --set p6On=1 --set muHigh=1.0 --set muLow=-1.0 --set stepSize=0.02
```

Selected checkpoints:
- 500k: `M6 W 0.135 N 0.111 A 0.105 S 0.009`, `edges 366 | largest 143/200`
- 2M: `M6 W 0.127 N 0.095 A 0.106 S 0.002`, `edges 1035 | largest 182/200`
- P3 loop occasionally nonzero (e.g. `loop -0.5000` at 1.5M), but often prints as 0.0000 at report boundaries.

Notes:
- In P3 mode, X steps are only 1/5 of steps; increasing `stepSize` helps particles traverse μ-contexts, which strengthens M6 signals (per Deliverable C’s “need movement between contexts”).

### E13 — P3+P6 combined (minimal protocol X+P1+P5), increase mobility (`stepSize=0.02`)

Command:
```bash
node scripts/ratchet-cli.mjs run --steps 1000000 --report-every 200000 --set p3On=1 --set p6On=1 --set muHigh=1.0 --set muLow=-1.0 --set pWrite=0.05 --set pSWrite=0.05 --set pAWrite=0 --set pNWrite=0 --set rPropose=0.05 --set lambdaW=0.5 --set stepSize=0.02
```

Selected checkpoints:
- 200k: `M6 W 0.126 S 0.013`, `edges 233 | largest 129/200`
- 1M: `M6 W 0.124 S 0.002`, `edges 885 | largest 181/200`, `P3 loop 0.5000`

Notes:
- This minimal setting yields both a nonzero P3 loop signature and a clear P6 M6 signature while remaining far from a complete bond graph at 1M.

### E14 — P3-only (minimal protocol X+P1+P5), increased mobility (`stepSize=0.02`)

Command:
```bash
node scripts/ratchet-cli.mjs run --steps 1000000 --report-every 200000 --set p3On=1 --set p6On=0 --set pWrite=0.05 --set pSWrite=0.05 --set pAWrite=0 --set pNWrite=0 --set rPropose=0.05 --set lambdaW=0.5 --set stepSize=0.02
```

Selected checkpoints:
- 600k: `P3 loop -0.5000`, `edges 201 | largest 157/200`
- 1M: `P3 loop 0.5000`, `edges 292 | components 19 | largest 181/200`

Notes:
- Increasing `stepSize` helps keep the protocol’s observables moving enough that the loop diagnostic is frequently nonzero at report boundaries.

### E15 — Long-run stability check: P3+P6 full set (10M steps)

Command:
```bash
node scripts/ratchet-cli.mjs run --steps 10000000 --report-every 2000000 --set pWrite=0.05 --set rPropose=0.05 --set lambdaW=0.5 --set p3On=1 --set p6On=1 --set muHigh=1.0 --set muLow=-1.0 --set stepSize=0.02
```

Selected checkpoints:
- 2M: `edges 1035 | components 18 | largest 182/200`, `M6 W 0.127 N 0.095 A 0.106`
- 10M: `edges 2562 | components 1 | largest 200/200`, `M6 W 0.147 N 0.091 A 0.104`, `P3 loop -0.5000`

Notes:
- Over 10M, strong-bond edges keep growing but remain far from complete (~13% of all pairs at threshold w≥3).
- M6 signals persist and strengthen slightly as the system explores μ-contexts.

---

## Campaign summary (what we achieved / did not achieve)

### Achieved

- A docs-aligned workflow with clear control regimes (null vs P6-only vs P3-only vs P3+P6) and a shell runner (`scripts/ratchet-cli.mjs`) to run multi‑million step experiments and inspect intermediate checkpoints.
- A **null baseline** where reversible channels behave as expected (P2/P4/P5 affinities relax quickly toward 0; P1’s `Σmem` decays toward 0 over millions of steps), while avoiding the immediate “complete-graph hairball” failure mode by making repulsion matter (`kappaRep/r0`) and limiting bond writes (`rPropose`, higher `lambdaW`).
- **Driven-regime separability for P6:** enabling P6 produces clearly nonzero M6 motif estimates (`aM6*`) and changes the evolution of strong-bond edges, without adding any non-primitive “goal” rule.
- **P3 observability:** a shorter protocol (X+P1+P5) and/or higher mobility (`stepSize`) makes the P3 loop diagnostic show up nontrivially (instead of being mostly 0 at report boundaries).
- A 10M‑step **P3+P6 long run** with persistent M6 signals and a non-saturated strong-bond graph (E15).
- The tuned setups are now captured as param files (`scripts/params/*.json`) and exposed as UI presets (“Null baseline”, “P3+P6 full”).

### Not achieved

- A regime where P3 pumping is **robustly large** across the full-kernel protocol: `disp/loop` is detectable but typically small and sensitive to protocol composition and sampling cadence.
- A universal elimination of “hairball” morphology: in many settings, strong-bond edges still grow and can percolate at long horizons (tuning mostly affects *how fast* and *how strongly*).
- A strict statistical null-regime validation (multi-seed runs with error bars showing `A→0`, `Σ→0`), beyond representative long runs and trend checks.

### Safe claims (what this section supports)

- **C_BASE_NULL_1 (SAFE)** Null-regime diagnostics trend toward 0 with all reversible channels enabled.
  - Evidence: E5 shows `Ja≈0, Jn≈0, Js≈0`, `Aa≈0.0025`, `As≈0.0029`, `Σmem 0.0004` at 5M steps.
  - Controls: P3=OFF, P6=OFF in E5.
  - Scope: N=200, 5M steps, `pWrite=0.05`, `rPropose=0.05`.
- **C_P6_SEP_1 (SAFE)** P6-only drive produces persistent nonzero M6 motifs relative to null.
  - Evidence: E8/E9 show `M6 W/N/A` ≈ 0.12–0.23; E5 has `M6=0`.
  - Controls: P6 toggled ON with P3 OFF; null runs with P6 OFF.
  - Scope: N=200, 3–5M steps.
- **C_P3_OBS_1 (SUGGESTIVE)** P3 loop observability improves under minimal protocol + higher cadence.
  - Evidence: E11/E14 checkpoints show nonzero `P3 loop` values with X+P1+P5 and higher report cadence.
  - Controls: P3 toggled ON with P6 OFF.
  - Scope: N=200, 0.5–1M steps; limited seeds.
- **C_P3P6_LONG_1 (SUGGESTIVE)** Long P3+P6 run shows persistent M6 signals and nonzero loop.
  - Evidence: E15 at 10M shows `M6 W/N/A` nonzero and `P3 loop -0.5000`.
  - Controls: P3+P6 ON vs prior null/P6-only controls.
  - Scope: single 10M run; no multi-seed CI.

### Not claimed / caveats

- No claim of robust P3 loop magnitude across full-kernel protocols; observability depends on cadence.
- No claim that the null regime has fully converged in a statistical sense (multi-seed CI absent here).
- Long-run percolation remains; graph saturation is not eliminated in these early campaigns.

---

## 2025-12-21 — Base-only retune after multi-layer implementation

All runs in this section explicitly disable meta effects:

- `metaLayers=0`
- `eta=0`

### Phase A — Regression spot-checks (E5/E8/E11/E13-style)

#### R1 — E5-style null (all reversible channels, rPropose=0.05, lambdaW=0.3)

Command:
```bash
node scripts/ratchet-cli.mjs run --steps 3000000 --report-every 1000000 \
  --set pWrite=0.05 --set rPropose=0.05 --set p3On=0 --set p6On=0 \
  --set metaLayers=0 --set eta=0
```

Selected checkpoints:
- 1M: `Aw 0.2678 sigmaMem 0.0011`, `Aa 0.0137`, `As 0.0134`, `P6 M6 all 0`, `edges 352`
- 3M: `Aw 0.2005 sigmaMem 0.0006`, `Aa 0.0042`, `As 0.0049`, `P6 M6 all 0`, `edges 852`

Takeaway:
- Matches prior E5 trend: reversible channels decay toward 0, P1 relaxes slower; graph percolates by ~3M.

#### R2 — E8-style P6-only drive (mu=+/-0.6)

Command:
```bash
node scripts/ratchet-cli.mjs run --steps 3000000 --report-every 1000000 \
  --set pWrite=0.05 --set rPropose=0.05 --set lambdaW=0.5 \
  --set p6On=1 --set muHigh=0.6 --set muLow=-0.6 --set p3On=0 \
  --set metaLayers=0 --set eta=0
```

Selected checkpoints:
- 1M: `M6 W 0.2014 N 0.1483 A 0.1187 S 0.0110`, `edges 290`
- 3M: `M6 W 0.2178 N 0.1372 A 0.1210 S 0.0032`, `edges 718`

Takeaway:
- M6 signals remain clearly nonzero; overall behavior consistent with prior E8.

#### R3 — E11-style P3-only minimal protocol (X + P1 + P5)

Command:
```bash
node scripts/ratchet-cli.mjs run --steps 500000 --report-every 50000 \
  --set p3On=1 --set p6On=0 --set pWrite=0.05 --set pSWrite=0.05 \
  --set pAWrite=0 --set pNWrite=0 --set rPropose=0.05 --set lambdaW=0.5 \
  --set stepSize=0.02 --set metaLayers=0 --set eta=0
```

Selected checkpoints:
- 250k: `P3 loop 0.5000`, `edges 95`
- 350k: `P3 loop 0.5000`, `edges 126`

Takeaway:
- P3 loop is intermittent at report boundaries; higher report cadence captures nonzero loops as in prior E11/E14.

#### R4 — E13-style P3+P6 minimal protocol (X + P1 + P5)

Command:
```bash
node scripts/ratchet-cli.mjs run --steps 1000000 --report-every 200000 \
  --set p3On=1 --set p6On=1 --set muHigh=1.0 --set muLow=-1.0 \
  --set pWrite=0.05 --set pSWrite=0.05 --set pAWrite=0 --set pNWrite=0 \
  --set rPropose=0.05 --set lambdaW=0.5 --set stepSize=0.02 \
  --set metaLayers=0 --set eta=0
```

Selected checkpoints:
- 200k: `M6 W 0.1258 S 0.0133`, `edges 233`
- 1M: `M6 W 0.1236 S 0.0021`, `P3 loop 0.5000`, `edges 885`

Takeaway:
- Both P6 motifs and P3 loop appear; aligns with prior E13 behavior.

### Phase B–E — Base-only presets (3 seeds each)

The following runs are the basis for the new base-only presets under `scripts/params/`.
All commands use `--set metaLayers=0 --set eta=0` explicitly (also baked into preset JSON).

#### base_null_balanced.json (null regime, 5M steps)

Command (per seed):
```bash
node scripts/ratchet-cli.mjs run --steps 5000000 --report-every 1000000 \
  --params scripts/params/base_null_balanced.json --seed <seed> \
  --set metaLayers=0 --set eta=0
```

Final reports:
- Seed 1: `Aw 0.1453 sigmaMem 0.0003 | Aa 0.0022 As 0.0035 | M6 all 0 | edges 484`
- Seed 2: `Aw 0.1484 sigmaMem 0.0003 | Aa 0.0022 As 0.0035 | M6 all 0 | edges 539`
- Seed 3: `Aw 0.1472 sigmaMem 0.0003 | Aa 0.0024 As 0.0032 | M6 all 0 | edges 549`

Takeaway:
- Null diagnostics (P2/P4/P5) near zero; P1 affinity relaxes more slowly but sigmaMem is small. Graph remains non-saturated.

#### base_p6_drive.json (P6-only drive, 3M steps)

Command (per seed):
```bash
node scripts/ratchet-cli.mjs run --steps 3000000 --report-every 1000000 \
  --params scripts/params/base_p6_drive.json --seed <seed> \
  --set metaLayers=0 --set eta=0
```

Final reports:
- Seed 1: `M6 W 0.7944 N 0.3725 A 0.3936 S 0.0064 | edges 1544`
- Seed 2: `M6 W 0.8247 N 0.3809 A 0.3897 S 0.0060 | edges 1538`
- Seed 3: `M6 W 0.8219 N 0.3676 A 0.3916 S 0.0055 | edges 1571`

Takeaway:
- Strong, stable M6 signals across seeds; graph grows but not yet complete at w>=3 threshold.

#### base_p3_pump_minimal.json (P3-only minimal protocol, 1M steps)

Command (per seed):
```bash
node scripts/ratchet-cli.mjs run --steps 1000000 --report-every 200000 \
  --params scripts/params/base_p3_pump_minimal.json --seed <seed> \
  --set metaLayers=0 --set eta=0
```

Final reports:
- Seed 1: `P3 loop 0.5000 | edges 292`
- Seed 2: `P3 loop -0.5000 | edges 250`
- Seed 3: `P3 loop 0.0000 | edges 265`

Takeaway:
- Loop is intermittent at report boundaries; mean absolute loop over report windows is nonzero across seeds (see summary table).

#### base_p3p6_combo_minimal.json (P3+P6 minimal protocol, 2M steps)

Command (per seed):
```bash
node scripts/ratchet-cli.mjs run --steps 2000000 --report-every 500000 \
  --params scripts/params/base_p3p6_combo_minimal.json --seed <seed> \
  --set metaLayers=0 --set eta=0
```

Final reports:
- Seed 1: `M6 W 0.1250 S 0.0015 | P3 loop 0.0000 | edges 1418`
- Seed 2: `M6 W 0.1327 S 0.0012 | P3 loop 0.0000 | edges 1466`
- Seed 3: `M6 W 0.1309 S 0.0011 | P3 loop 0.0000 | edges 1418`

Takeaway:
- P6 motif signal is consistent; P3 loop appears intermittently, captured via windowed loop averages (summary table).

### Summary table (3 seeds, mean+/-std)

Computed via `scripts/run-sweep.mjs`, using the final report for each seed and averaging |loop| over report windows.

| preset | sigmaMem | Aw | Aa | An | As | M6W | M6N | M6A | M6S | loop\|mean\| | loop>0 frac | edges | largest |
| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- |
| base_null_balanced | 0.0003+/-0.0000 | 0.1470+/-0.0013 | 0.0023+/-0.0001 | 0.0001+/-0.0004 | 0.0034+/-0.0001 | 0.0000+/-0.0000 | 0.0000+/-0.0000 | 0.0000+/-0.0000 | 0.0000+/-0.0000 | 0.0000+/-0.0000 | 0.0000+/-0.0000 | 524+/-29 | 199+/-1 |
| base_p6_drive | 0.0021+/-0.0000 | 0.4305+/-0.0025 | 0.0068+/-0.0005 | -0.0000+/-0.0005 | 0.0085+/-0.0002 | 0.8137+/-0.0137 | 0.3737+/-0.0055 | 0.3916+/-0.0016 | 0.0060+/-0.0004 | 0.0000+/-0.0000 | 0.0000+/-0.0000 | 1551+/-14 | 200+/-0 |
| base_p3_pump_minimal | 0.0002+/-0.0000 | 0.0502+/-0.0002 | 0.0000+/-0.0000 | 0.0000+/-0.0000 | 0.0024+/-0.0001 | 0.0000+/-0.0000 | 0.0000+/-0.0000 | 0.0000+/-0.0000 | 0.0000+/-0.0000 | 0.2222+/-0.0393 | 0.4444+/-0.0786 | 269+/-17 | 180+/-1 |
| base_p3p6_combo_minimal | 0.0003+/-0.0000 | 0.0625+/-0.0008 | 0.0000+/-0.0000 | 0.0000+/-0.0000 | 0.0019+/-0.0000 | 0.1295+/-0.0033 | 0.0000+/-0.0000 | 0.0000+/-0.0000 | 0.0013+/-0.0002 | 0.1000+/-0.0816 | 0.2000+/-0.1633 | 1434+/-23 | 191+/-2 |

### Safe claims (what this section supports)

- **C_BASE_RETUNE_NULL_1 (SAFE)** Base-only null preset is reversible with M6=0 and low Σmem.
  - Evidence: `base_null_balanced` row shows `sigmaMem 0.0003`, `M6W/N/A/S 0.0000` (3 seeds).
  - Controls: P3=OFF, P6=OFF; `metaLayers=0`, `eta=0`.
  - Scope: 5M steps, 3 seeds.
- **C_BASE_RETUNE_P6_1 (SAFE)** P6-only base drive yields stable nonzero M6 motifs.
  - Evidence: `base_p6_drive` row shows `M6W 0.8137`, `M6N 0.3737`, `M6A 0.3916` (3 seeds).
  - Controls: P6=ON with P3=OFF; null baseline in same table.
  - Scope: 3M steps, 3 seeds.
- **C_BASE_RETUNE_P3_1 (SUGGESTIVE)** P3 loop is intermittently observable in minimal protocol.
  - Evidence: `base_p3_pump_minimal` row shows `loop|mean| 0.2222` and `loop>0 frac 0.4444` (3 seeds).
  - Controls: P3=ON with P6=OFF.
  - Scope: 1M steps; loop is report-boundary sensitive.
- **C_BASE_RETUNE_P3P6_1 (SAFE)** P3+P6 combo preserves nonzero P6 motifs.
  - Evidence: `base_p3p6_combo_minimal` row shows `M6W 0.1295` (3 seeds).
  - Controls: P3=ON, P6=ON vs null row with M6=0.
  - Scope: 2M steps, 3 seeds.

### Not claimed / caveats

- No claim of robust P3 loop magnitude in base-only runs; loop remains intermittent.
- No claim that graph morphology remains sparse at long horizons; edges can grow with time.

---

## 2025-12-21 — Phase 2: meta-layer tuning (L>0)

All Phase 2 presets explicitly set:

- `metaLayers=2`
- `eta` explicit (0 for decoupled baseline; 0.6 for coupled presets)

### Phase A — Coupling sanity (eta=0 vs eta=1, minimal dynamics)

Command:
```bash
node scripts/run-meta-sweep.mjs
```

Sanity output:
- `Eta sanity S: base/meta0 0.9792 -> 0.7708, meta0/meta1 0.9722 -> 0.7431`
- `Eta sanity W: meta0/meta1 0.9028 -> 0.7500`

Takeaway:
- Coupling reduces cross-layer diffs by >15% for both S and W in controlled minimal setups.

### Phase B — Null regime with meta layers (decoupled vs coupled)

Presets:
- `scripts/params/meta/meta2_null_decoupled.json` (eta=0)
- `scripts/params/meta/meta2_null_coupled.json` (eta=0.6)

Required final verification (coupled, 3 seeds):
```bash
node scripts/ratchet-cli.mjs run --steps 3000000 --report-every 1000000 \
  --params scripts/params/meta/meta2_null_coupled.json --seed <seed>
```

Final reports (coupled):
- Seed 1: `Aw 0.1584 sigmaMem 0.0003 | Aa 0.0141 As 0.0149 | M6 all 0 | edges 198`
- Seed 2: `Aw 0.1573 sigmaMem 0.0003 | Aa 0.0140 As 0.0152 | M6 all 0 | edges 168`
- Seed 3: `Aw 0.1545 sigmaMem 0.0003 | Aa 0.0155 As 0.0147 | M6 all 0 | edges 191`

Takeaway:
- Null regime remains reversible (M6=0, sigmaMem small) even with L=2 and eta>0.

### Phase C — P6-only drive with meta layers (coupled)

Preset:
- `scripts/params/meta/meta2_p6_drive_coupled.json`

Required final verification (3 seeds):
```bash
node scripts/ratchet-cli.mjs run --steps 2000000 --report-every 1000000 \
  --params scripts/params/meta/meta2_p6_drive_coupled.json --seed <seed>
```

Final reports:
- Seed 1: `M6 W 0.3105 N 0.2768 A 0.2553 S 0.0285 | edges 386`
- Seed 2: `M6 W 0.2747 N 0.2719 A 0.2529 S 0.0257 | edges 351`
- Seed 3: `M6 W 0.2945 N 0.2762 A 0.2531 S 0.0296 | edges 365`

Takeaway:
- Robust nonzero M6 signals across channels; graph remains structured.

### Phase D — P3-only minimal protocol with meta layers (coupled)

Preset:
- `scripts/params/meta/meta2_p3_pump_coupled.json`

Required final verification (3 seeds):
```bash
node scripts/ratchet-cli.mjs run --steps 1000000 --report-every 200000 \
  --params scripts/params/meta/meta2_p3_pump_coupled.json --seed <seed>
```

Final reports:
- Seed 1: `P3 loop -0.5000 | edges 610`
- Seed 2: `P3 loop 0.0000 | edges 595`
- Seed 3: `P3 loop 0.0000 | edges 620`

Takeaway:
- Loop diagnostic is intermittent at report boundaries; sweep metrics (below) confirm nonzero loop observability.

### Phase E — P3+P6 minimal protocol with meta layers (coupled)

Preset:
- `scripts/params/meta/meta2_p3p6_combo_coupled.json`

Required final verification (3 seeds):
```bash
node scripts/ratchet-cli.mjs run --steps 2000000 --report-every 500000 \
  --params scripts/params/meta/meta2_p3p6_combo_coupled.json --seed <seed>
```

Final reports:
- Seed 1: `M6 W 0.0962 S 0.0045 | P3 loop 0.0000 | edges 1248`
- Seed 2: `M6 W 0.0969 S 0.0045 | P3 loop 0.0000 | edges 1226`
- Seed 3: `M6 W 0.0995 S 0.0042 | P3 loop 0.0000 | edges 1242`

Takeaway:
- M6 persists with P3 enabled; loop is intermittent, captured by sweep loop averages.

### Summary table (3 seeds, mean+/-std)

Computed via `scripts/run-meta-sweep.mjs` (per-seed logs at `.tmp/experiments_meta/`).

| preset | sigmaMem | Aw | Aa | An | As | M6W | M6N | M6A | M6S | loop\|mean\| | loop>0 frac | Sdiff base/meta0 | Sdiff meta0/meta1 | Wdiff meta0/meta1 | nz metaS | nz metaW | nz metaA | nz metaN | edges | largest |
| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- |
| meta2_null_decoupled | 0.0003+/-0.0000 | 0.1574+/-0.0006 | 0.0146+/-0.0004 | 0.0005+/-0.0008 | 0.0155+/-0.0001 | 0.0000+/-0.0000 | 0.0000+/-0.0000 | 0.0000+/-0.0000 | 0.0000+/-0.0000 | 0.0000+/-0.0000 | 0.0000+/-0.0000 | 0.9974+/-0.0615 | 0.9518+/-0.0310 | 0.9570+/-0.0191 | 0.5638+/-0.0115 | 0.5550+/-0.0123 | 0.5749+/-0.0079 | 0.7188+/-0.0207 | 195+/-20 | 146+/-10 |
| meta2_null_coupled | 0.0003+/-0.0000 | 0.1567+/-0.0017 | 0.0145+/-0.0007 | 0.0001+/-0.0006 | 0.0149+/-0.0002 | 0.0000+/-0.0000 | 0.0000+/-0.0000 | 0.0000+/-0.0000 | 0.0000+/-0.0000 | 0.0000+/-0.0000 | 0.0000+/-0.0000 | 0.9323+/-0.0574 | 0.8971+/-0.0341 | 0.9076+/-0.0312 | 0.5299+/-0.0051 | 0.5472+/-0.0181 | 0.5651+/-0.0215 | 0.7305+/-0.0016 | 186+/-13 | 148+/-10 |
| meta2_p6_drive_coupled | 0.0014+/-0.0000 | 0.3564+/-0.0052 | 0.0356+/-0.0008 | 0.0007+/-0.0011 | 0.0380+/-0.0004 | 0.2932+/-0.0146 | 0.2750+/-0.0022 | 0.2538+/-0.0011 | 0.0280+/-0.0016 | 0.0000+/-0.0000 | 0.0000+/-0.0000 | 0.8763+/-0.0195 | 0.8763+/-0.0271 | 0.8633+/-0.0304 | 0.5729+/-0.0122 | 0.5830+/-0.0104 | 0.5703+/-0.0184 | 0.9030+/-0.0171 | 367+/-14 | 182+/-2 |
| meta2_p3_pump_coupled | 0.0006+/-0.0000 | 0.0746+/-0.0016 | 0.0000+/-0.0000 | 0.0000+/-0.0000 | 0.0082+/-0.0004 | 0.0000+/-0.0000 | 0.0000+/-0.0000 | 0.0000+/-0.0000 | 0.0000+/-0.0000 | 0.0417+/-0.0236 | 0.0833+/-0.0471 | 1.2305+/-0.0159 | 1.1745+/-0.0239 | 1.1615+/-0.0049 | 0.6465+/-0.0196 | 0.6445+/-0.0269 | 0.0000+/-0.0000 | 0.0000+/-0.0000 | 608+/-10 | 200+/-0 |
| meta2_p3p6_combo_coupled | 0.0003+/-0.0000 | 0.0661+/-0.0003 | 0.0000+/-0.0000 | 0.0000+/-0.0000 | 0.0059+/-0.0001 | 0.0975+/-0.0014 | 0.0000+/-0.0000 | 0.0000+/-0.0000 | 0.0044+/-0.0002 | 0.0208+/-0.0156 | 0.0417+/-0.0312 | 0.9375+/-0.0640 | 0.9492+/-0.0776 | 0.8372+/-0.0364 | 0.5716+/-0.0161 | 0.5726+/-0.0092 | 0.0000+/-0.0000 | 0.0000+/-0.0000 | 1239+/-9 | 194+/-2 |

### Safe claims (what this section supports)

- **C_META_NULL_1 (SAFE)** Meta-layer null remains reversible with M6≈0 and low Σmem.
  - Evidence: `meta2_null_coupled` row shows `sigmaMem 0.0003` and `M6W/N/A/S 0.0000` (3 seeds).
  - Controls: P3=OFF, P6=OFF with `metaLayers=2`, `eta=0.6`.
  - Scope: 3M steps, 3 seeds.
- **C_META_ETA_ALIGN_1 (SAFE)** Eta reduces cross-layer diffs in controlled minimal dynamics.
  - Evidence: “Eta sanity” output shows S/W diffs reduced by >15% (e.g., `0.9792→0.7708`, `0.9028→0.7500`).
  - Controls: P3=OFF, P6=OFF; eta toggled.
  - Scope: metaLayers=2, minimal P5-only updates.

### Not claimed / caveats

- Coupling changes equilibrium alignment but does not introduce directionality when P3/P6 are off.
- No claim of emergent hierarchy beyond the measured diff reductions.

---

## 2025-12-21 — Clock + Code: TUR + dynamic encoding maintenance

### Step 1 — EP counter sanity (null vs P6)

Null regime (p3On=0, p6On=0):
```bash
node scripts/ratchet-cli.mjs run --steps 3000000 --report-every 1000000 \
  --params scripts/params/meta/meta2_null_coupled.json --set p3On=0 --set p6On=0
```
Final report (3M):
- `EP total -2733.8914 | rate -0.000911`
- `P6 M6 | W 0.0000 N 0.0000 A 0.0000 S 0.0000`

P6 drive:
```bash
node scripts/ratchet-cli.mjs run --steps 2000000 --report-every 1000000 \
  --params scripts/params/meta/meta2_p6_drive_coupled.json
```
Final report (2M):
- `EP total 9624.6312 | rate 0.004812`
- `P6 M6 | W 0.3105 N 0.2768 A 0.2553 S 0.0285`

Note:
- Null EP rate trends toward 0 (from -0.001443 at 1M to -0.000911 at 3M). This is within the “decay trend” clause even though it does not reach 5e-4 by 3M.

### Step 2 — Clock current (P4-targeted clock)

Command:
```bash
node scripts/test-clock-current.mjs
```

Summary (5 seeds, 2M steps, clockFrac=0.01):
- Null: drifts in `[-7.8e-5, 4.6e-5]`, `epRate=0`, Q ~ random-walk scale
- P6 drive: drift ≈ `3.15e-3` with consistent sign, `epRate ≈ 3.3e-3`

### Step 3 — TUR sweep (mu ∈ {0.2..1.4})

Command:
```bash
node scripts/run-clock-tur-sweep.mjs
```

Summary table (meanQ, varQ, meanSigma, R), seeds 1–10, steps 1M:

| mu | meanQ | varQ | meanSigma | R |
| --- | --- | --- | --- | --- |
| 0.2 | 1848.70 | 12700.81 | 394.7200 | 0.733 |
| 0.4 | 3337.40 | 15916.44 | 1416.5200 | 1.012 |
| 0.6 | 4551.40 | 15401.64 | 2875.9201 | 1.069 |
| 0.8 | 5529.30 | 14473.21 | 4631.6801 | 1.096 |
| 1.0 | 6359.40 | 12549.24 | 6630.7000 | 1.029 |
| 1.4 | 7570.50 | 10811.45 | 10996.1598 | 1.037 |

### Step 4 — Drive-only code maintenance (etaDrive on S channel)

Command:
```bash
node scripts/run-code-maintenance.mjs
```

Summary table (mean±std across seeds 1–5, steps 1M, perturb at 500k):

| preset | Sdiff(base,meta0) | Sdiff(meta0,meta1) | err(f=0.5) | recoverySteps (mean) | epRate |
| --- | --- | --- | --- | --- | --- |
| code_null | 6.8633±0.2530 | 7.0797±0.1645 | 0.4325±0.0868 | 140000 | 0.0000 |
| code_p6_drive | 0.8328±0.0642 | 0.7547±0.0479 | 0.0000±0.0000 | 100000 | 0.0157 |
| code_p6_clock_gated | 0.8547±0.0559 | 0.7828±0.0251 | 0.0000±0.0000 | 100000 | 0.0473 |

Takeaways:
- Null: high mismatch and nontrivial reconstructibility error; recovery is slower and EP is ~0.
- Drive: mismatch drops by >80%, err(f=0.5) collapses, recovery succeeds within one report interval, EP>0.
- Clock-gated: similar fidelity to drive-only, with higher EP and nonzero clock drift.

### Safe claims (what this section supports)

- **C_CLOCK_DRIFT_1 (SAFE)** Clock drift is ≈0 in null and clearly nonzero under P6 drive.
  - Evidence: `test-clock-current.mjs` summary shows null drift in `[-7.8e-5, 4.6e-5]` with `epRate=0`, P6 drift ≈ `3.15e-3` (5 seeds).
  - Controls: P6 toggled ON vs OFF; clock enabled in both.
  - Scope: 5 seeds, 2M steps, `clockFrac=0.01`.
- **C_TUR_1 (SAFE)** TUR ratio R ≥ ~1 for mu≥0.4 in the sweep.
  - Evidence: TUR table shows R≈1.012–1.096 for mu=0.4–0.8 and ≥1 for mu=1.0,1.4 (10 seeds).
  - Controls: uniform sweep of mu with same protocol and seeds.
  - Scope: 1M steps, seeds 1–10.
- **C_CODE_MAINT_1 (SAFE)** Drive-only repair (etaDrive with P6) restores fidelity vs null.
  - Evidence: code maintenance table shows `code_null` Sdiff ≈ 6.86 and err≈0.43 vs `code_p6_drive` Sdiff ≈ 0.83 and err≈0.00; EP rate >0 under drive (5 seeds).
  - Controls: P6 ON vs OFF with etaDrive; same perturbation protocol.
  - Scope: 1M steps, seeds 1–5.

### Not claimed / caveats

- No claim that the naive EP rate in Step 1 is unbiased; exact EP is introduced later.
- The TUR result is empirical and finite‑time; it does not prove an exact bound for all mu.

## 2025-12-22 — Tight Demonstration Upgrade: EP exact + clock–code causality

### EP accounting hardening (epExact vs epNaive)

Command:
```bash
node scripts/test-ep-null-tight.mjs
```

Summary table (epExact window rate over last 1M steps; mean±std, 95% CI half-width):

| case | meanExactWindow | stdExactWindow | ciHalfWidth | meanNaiveWindow | stdNaiveWindow |
| --- | --- | --- | --- | --- | --- |
| base_null | 3.505e-6 | 5.345e-5 | 4.685e-5 | 3.505e-6 | 5.345e-5 |
| meta_null | 3.830e-5 | 6.396e-5 | 5.607e-5 | 3.830e-5 | 6.396e-5 |
| p6_drive | 4.255e-3 | 1.122e-4 | 1.270e-4 | 4.255e-3 | 1.122e-4 |

Notes:
- Null cases meet the tight window threshold (|epExactRateWindow_last| ≤ 2e-4) with CI containing 0.
- P6 drive shows a clearly positive epExact rate.

### Clock traversal necessity (gated repair controls)

Command:
```bash
node scripts/test-clock-traversal-necessity.mjs
```

Summary table (mean across seeds 1–5):

| preset | err(f=0.5) | Sdiff(base,meta0) | recovery mean | recovery P95 | err<=0.1 count |
| --- | --- | --- | --- | --- | --- |
| A_ungated | 0.230 | 0.494 | 100000 | 100000 | 1 |
| B_gated_clock | 0.000 | 0.000 | 100000 | 100000 | 5 |
| C_gated_static | 0.028 | 2.584 | — | — | 5 |
| D_gated_random | 0.000 | 0.000 | 100000 | 100000 | 5 |

Notes:
- Gated + drifting clock succeeds robustly.
- Static gated control no longer recovers within the test window (no finite recovery times).
- Random-walk clock matches drifting clock in this setting (orientation not required for repair here).

### Deadline traversal (stripe-gated repair under noise)

Command:
```bash
node scripts/test-clock-deadline-traversal.mjs
```

Setup notes:
- Stripe gating (`repairGateMode=1`) with `clockK=8` bins.
- Targeted perturbation in quadrant 2, with continuous symmetric noise.
- Deadline = 25k steps, report cadence = 5k.

Summary table (mean across seeds 1–5):

| preset | err(f=0.5) | Sdiff(q2) | recovery mean | miss deadline |
| --- | --- | --- | --- | --- |
| drift | 0.458 | 5.463 | 18000 | 1/5 |
| random | 0.457 | 5.550 | 36000 | 4/5 |
| static | 0.480 | 8.600 | 535000 | 5/5 |

### Joint budget sweep (clock precision vs code maintenance vs EP)

Command:
```bash
node scripts/run-clock-code-joint-sweep.mjs
```

Summary table (seeds 1–10, steps 1.5M):

| mu | meanSigma | epExactRateWindowLast | meanQ | relVar | R | err(f=0.5) median | recovery median | recovery P95 | epClock mean | epRepair mean |
| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- |
| 0.2 | 19550.56 | 0.00691 | 24918.0 | 0.000204 | 1.998 | 0.00625 | 150000 | 150000 | 9967.20 | 9583.36 |
| 0.4 | 60432.64 | 0.02242 | 41385.6 | 0.000051 | 1.533 | 0.00000 | 150000 | 150000 | 33108.48 | 27324.16 |
| 0.6 | 106437.48 | 0.04211 | 52441.9 | 0.000022 | 1.175 | 0.00000 | 150000 | 150000 | 62930.28 | 43507.20 |
| 0.8 | 153982.72 | 0.06403 | 59956.0 | 0.000013 | 1.023 | 0.00000 | 150000 | 150000 | 95929.60 | 58053.12 |
| 1.0 | 202367.80 | 0.08670 | 64900.7 | 0.000017 | 1.761 | 0.00000 | 150000 | 150000 | 129801.40 | 72566.40 |
| 1.4 | 298974.47 | 0.13157 | 70493.4 | 0.000021 | 3.095 | 0.00000 | 150000 | 150000 | 197381.52 | 101592.96 |

Diagnostics:
- meanSigma monotonic count: 5/5.
- relVar monotonic count: 3/5; Spearman corr(EP, 1/relVar) = 0.771.
- Code metric monotonic: err median drops to 0 by mu≥0.4; recovery median stays at 150k.

### Safe claims (what this section supports)

- **C_EP_EXACT_NULL_1 (SAFE)** epExact window rates in null are near 0 with CI containing 0.
  - Evidence: epExact table shows `base_null` mean 3.505e-6 ± 4.685e-5 and `meta_null` mean 3.830e-5 ± 5.607e-5.
  - Controls: P3=OFF, P6=OFF in null cases; P6 drive as positive control.
  - Scope: 5 seeds, 10M steps (null); 3 seeds, 2M steps (drive).
- **C_TRAVERSAL_NEED_1 (SAFE)** Gated repair requires traversal; static gate does not recover.
  - Evidence: traversal table shows `B_gated_clock` err=0 with recovery, while `C_gated_static` has no finite recovery.
  - Controls: gated static vs gated moving clock under same settings.
  - Scope: 5 seeds, 1M steps.
- **C_TRAVERSAL_ORIENT_1 (SAFE)** In this gated repair setting, random clock matches drifting clock.
  - Evidence: `D_gated_random` matches `B_gated_clock` (err=0, Sdiff=0).
  - Controls: clockUsesP6 toggled OFF for random walk.
  - Scope: 5 seeds, 1M steps.

### Not claimed / caveats

- Orientation is not required in this specific gated‑repair test; orientation effects are evaluated separately under deadlines.
- The traversal results are specific to the gating scheme and noise parameters listed here.

## 2025-12-22 — Deadline campaign v2: phase diagram, fidelity separation, EP efficiency

### Event-based deadline stats (repeated corruption, 10 seeds)

Command:
```bash
node scripts/run-deadline-event-stats.mjs --preset scripts/params/clock_code/code_deadline_gated_clock.json --variant all \
  --seeds 1,2,3,4,5,6,7,8,9,10 --steps 2000000 --reportEvery 1000 --eventEvery 50000 --deadline 15000 \
  --region stripe --regionIndex 12 --errGood 0.4 --sdiffGood 7.0
```

Summary table (means across seeds 1–10):

| variant | missFrac | recP95 | uptime | errEnd | epTotalRate | epClockRate | epRepairRate |
| --- | --- | --- | --- | --- | --- | --- | --- |
| drift | 0.015 | 2600 | 0.708 | 0.150 | 33.679 | 0.00077 | 33.676 |
| random | 0.126 | 3300 | 0.610 | 0.125 | 33.762 | 0.00000 | 33.759 |
| static | 0.846 | 9000 | 0.009 | 0.200 | 32.004 | 0.00000 | 32.001 |

Notes:
- Static missFrac >= 0.8 holds; drift missFrac <= 0.5 * random holds.
- Uptime separation and recoveryP95 ratio do not reach the 0.2 / 0.7 targets at 2M steps; see diagnostics section below.

### Phase diagram (drift vs random)

Command:
```bash
node scripts/run-deadline-phase-diagram.mjs
```

Result:
- 288 grid points evaluated.
- Strong-separation points (sepMiss >= 0.4 && sepUptime >= 0.2): 10.
- Full CSV: `.tmp/clock_code/deadline_phase_diagram.csv`

Top 10 configs by sepScore:

| grid | gateSpan | noise | deadline | mu | etaDrive | sepMiss | sepUptime | sepScore |
| --- | --- | --- | --- | --- | --- | --- | --- | --- |
| 32 | 2 | 0.1 | 15000 | 1.6 | 0.8 | 0.456 | 0.313 | 0.613 |
| 32 | 2 | 0.1 | 15000 | 1.6 | 1.0 | 0.456 | 0.313 | 0.613 |
| 32 | 2 | 0.1 | 20000 | 1.6 | 0.8 | 0.456 | 0.313 | 0.613 |
| 32 | 2 | 0.1 | 20000 | 1.6 | 1.0 | 0.456 | 0.313 | 0.613 |
| 32 | 2 | 0.1 | 15000 | 2.0 | 0.8 | 0.439 | 0.325 | 0.601 |
| 32 | 2 | 0.1 | 15000 | 2.0 | 1.0 | 0.439 | 0.325 | 0.601 |
| 32 | 2 | 0.1 | 15000 | 2.4 | 0.8 | 0.421 | 0.291 | 0.566 |
| 32 | 2 | 0.1 | 15000 | 2.4 | 1.0 | 0.421 | 0.291 | 0.566 |
| 32 | 2 | 0.1 | 20000 | 2.0 | 0.8 | 0.404 | 0.325 | 0.566 |
| 32 | 2 | 0.1 | 20000 | 2.0 | 1.0 | 0.404 | 0.325 | 0.566 |

### Fidelity separation search

Command:
```bash
node scripts/run-fidelity-separation-search.mjs
```

Result:
- No candidate met the strict fidelity criteria (`uptime >= 0.8`, `errEnd <= 0.05`).
- The best available candidate was written to `scripts/params/clock_code/deadline_fidelity_found.json` with a note.

Best-candidate summary (mean over seeds 1–10):

| variant | missFrac | uptime | errEnd | recP95 | epTotalRate | epClockRate | epRepairRate |
| --- | --- | --- | --- | --- | --- | --- | --- |
| drift | 0.064 | 0.664 | 0.275 | 5800 | 12.476 | 0.00391 | 12.472 |
| random | 0.203 | 0.548 | 0.225 | 6200 | 12.343 | 0.00000 | 12.343 |
| static | 0.974 | 0.006 | 0.275 | 5600 | 9.468 | 0.00000 | 9.465 |

### EP efficiency (found candidate)

| avoidedMiss | uptimeGain | deltaEP | deltaEPClock | EP/avoidedMiss | EPclock/avoidedMiss | EP/uptimeGain |
| --- | --- | --- | --- | --- | --- | --- |
| 0.138 | 0.116 | 0.133 | 0.00391 | 0.959 | 0.0282 | 1.148 |

Diagnostics:
- Strong separation in the phase diagram appears for higher noise (0.1–0.15) and mid mu (1.6–2.0).
- Strict fidelity criteria remain unmet in this round; errEnd and uptime targets appear too tight for the current gate/noise tradeoff.

## 2025-12-23 — Deadline Campaign v3 (calibrated)

### Calibration example (stripe gate)

Command:
```bash
node scripts/calibrate-gate-gaps.mjs --preset scripts/params/clock_code/deadline_fidelity_drift.json --variant all --region stripe --regionIndex 24
```

| variant | gapP95 | gapMax | deadlineRec |
| --- | --- | --- | --- |
| drift | 53000 | 64000 | 63600 |
| random | 32000 | 103000 | 38400 |

### Phase diagram v3 (tail metrics + calibrated deadline)

Command:
```bash
node scripts/run-deadline-phase-diagram.mjs
```

Summary:
- points evaluated: 624
- strong separation points: 13 (`driftMiss <= 0.2`, `uptimeTail >= 0.7`, `sepScore >= 0.5`)
- full table: `.tmp/clock_code/deadline_phase_v3.csv`

Top 10 sepScore rows:

| gridSize | gateSpan | codeNoiseRate | deadline | mu | etaDrive | driftMiss | randomMiss | driftUptimeTail | randomUptimeTail | driftErrTail | randomErrTail | sepScore |
| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- |
| 64 | 3 | 0.012 | 42000 | 2.200 | 0.400 | 0 | 0.161 | 0.925 | 0.269 | 0.019 | 0.183 | 0.735 |
| 64 | 3 | 0.012 | 42000 | 2.200 | 0.450 | 0 | 0.161 | 0.925 | 0.269 | 0.019 | 0.183 | 0.735 |
| 64 | 3 | 0.012 | 42000 | 2.200 | 0.500 | 0 | 0.161 | 0.925 | 0.269 | 0.019 | 0.183 | 0.735 |
| 64 | 3 | 0.012 | 42000 | 2.200 | 0.550 | 0 | 0.161 | 0.925 | 0.269 | 0.019 | 0.183 | 0.735 |
| 64 | 3 | 0.012 | 42000 | 2.200 | 0.600 | 0 | 0.161 | 0.925 | 0.269 | 0.019 | 0.183 | 0.735 |
| 64 | 3 | 0.012 | 42000 | 2.200 | 0.650 | 0 | 0.161 | 0.925 | 0.269 | 0.019 | 0.183 | 0.735 |
| 64 | 3 | 0.012 | 42000 | 2.200 | 0.700 | 0 | 0.161 | 0.925 | 0.269 | 0.019 | 0.183 | 0.735 |
| 64 | 3 | 0.012 | 42000 | 2.200 | 0.750 | 0 | 0.161 | 0.925 | 0.269 | 0.019 | 0.183 | 0.735 |
| 64 | 3 | 0.012 | 42000 | 2.200 | 0.800 | 0 | 0.161 | 0.925 | 0.269 | 0.019 | 0.183 | 0.735 |
| 64 | 3 | 0.012 | 42000 | 2.200 | 0.850 | 0 | 0.161 | 0.925 | 0.269 | 0.019 | 0.183 | 0.735 |

### Fidelity separation search v3

Command:
```bash
node scripts/run-fidelity-separation-search.mjs
```

Result:
- No candidate met the strict fidelity criteria (`driftMiss <= 0.2`, `uptimeTail >= 0.8`, `errTailMean <= 0.05`).
- Best candidate saved to `scripts/params/clock_code/deadline_fidelity_found.json`.

Best-candidate summary (mean over seeds 1–10):

| variant | missFrac | uptimeTail | errTailMean | recP95 | epTotalRate | epClockRate | epRepairRate |
| --- | --- | --- | --- | --- | --- | --- | --- |
| drift | 0.090 | 0.516 | 0.140 | 6500 | 0.146 | 0.0832 | 0.0628 |
| random | 0.136 | 0.378 | 0.169 | 7600 | 0.0618 | 0.0000 | 0.0618 |
| static | 0.485 | 0.213 | 0.261 | 7300 | 0.0598 | 0.0000 | 0.00615 |

### EP efficiency (found candidate)

| avoidedMiss | uptimeTailGain | deltaEP | deltaEPClock | EP/avoidedMiss | EPclock/avoidedMiss | EP/uptimeTailGain |
| --- | --- | --- | --- | --- | --- | --- |
| 0.046 | 0.138 | 0.0843 | 0.0832 | 1.826 | 1.803 | 0.604 |

Diagnostics:
- Strong separation still comes from a narrow band (gridSize 64, gateSpan 3, codeNoiseRate 0.012, mu 2.2).
- Strict fidelity criteria remain unmet in the 10-seed search; drift uptime and errTail remain below targets.

### Success probability + bootstrap CIs (found config)

Command:
```bash
node scripts/run-deadline-success-ci.mjs --found scripts/params/clock_code/deadline_fidelity_found.json --seedCount 30
```

Per-variant summary (30 seeds):

| variant | missFracMean | uptimeTailMean | errTailMean | epClockRateMean | successRate |
| --- | --- | --- | --- | --- | --- |
| drift | 0.126 | 0.576 | 0.122 | 0.0834 | 0.267 |
| random | 0.168 | 0.292 | 0.205 | 0.0000 | 0.067 |
| static | 0.516 | 0.195 | 0.270 | 0.0000 | 0.000 |

Bootstrap CIs (drift vs random):

| metric | mean | CI_low | CI_high |
| --- | --- | --- | --- |
| missFrac_drift_minus_random | -0.041 | -0.096 | 0.014 |
| uptimeTail_drift_minus_random | 0.285 | 0.138 | 0.429 |
| errTail_random_minus_drift | 0.084 | 0.034 | 0.130 |
| deltaEPClockRate | 0.0834 | 0.0832 | 0.0835 |

### Safe claims (what this section supports)

- **C_DEADLINE_DRIFT_1 (SAFE)** Drift improves uptimeTail relative to random in the found config.
  - Evidence: bootstrap CI for `uptimeTail_drift_minus_random` is [0.138, 0.429] (30 seeds).
  - Controls: drift vs random vs static variants under identical deadline/noise settings.
  - Scope: found config, 30 seeds (CI table).
- **C_DEADLINE_CLOCK_EP_1 (SAFE)** Drift adds a positive clock EP cost.
  - Evidence: `deltaEPClockRate` CI [0.0832, 0.0835] > 0 in bootstrap table.
  - Controls: drift vs random in the same config.
  - Scope: 30 seeds.

### Not claimed / caveats

- missFrac drift–random CI crosses 0 (`-0.096` to `0.014`), so missFrac separation is not SAFE here.
- Strict fidelity criteria were not met in v2/v3 searches; claims are limited to observed uptimeTail/errTail advantages.

## 2025-12-23 — Operator-lifted coupling (K tokens)

Commands:
```bash
node scripts/test-opcoupling-invariants.mjs
node scripts/test-opcoupling-null-ep.mjs
node scripts/test-opcoupling-effect.mjs
node scripts/ratchet-cli.mjs run --steps 1000000 --report-every 500000 --params scripts/params/meta/meta2_null_coupled.json --set sCouplingMode=0 --set opCouplingOn=0
```

### K invariants
- Budgeted K tokens stayed in [0, B_K] with per-cell sums exactly B_K (200 random samples).

### Null EP (op coupling on)

| id | meanExact | stdExact | ciHalfWidth | meanNaive | stdNaive | kChanged |
| --- | --- | --- | --- | --- | --- | --- |
| op_null_eta | 0.000006149 | 0.000078933 | 0.000097993 | 0.000006149 | 0.000078933 | 5 |
| op_null_eta0 | -0.000025401 | 0.000022535 | 0.000027976 | -0.000025401 | 0.000022535 | 5 |

### Coupling effect (eta vs 0)
- Sdiff_op eta=0: 0.308521
- Sdiff_op eta=0.6: 0.231604
- ratio: 0.7507

### Legacy coupling sanity
- `meta2_null_coupled.json` with `sCouplingMode=0` and `opCouplingOn=0` remains null-like: Σmem ~0.0008 at 1M steps; P6 motifs remain 0.

### Safe claims (what this section supports)

- **C_OPK_INV_1 (SAFE)** K token budget invariants hold (per-cell sums fixed).
  - Evidence: “K invariants” check reports sums exactly B_K (200 random samples).
  - Controls: invariant check under op coupling enabled.
  - Scope: 200 sampled cells.
- **C_OPK_NULL_EP_1 (SAFE)** op coupling preserves null EP near 0.
  - Evidence: `op_null_eta` and `op_null_eta0` tables show meanExact near 0 with small CI.
  - Controls: P3=OFF, P6=OFF; eta=0.6 and eta=0.
  - Scope: 5 seeds.
- **C_OPK_EFFECT_1 (SAFE)** Conservative coupling reduces operator mismatch.
  - Evidence: Sdiff_op ratio 0.7507 (`eta=0.6` vs `eta=0`).
  - Controls: eta toggled in the same setup.
  - Scope: 2M steps (effect test).

### Not claimed / caveats

- No claim that K induces hierarchy or improves deadline performance in this section.

## 2025-12-23 — Operator-lifted coupling: diagnostics, hierarchy, selection, deadline

Commands:
```bash
node scripts/run-opk-diagnostics.mjs
node scripts/run-opk-hierarchy-search.mjs
node scripts/test-opk-drive-selection.mjs
node scripts/run-deadline-opk-compare.mjs
```

### opK diagnostics (final rows)

| variant | step | m0Mean | HMean | R2Mean | AMean | cohMean | SdiffMean | epExactRateWindow | epOpKRateWindow |
| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- |
| eta0 | 1200000 | 0.105835 | 1.566297 | 1.341187 | 0.139069 | 1.149292 | 0.295471 | 0 | 0 |
| eta06 | 1200000 | 0.117798 | 1.559471 | 1.328979 | 0.136688 | 1.166748 | 0.276660 | -0.00000834 | 0.00000127 |

### opK hierarchy search (best configs)

| regime | metaLayers | opStencil | opBudgetK | rhoR2Mean | rhoHMean | rhoCohMean | note |
| --- | --- | --- | --- | --- | --- | --- | --- |
| null | 4 | 0 | 32 | -0.7333 | 0.2000 | -0.3333 | BEST_R2 |
| null | 2 | 0 | 16 | -0.3333 | 1.0000 | -1.0000 | BEST_H |
| null | 2 | 0 | 16 | -0.3333 | 1.0000 | -1.0000 | BEST_COH |
| drive | 2 | 1 | 8 | 1.0000 | -0.3333 | 0.3333 | BEST_R2 |
| drive | 2 | 0 | 8 | -0.3333 | -1.0000 | 0.3333 | BEST_H |

### Drive-selection ablation (summary)

| id | sdiffStartMean | sdiffEndMean | deltaSdiffMean | epExactWindowMean | epOpKWindowMean |
| --- | --- | --- | --- | --- | --- |
| A_drive_selects | 0.250391 | 0.194502 | -0.055889 | 0.227177 | 0.033140 |
| B_drive_no_k | 0.250391 | 0.229373 | -0.021018 | 0.293289 | 0 |
| C_null | 0.250391 | 0.250684 | 0.000293 | -0.0000105 | 0 |
| D_equilibrium | 0.250391 | 0.228823 | -0.021567 | 0.0000274 | -0.00000267 |

### Deadline compare (legacy vs best op)

| id | opStencil | opBudgetK | missFracMean | uptimeTailMean | errTailMean | epTotalRateMean | epOpKRateMean |
| --- | --- | --- | --- | --- | --- | --- | --- |
| legacy |  |  | 0.0718 | 0.4515 | 0.1351 | 0.1363 | 0 |
| op_s0_b32 | 0 | 32 | 0.1154 | 0.4758 | 0.1311 | 0.1007 | -0.0000388 |

Notes:
- Hierarchy signals appear in both null and drive regimes with strong Spearman slopes.
- Drive-selection ablation passes (A beats B/C on Sdiff, epOpK > 0).
- Deadline comparison: NO_IMPROVEMENT_OVER_LEGACY; best op config saved at `scripts/params/op_coupling/deadline_opk_best.json`.

### Safe claims (what this section supports)

- **C_OPK_DRIVE_SELECT_1 (SAFE)** Drive-selection ablation shows lower mismatch with epOpK>0 only in the drive‑selecting case.
  - Evidence: ablation table shows `A_drive_selects` Sdiff end 0.1945 vs `B_drive_no_k` 0.2294, with `epOpKWindowMean 0.033140` only in A.
  - Controls: A vs B/C under identical params; P6 OFF in null control.
  - Scope: 2M steps, multiple seeds (see summary table).
- **C_OPK_HIER_1 (SUGGESTIVE)** Interface-level hierarchy metrics show slopes in best configs.
  - Evidence: hierarchy table lists BEST_R2/H/COH with nonzero rho values.
  - Controls: none beyond sweep; earlier degeneracy for metaLayers=2.
  - Scope: 3 seeds per config; no CI.

### Not claimed / caveats

- Hierarchy evidence is not definitive; per‑interface rho can be unstable and depends on metaLayers.
- Deadline comparison here is confounded by scheduler dilution; see v2/v3 decomposition.

## 2025-12-24 — Operator coupling v2: hierarchy fix + deadline decomposition

Commands:
```bash
node scripts/run-opk-hierarchy-search.mjs
node scripts/run-deadline-opk-decomposition.mjs
```

### opK hierarchy search (metaLayers>=3 only)

| regime | metaLayers | opStencil | opBudgetK | rhoR2Mean | rhoHMean | rhoCohMean | deltaR2Mean | deltaHMean | deltaCohMean | note |
| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- |
| null | 4 | 0 | 32 | -0.7333333333333334 | 0.20000000000000004 | -0.3333333333333333 | -0.0111083984375 | 0.0035224161477747837 | -0.007242838541666667 | BEST_R2 |
| null | 3 | 1 | 16 | 0.16666666666666666 | 0.5 | -0.16666666666666666 | 0.008056640625 | 0.00509017606416647 | 0.0035807291666666665 | BEST_H |
| null | 3 | 1 | 32 | 0.16666666666666666 | -0.16666666666666666 | 0.5 | 0.0211181640625 | -0.00010461573471109986 | 0.011881510416666666 | BEST_COH |
| drive | 3 | 1 | 8 | 0.5 | -0.3333333333333333 | 0 | 0.015299479166666666 | -0.013952026579811427 | 0.009114583333333334 | BEST_R2 |
| drive | 3 | 0 | 8 | -0.16666666666666666 | -0.8333333333333334 | 0.5 | 0.00732421875 | -0.03930164940094494 | 0.0478515625 | BEST_H |
| drive | 3 | 0 | 16 | 0.16666666666666666 | -0.8333333333333334 | 0.6666666666666666 | 0.00146484375 | -0.018355843094732 | 0.029296875 | BEST_COH |

### Deadline decomposition (A/B vs best C/D)

| id | opStencil | opBudgetK | missFracMean | uptimeTailMean | errTailMean | epTotalRateMean | epClockRateMean | epRepairRateMean | epOpKRateMean | p5MetaToRecoverMean | repairRateMean |
| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- |
| A_legacy |  |  | 0.07777777777777778 | 0.45757575757575764 | 0.1322390572390572 | 0.1253969239883423 | 0.08373599778175353 | 0.041660926206588754 | 0 | 45.48174603174603 | 0.006371 |
| B_dilution_only |  |  | 0.11111111111111112 | 0.44848484848484854 | 0.13787878787878785 | 0.10954079666843415 | 0.08358623778572083 | 0.025954558882713318 | 0 | 16.864484126984127 | 0.003816 |
| BEST_C (C_op_noKdrive_s1_b32) | 1 | 32 | 0.07777777777777779 | 0.5181818181818183 | 0.12045454545454545 | 0.0927407587062845 | 0.08348903778829575 | 0.009251720917988756 | 0 | 11.391666666666666 | 0.002207 |
| BEST_D (D_op_withKdrive_s1_b16) | 1 | 16 | 0.1 | 0.49393939393939396 | 0.1265151515151515 | 0.09312033762556948 | 0.0833061577931404 | 0.009903779894685744 | -0.00008960006225667895 | 14.582738095238096 | 0.0023806000000000005 |

### Dilution attribution
- `DILUTION_ATTRIBUTION: miss(A)->miss(B)=0.0333 miss(A)->miss(bestD)=0.0222 ratio=1.50 explained=true`

Notes:
- metaLayers=2 excluded from rho ranking; effect-size signals computed with metaLayers>=3.
- Deadline regression is largely explained by P5 dilution (A→B); best D does not recover that loss.

### Safe claims (what this section supports)

- **C_OPK_DILUTION_1 (SAFE)** Deadline regression is largely explained by scheduler dilution.
  - Evidence: `DILUTION_ATTRIBUTION` line shows ratio 1.50 (A→B > A→bestD).
  - Controls: A/B/C/D decomposition with identical deadline setup.
  - Scope: 10 seeds, 500k steps.

### Not claimed / caveats

- No claim that op coupling improves deadline performance in this table.
- Hierarchy metrics here remain exploratory without CI or consistent sign across seeds.

## 2025-12-24 — Operator coupling v3: throughput matching, frontier, composed hierarchy

Commands:
```bash
node scripts/run-deadline-opk-throughput-matched.mjs
node scripts/run-deadline-opk-frontier.mjs
node scripts/run-opk-composed-hierarchy.mjs
```

### Throughput-matched summary (A/B/C/D)

| id | pSWriteMean | repairRateMean | missFracMean | uptimeTailMean | errTailMean | epTotalRateMean | epRepairRateMean | p5MetaToRecoverSuccessMean | repairEfficiencySuccessMean | calibrationOkCount |
| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- |
| A_legacy | 1 | 0.006371 | 0.07777777777777778 | 0.45757575757575764 | 0.1322390572390572 | 0.1253969239883423 | 0.041660926206588754 | 45.48174603174603 | 145.72854755737256 | 10 |
| B_dilution_only | 0.9704687500000002 | 0.0038573999999999995 | 0.11111111111111112 | 0.5515151515151515 | 0.11212121212121211 | 0.11135111925361336 | 0.025843966887474056 | 20.362103174603174 | 221.45785013988356 | 0 |
| C_candidate (opStencil=1, opBudgetK=32) | 0.9697656250000002 | 0.002217 | 0.1777777777777778 | 0.5333333333333334 | 0.11666666666666665 | 0.09447671411796049 | 0.008904696667763962 | 11.50690476190476 | 442.0686424955735 | 0 |
| D_candidate (opStencil=1, opBudgetK=16) | 0.9644921875 | 0.002334 | 0.18888888888888888 | 0.3939393939393939 | 0.15151515151515152 | 0.09569308464210331 | 0.010027331902828066 | 11.528769841269842 | 341.2969894369107 | 0 |

Notes:
- `RATE_MATCH_OK=false` because op modes cannot reach the A_legacy repair rate within the pSWrite [0.1,1.0] bound (pSWrite saturates near 1.0).

### EP vs miss iso-threshold table (from frontier sweep)

| mode | tau | epTotalRate | pSWrite | etaDrive |
| --- | --- | --- | --- | --- |
| A_legacy | 0.05 | NA | NA | NA |
| A_legacy | 0.08 | 0.117624 | 0.65 | 0.4 |
| A_legacy | 0.10 | 0.117624 | 0.65 | 0.4 |
| A_legacy | 0.12 | 0.117624 | 0.65 | 0.4 |
| B_dilution_only | 0.05 | 0.093884 | 0.2 | 0.4 |
| B_dilution_only | 0.08 | 0.093884 | 0.2 | 0.4 |
| B_dilution_only | 0.10 | 0.093884 | 0.2 | 0.4 |
| B_dilution_only | 0.12 | 0.093884 | 0.2 | 0.4 |
| C_op_noKdrive | 0.05 | 0.090401 | 0.2 | 0.4 |
| C_op_noKdrive | 0.08 | 0.090401 | 0.2 | 0.4 |
| C_op_noKdrive | 0.10 | 0.090401 | 0.2 | 0.4 |
| C_op_noKdrive | 0.12 | 0.090401 | 0.2 | 0.4 |
| D_op_withKdrive | 0.05 | 0.094338 | 0.2 | 1 |
| D_op_withKdrive | 0.08 | 0.092877 | 0.2 | 0.8 |
| D_op_withKdrive | 0.10 | 0.091453 | 0.2 | 0.6 |
| D_op_withKdrive | 0.12 | 0.091453 | 0.2 | 0.6 |

### Composed-operator hierarchy summary

| config | metaLayers | opStencil | opBudgetK | rhoR2Mean | rhoTVMean | r2Consistent | tvConsistent | signal |
| --- | --- | --- | --- | --- | --- | --- | --- | --- |
| null | 4 | 0 | 8 | 1 | -0.20461032471872267 | 10 | 2 | COMPOSED_HIERARCHY_SIGNAL_FOUND |
| drive | 3 | 0 | 8 | 1 | -0.5632455532033676 | 10 | 3 | COMPOSED_HIERARCHY_SIGNAL_FOUND |

### Safe claims (what this section supports)

- **C_OPK_COMPOSED_R2_1 (SAFE)** Composed operator reach (R2_eff) grows with depth in measured configs.
  - Evidence: composed hierarchy table shows `rhoR2Mean = 1` with `r2Consistent = 10` in both null and drive configs.
  - Controls: same op coupling setup across seeds; depth index as ordered variable.
  - Scope: 10 seeds, 1M steps, metaLayers 3–4, opStencil=0, opBudgetK=8.

### Not claimed / caveats

- TV vs layer index is not consistently monotone (rhoTV is negative and less consistent).
- Throughput matching failed in this v3 run (RATE_MATCH_OK=false), so wall‑clock comparisons remain confounded.

## 2025-12-25 — Operator coupling v4: resource allocation, CI, repair-budget curves

Commands:
```bash
node scripts/test-opk-null-ep-weights.mjs
node scripts/run-deadline-opk-throughput-matched-v4.mjs
node scripts/run-deadline-opk-ci-iso.mjs
node scripts/run-deadline-opk-repair-budget-curves.mjs
```

### Null EP across opKTargetWeight

| weight | seed | epExactRateWindowLast | pass |
| --- | --- | --- | --- |
| 0 | 1 | -0.000018540889471769332 | true |
| 0 | 2 | 0.00003167414490878582 | true |
| 0 | 3 | 0.000041335817232728006 | true |
| 0.25 | 1 | -0.000013321051119581824 | true |
| 0.25 | 2 | -0.00004786590932894461 | true |
| 0.25 | 3 | -0.000046520789032945204 | true |
| 0.5 | 1 | 0.00000527027480234699 | true |
| 0.5 | 2 | 0.000019545628705096136 | true |
| 0.5 | 3 | -0.00001030601475653407 | true |
| 1 | 1 | -0.000009790303060319786 | true |
| 1 | 2 | 0.000027828242737074986 | true |
| 1 | 3 | 0.000014448046209230113 | true |
| 2 | 1 | 0.00003733586821995507 | true |
| 2 | 2 | 0.000025038831470936288 | true |
| 2 | 3 | -0.000034798192052560834 | true |

### Throughput-matched v4 summary (A/B/C/D)

| id | opKTargetWeightMean | repairRateMean | missFracMean | epTotalRateMean | p5MetaToRecoverSuccessMean | calibrationOkCount |
| --- | --- | --- | --- | --- | --- | --- |
| A_legacy | 1 | 0.006371 | 0.07777777777777778 | 0.1253969239883423 | 45.48174603174603 | 10 |
| B_dilution_only | 0.103125 | 0.0060062 | 0.06666666666666667 | 0.12252138809556963 | 40.2125 | 10 |
| C_op_noKdrive | 0.0060546875 | 0.0039014 | 0.04444444444444444 | 0.09945762057679743 | 26.01805555555556 | 0 |
| D_op_withKdrive | 0.02470703125 | 0.0038655999999999994 | 0.04444444444444444 | 0.09944462853691052 | 31.40892857142857 | 0 |

Notes:
- C/D cannot reach the baseline repair rate even with opKTargetWeight→0 and pSWrite=1.0; calibrationOkCount remains 0 for those modes.

### ISO-miss CI replication (tau=0.08 points)

| mode | meanEp | epCiLow | epCiHigh | meanMiss | missCiLow | missCiHigh |
| --- | --- | --- | --- | --- | --- | --- |
| B_dilution_only | 0.09352591330352379 | 0.09329667223442643 | 0.09376987946156241 | 0.0488888888888889 | 0.02666666666666667 | 0.07333333333333336 |
| C_op_noKdrive | 0.09018334786495269 | 0.08996306901039725 | 0.09039783648143677 | 0.04222222222222223 | 0.02 | 0.0688888888888889 |
| DELTA_EP_C_MINUS_B | -0.0033425654385711018 | -0.003668463333649355 | -0.003020114137446586 |  |  |  |

### Repair-budget curve excerpt (with bootstrap CI)

| mode | budget | P_succ | CI_low | CI_high | events |
| --- | --- | --- | --- | --- | --- |
| A_legacy | 10 | 0.5 | 0.3888888888888889 | 0.6 | 90 |
| A_legacy | 20 | 0.5 | 0.4 | 0.6 | 90 |
| A_legacy | 40 | 0.6222222222222222 | 0.5222222222222223 | 0.7222222222222222 | 90 |
| A_legacy | 80 | 0.7 | 0.6111111111111112 | 0.7888888888888889 | 90 |
| B_dilution_only | 10 | 0.5333333333333333 | 0.43333333333333335 | 0.6333333333333333 | 90 |
| B_dilution_only | 20 | 0.5444444444444444 | 0.4444444444444444 | 0.6555555555555556 | 90 |
| B_dilution_only | 40 | 0.6111111111111112 | 0.5111111111111111 | 0.7111111111111111 | 90 |
| B_dilution_only | 80 | 0.7666666666666667 | 0.6777777777777778 | 0.8555555555555555 | 90 |
| C_op_noKdrive | 10 | 0.6222222222222222 | 0.5222222222222223 | 0.7222222222222222 | 90 |
| C_op_noKdrive | 20 | 0.6666666666666666 | 0.5666666666666667 | 0.7666666666666667 | 90 |
| C_op_noKdrive | 40 | 0.7555555555555555 | 0.6666666666666666 | 0.8444444444444444 | 90 |
| C_op_noKdrive | 80 | 0.8555555555555555 | 0.7777777777777778 | 0.9222222222222223 | 90 |
| D_op_withKdrive | 10 | 0.4888888888888889 | 0.3888888888888889 | 0.5888888888888889 | 90 |
| D_op_withKdrive | 20 | 0.5888888888888889 | 0.4888888888888889 | 0.6888888888888889 | 90 |
| D_op_withKdrive | 40 | 0.7111111111111111 | 0.6111111111111112 | 0.8 | 90 |
| D_op_withKdrive | 80 | 0.8222222222222222 | 0.7444444444444445 | 0.9 | 90 |

### Safe claims (what this section supports)

- **C_OPK_WEIGHT_NULL_EP_1 (SAFE)** opKTargetWeight sweep preserves null EP≈0.
  - Evidence: all weights in null EP table pass |epExactRateWindowLast| ≤ 2e-4 across 3 seeds.
  - Controls: P3=OFF, P6=OFF with op coupling ON.
  - Scope: 3 seeds per weight, 3M steps.
- **C_OPK_CI_EP_1 (SAFE)** Operator coupling reduces EP at the iso‑miss point relative to dilution‑only.
  - Evidence: `DELTA_EP_C_MINUS_B` CI [-0.003668, -0.003020] excludes 0.
  - Controls: C vs B under the same iso‑miss settings (tau=0.08).
  - Scope: 50 seeds, 0.5M steps.
- **C_OPK_REPAIR_DOM_1 (SAFE)** Per‑repair success curves show C dominates A/B at observed budgets.
  - Evidence: P_succ_C(N) ≥ P_succ_B(N) and ≥ P_succ_A(N) for N∈{10,20,40,80}, with bootstrap CIs listed.
  - Controls: identical deadline schedule; same corruption and gating.
  - Scope: 10 seeds, 90 events per mode.

### Not claimed / caveats

- Throughput matching failed for C/D even with opKTargetWeight→0; comparisons remain acceptance‑limited.
- Dominance is shown for the listed budgets only; no claim of asymptotic or universal dominance.

## 2025-12-25 — Motif language under selection pressure (deadline/noise)

Commands:

```bash
node scripts/run-deadline-opk-motif-compare.mjs
```

Tuned deadline:

- `deadline=4517` (auto-tune missFracMean≈0.0427; band target 0.05–0.70 not reached within 6 iterations)

### cond_summary.csv (A/B/C)

| condition | missFracMean | uptimeTailMean | errTailMean | epTotalRateMean | epRepairRateMean | epOpKRateMean | hazardM1_HMean | hazardM2_HMean | asymmetryM1Mean | asymmetryM2Mean |
| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- |
| A_legacy | 0.2564102564102564 | 0.7135135135135136 | 0.08926426426426425 | 0.14239119133827685 | 0.05373273368692398 | 0 | 0 | 0 | 0 | 0 |
| B_op_noKdrive | 0.4692307692307692 | 0.4999999999999999 | 0.13873873873873874 | 0.10808391344751156 | 0.020136635777318855 | 0 | 1.5728661382379656 | 1.706803126362442 | 0.3797117223244055 | 0.07747239861013419 |
| C_op_withKdrive | 0.3384615384615385 | 0.6945945945945946 | 0.07342342342342342 | 0.10766184017458084 | 0.019857008032436296 | -0.000014285531443252694 | 1.5733628624802445 | 1.7069067708413725 | 0.3718267981771392 | 0.07771470677774085 |

### success_conditioned.csv (A/B/C)

| condition | family | H_succ | H_fail | H_delta | A_succ | A_fail | A_delta | epPerChange_succ | epPerChange_fail | epPerChange_delta |
| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- |
| A_legacy | M1 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 |
| A_legacy | M2 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 |
| B_op_noKdrive | M1 | 1.573614296924284 | 1.5762589728672722 | -0.0026446759429883393 | 0.41139759162148914 | 0.3438703291850816 | 0.06752726243640755 | 0 | 0 | 0 |
| B_op_noKdrive | M2 | 1.7036454600298196 | 1.7024756230732045 | 0.0011698369566151001 | 0.08405063726001939 | 0.07003144013731316 | 0.014019197122706231 | 0 | 0 | 0 |
| C_op_withKdrive | M1 | 1.5769888292467444 | 1.5716792280741265 | 0.005309601172617917 | 0.3926526848579972 | 0.3311216560281895 | 0.06153102882980771 | 0 | 0.01161602800282923 | -0.01161602800282923 |
| C_op_withKdrive | M2 | 1.6992019316735938 | 1.7075482360215177 | -0.008346304347923938 | 0.07590912441126242 | 0.08124379958494857 | -0.005334675173686146 | 0 | 0.011906972121512835 | -0.011906972121512835 |

Notes:
- opK motif metrics are trivial in A (opCouplingOff), as expected; B/C show comparable M1/M2 entropies and asymmetry magnitudes in this run.
- epOpKRate remains ~0 for B and slightly negative (near zero) for C in this run; per‑event epPerChange differs by condition but remains small.

## 2025-12-26 — Motif language v2 under selection pressure (deadline/noise)

Commands:

```bash
node scripts/run-deadline-opk-motif-compare.mjs --basis v2
node scripts/run-deadline-opk-motif-p2-ablation.mjs
```

Tuned deadline:

- `deadline=38400` (auto-tune missFracMean≈0.2479; band target 0.05–0.70)

### cond_summary.csv (A/B/C)

| condition | missFracMean | uptimeTailMean | errTailMean | epTotalRateMean | epRepairRateMean | epOpKRateMean | hazardBase_HMean | hazardOp_HMean | coarseEPBaseMean | coarseEPOpMean |
| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- |
| A_legacy | 0.2512820512820513 | 0.23636363636363633 | 0.24469696969696972 | 0.133769119624114 | 0.05026406183624267 | 0 | 2.6718933053442475 | 0.9588717510254655 | 212.84609084396348 | 1.9202902045387948 |
| B_op_noKdrive | 0.3153846153846154 | 0.23636363636363633 | 0.2507575757575758 | 0.10283671106562635 | 0.019077493284487906 | 0 | 2.6459860280579064 | 0.9798245172759733 | 112.6522464624773 | 3.58681227565486 |
| C_op_withKdrive | 0.35128205128205126 | 0.19696969696969693 | 0.25833333333333336 | 0.10236620345520286 | 0.0187989005443288 | -0.000010114875080401543 | 2.647144441053593 | 0.9783218673723935 | 111.81543092803777 | 3.753527255092137 |

### coarseep_summary.csv (A/B/C)

| condition | region | family | coarseEP |
| --- | --- | --- | --- |
| A_legacy | hazard | M_base | 1493.7243555094108 |
| A_legacy | outside | M_base | 1935.3599331114342 |
| A_legacy | hazard | M_op | 13.048042580293265 |
| A_legacy | outside | M_op | 14.990067147217134 |
| B_op_noKdrive | hazard | M_base | 596.3934037807493 |
| B_op_noKdrive | outside | M_base | 691.6155643935975 |
| B_op_noKdrive | hazard | M_op | 23.59066597241859 |
| B_op_noKdrive | outside | M_op | 28.690050820080515 |
| C_op_withKdrive | hazard | M_base | 602.6360996817923 |
| C_op_withKdrive | outside | M_base | 655.2426252436386 |
| C_op_withKdrive | hazard | M_op | 24.784815335979463 |
| C_op_withKdrive | outside | M_op | 26.121550600744968 |

### event_conditioned_summary.csv (excerpt)

| condition | family | window | H_succ | H_fail | H_delta | js_divergence | asym_succ | asym_fail | coarseEP_succ | coarseEP_fail |
| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- |
| A_legacy | M_base | pre | 2.7363660140685457 | 2.711966452889924 | 0.02439956117862163 | 0.0009324873285650211 | 0.12379009431002905 | 0.12636924529499993 | 26.92772134232639 | 28.44436686763767 |
| A_legacy | M_op | pre | 0.9738218245637698 | 0.9586706646274381 | 0.015151159936331648 | 0.00014846312680755723 | 0.03873198227447392 | 0.04593317081534838 | 0.8826413303965502 | 1.2396501729409946 |
| B_op_noKdrive | M_base | pre | 2.7058216062274676 | 2.7149037081497127 | -0.009082101922245123 | 0.0002518705016518187 | 0.16122485118935712 | 0.15074801256541123 | 25.444702874414734 | 22.23597211028751 |
| B_op_noKdrive | M_op | pre | 0.9924126238821702 | 0.9966383287740093 | -0.004225704891839044 | 0.000027697837655358572 | 0.06376588385943271 | 0.04956025133638756 | 1.5383660525361627 | 0.7825844275419459 |
| C_op_withKdrive | M_base | pre | 2.7055862894856353 | 2.710048893051421 | -0.004462603565785539 | 0.00009110339108129765 | 0.15746420065479236 | 0.15406285491720603 | 24.141324099103226 | 23.266529734693687 |
| C_op_withKdrive | M_op | pre | 0.9913152930007665 | 0.9932122956295517 | -0.001897002628785227 | 0.000005053103968589672 | 0.06495564530204502 | 0.056963442753442145 | 1.5546747419286429 | 1.144745673135859 |

### p2_ablation_summary.csv (A/B/C)

| p2Mode | condition | missFracMean | uptimeTailMean | errTailMean | hazardBaseVeffMean | hazardOpVeffMean |
| --- | --- | --- | --- | --- | --- | --- |
| p2_off | A_legacy | 0.2 | 0.20909090909090913 | 0.25606060606060604 | 14.182188046411914 | 2.529369631825168 |
| p2_off | B_op_noKdrive | 0.32564102564102565 | 0.2666666666666666 | 0.2537878787878788 | 13.790842314206083 | 2.606372326957197 |
| p2_off | C_op_withKdrive | 0.30512820512820515 | 0.17575757575757572 | 0.28106060606060607 | 13.756764048768591 | 2.60213983196099 |
| p2_on | A_legacy | 0.24358974358974356 | 0.26363636363636356 | 0.23863636363636362 | 14.696252242634994 | 2.617263601099109 |
| p2_on | B_op_noKdrive | 0.34615384615384615 | 0.20606060606060606 | 0.26969696969696966 | 14.326807279521919 | 2.674045264452011 |
| p2_on | C_op_withKdrive | 0.3333333333333333 | 0.23636363636363633 | 0.26363636363636367 | 14.421980324737135 | 2.676640793172725 |

Notes:
- M_base (S‑only) avoids A_legacy degeneracy; hazard vs outside statistics now differ meaningfully.
- coarseEP values are finite and nonzero for both M_base and M_op under selection pressure.

## 2025-12-27 — Motif language under deadline selection v3 (normalized + event-conditioned)

Commands:

```bash
node scripts/run-deadline-opk-motif-compare.mjs --basis v2 --opBinsMode 1
node scripts/run-deadline-opk-motif-compare.mjs --basis v2 --opBinsMode 2
node scripts/run-deadline-opk-motif-p2-ablation.mjs
```

Tuned deadline:

- `deadline=38400` (auto-tune missFracMean≈0.2991; opBinsMode=2 run)

### compare_summary.csv (A/B/C, opBinsMode=2)

| condition | missFracMean | uptimeTailMean | errTailMean | epTotalRateMean | epRepairRateMean | epOpKRateMean | hazardOp_uniqueStatesMean | hazardOp_changeFracMean | hazardOp_coarseEP_perTrans | hazardOp_asym_perTrans |
| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- |
| A_legacy | 0.24871794871794872 | 0.2333333333333333 | 0.2537878787878788 | 0.13349148763377666 | 0.050124669842243195 | 0 | 3 | 0.006052571614583302 | 0.00008048113036620732 | 0.008943985801489768 |
| B_op_noKdrive | 0.3153846153846154 | 0.21818181818181817 | 0.2681818181818182 | 0.10238752782550212 | 0.018913970036796297 | 0 | 3 | 0.003926529947916729 | 0.0002447381395625417 | 0.01559404093746632 |
| C_op_withKdrive | 0.34615384615384615 | 0.23333333333333334 | 0.2643939393939394 | 0.1020039492419896 | 0.018543692047318257 | -0.000009160594144137576 | 3 | 0.003918164062500067 | 0.0002713289259595793 | 0.016167356230164666 |

### event_conditioned_summary.csv (hazard recovery window, opBinsMode=2)

| condition | family | window | js_divergence | coarseEP_perTrans_succ | coarseEP_perTrans_fail | epTotalPerChange_succ | epRepairPerChange_succ | epOpKPerChange_succ | recoverySampleCountMean |
| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- |
| A_legacy | M_base | recovery | 0 | 0 | 0 | 0 | 0 |  | 0 |
| A_legacy | M_op | recovery | 0 | 0 | 0 |  |  | 0 | 0 |
| B_op_noKdrive | M_base | recovery | 0 | 0 | 0 | 0 | 0 |  | 0 |
| B_op_noKdrive | M_op | recovery | 0 | 0 | 0 |  |  | 0 | 0 |
| C_op_withKdrive | M_base | recovery | 0 | 0 | 0 | 0 | 0 |  | 0 |
| C_op_withKdrive | M_op | recovery | 0 | 0 | 0 |  |  | 0 | 0 |

### p2_ablation_summary.csv (A/B/C; opBinsMode=2)

| p2Mode | condition | missFracMean | uptimeTailMean | errTailMean | hazardOpUniqueMean | hazardOpVeffMean | hazardOp_coarseEP_perTrans | hazardOp_js_divergence |
| --- | --- | --- | --- | --- | --- | --- | --- | --- |
| p2_off | A_legacy | 0.2282051282051282 | 0.2212121212121212 | 0.25 | 3 | 2.5279409911856576 | 0.00004960333179867984 | 0 |
| p2_off | B_op_noKdrive | 0.3205128205128205 | 0.22727272727272724 | 0.2681818181818182 | 3 | 2.6041135170514145 | 0.0001697541512058769 | 0 |
| p2_off | C_op_withKdrive | 0.2871794871794872 | 0.3 | 0.22348484848484848 | 3 | 2.6016170729013766 | 0.00016933314031578013 | 0 |
| p2_on | A_legacy | 0.2769230769230769 | 0.21515151515151518 | 0.26136363636363635 | 3 | 2.6100631515173585 | 0.00007903375922533348 | 0 |
| p2_on | B_op_noKdrive | 0.28974358974358977 | 0.2909090909090909 | 0.21893939393939393 | 3 | 2.674391340770028 | 0.0002498077397093044 | 0 |
| p2_on | C_op_withKdrive | 0.32051282051282054 | 0.2636363636363636 | 0.25 | 3 | 2.677465714941803 | 0.00029385358690402566 | 0 |

Notes:
- opBinsMode=1 and opBinsMode=2 both reported `MOP_STILL_COLLAPSED` (M_op hazard uniqueStatesVisited=3; changeFrac≈0.004–0.006).
- `RECOVERY_WINDOW_UNOBSERVED` persisted (recoverySampleCountMean=0 in hazard recovery window).

### Exploratory: Motif language under deadline selection v4 (gate‑conditioned)

Changes in v4:
- Gate‑conditioned sampling (hazard transitions counted only while gate is open).
- opBinsMode wired end‑to‑end with a symmetric dir‑9 M_op descriptor.
- Recovery window defined by absolute time offsets (pre/recovery/tail).
- EP allocation to motif transitions (repair/opK/total per edge).

Commands:

```bash
node scripts/run-deadline-opk-motif-compare.mjs --basis v2 --opBinsMode 2 --gateConditioned 1 --gateCheckEvery 5000
node scripts/run-deadline-opk-motif-compare.mjs --basis v2 --opBinsMode 2 --gateConditioned 1 --gateCheckEvery 1000
```

### compare_summary.csv (A/B/C, opBinsMode=2, gateCheckEvery=1000)

| condition | missFracMean | uptimeTailMean | errTailMean | epTotalRateMean | hazardOpUniqueStatesMean | hazardOpChangeFracMean | eventRecoveryWindowSampleCountMean |
| --- | --- | --- | --- | --- | --- | --- | --- |
| A_legacy | 0.24871794871794872 | 0.24015151515151514 | 0.24992897727272725 | 0.13349148763377666 | 3 | 0 | 4.338461538461539 |
| B_op_noKdrive | 0.3153846153846154 | 0.2149621212121212 | 0.27114109848484846 | 0.10238752782550212 | 45 | 0.00234375 | 4.35128205128205 |
| C_op_withKdrive | 0.34615384615384615 | 0.234375 | 0.2640625 | 0.1020039492419896 | 45 | 0 | 4.402564102564103 |

Top M_op transition edge (by count, hazard):
- `B: 28 -> 29` with `count=1`, `countRev=1`, `epTotalPerTrans=3.6749998728434243`

Notes:
- Exploratory signal only; not a safe claim.
- Verdict from run: `MOTIF_INSTRUMENTATION_TOO_SPARSE` (changeFrac below threshold).

## 2025-12-28 — Motif pressure v5: move-edge motifs (selection pressure only)

Changes in v5:
- Move-edge motifs from accepted opK token transfers (no state-diff sparsity).
- Exact epExact attribution per opK move (accept log).
- Recovery window defined by absolute time offsets only.

Command:

```bash
node scripts/run-deadline-opk-motif-compare.mjs --motifMode move_edges
```

### compare_move_edges_summary.csv (A/B/C, hazard + outside)

| condition | seeds | totalMovesHazardMean | uniqueEdgesHazardMean | totalEpHazardMean | epPerMoveHazardMean | topEdgeMass10HazardMean | symmetryGapHazardMean | coarseEPHazardMean | totalMovesOutsideMean | uniqueEdgesOutsideMean | totalEpOutsideMean | epPerMoveOutsideMean | topEdgeMass10OutsideMean | symmetryGapOutsideMean | coarseEPOutsideMean |
| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- |
| A_legacy | 10 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 |
| B_op_noKdrive | 10 | 22532.7 | 20 | 0 | 0 | 0.5119178471181178 | 0.017578651892342284 | 19.928591939468326 | 20795.1 | 20 | 0 | 0 | 0.5119368935003294 | 0.016375471493880113 | 17.334523673212953 |
| C_op_withKdrive | 10 | 22344.9 | 20 | 2.8141874461667613 | 0.00012711982646427508 | 0.512605558277774 | 0.01702172904215538 | 20.610106955189245 | 20594.3 | 20 | -1.7895937687018886 | -0.00008876035187131501 | 0.5124042448457777 | 0.016979679656265532 | 18.911044719781472 |

### top_edges_hazard.csv (top 10 by count)

| condition | fromIdx | toIdx | countMean | epSumMean |
| --- | --- | --- | --- | --- |
| B_op_noKdrive | 0 | 2 | 1172.8 | 0 |
| B_op_noKdrive | 0 | 3 | 1168.9 | 0 |
| B_op_noKdrive | 0 | 4 | 1164.1 | 0 |
| C_op_withKdrive | 0 | 4 | 1155.4 | 0.9177187407389283 |
| C_op_withKdrive | 0 | 1 | 1154.9 | 0.6921562643023208 |
| C_op_withKdrive | 0 | 2 | 1152.6 | 0.6837187497643754 |
| B_op_noKdrive | 0 | 1 | 1149 | 0 |
| C_op_withKdrive | 0 | 3 | 1144.3 | 0.5937187547795475 |
| B_op_noKdrive | 3 | 4 | 1134.6 | 0 |
| B_op_noKdrive | 1 | 0 | 1127.4 | 0 |

Verdict: `MOVE_EDGE_MOTIF_SIGNAL_TOO_WEAK`.

Notes:
- Exploratory signal only; not a safe claim.

## 2025-12-29 — Exploratory v6: P5 repair-action motifs (deadline selection)

Goal: detect whether selection pressure induces a non-trivial action-motif transition graph and measurable irreversibility/EP per motif.

Conditions (A/B/C):
- A_legacy: `opCouplingOn=0`, `sCouplingMode=0`, `opDriveOnK=0`
- B_op_noKdrive: `opCouplingOn=1`, `sCouplingMode=1`, `opDriveOnK=0`
- C_op_withKdrive: `opCouplingOn=1`, `sCouplingMode=1`, `opDriveOnK=1`

Preset: `scripts/params/op_motifs_selection/selection_base_tuned.json`

Motif definition:
- Motif stream = sequence of **accepted P5 moves** mapped to a discrete motif ID.
- Encodes: base vs meta (P5Base vs P5Meta), mismatch sign at q before write, and local K direction argmax at q (pre-write).
- Hazard window includes accepted moves within `[t0, t0 + deadline)` for each corruption event.
- Transitions are consecutive accepted motifs in each region (hazard vs outside).

EP attribution:
- `epPerMove` uses the exact per-move EP delta from the accept log for P5Base/P5Meta, aggregated per motif and per transition.

Command:

```bash
node scripts/run-deadline-opk-motif-compare.mjs --motifMode p5_actions
```

Verdict: `P5_ACTION_MOTIF_SIGNAL_PRESENT`.

Outputs:
- `.tmp/motif_pressure_v6/compare_p5_actions_summary.csv`
- `.tmp/motif_pressure_v6/top_p5_motifs_hazard.csv`
- `.tmp/motif_pressure_v6/top_p5_motifs_outside.csv`
- Per-seed `p5_actions_*.csv` and `p5_actions_summary.json` under `.tmp/motif_pressure_v6/<condition>/seed_<seed>/`

### compare_p5_actions_summary.csv (A/B/C)

| condition | seeds | totalMovesHazardMean | uniqueMotifsHazardMean | totalEpHazardMean | epPerMoveHazardMean | entropyHazardMean | topMotifMass10HazardMean | symmetryGapHazardMean | coarseEPHazardMean | totalMovesOutsideMean | uniqueMotifsOutsideMean | totalEpOutsideMean | epPerMoveOutsideMean | entropyOutsideMean | topMotifMass10OutsideMean | symmetryGapOutsideMean | coarseEPOutsideMean |
| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- |
| A_legacy | 10 | 11719 | 8 | 4525.055805206299 | 0.3861864998657495 | 1.2522111258239126 | 1 | 0.019053230093606842 | 40.03893908722678 | 10735.3 | 8 | 3109.247866153717 | 0.28951657204628956 | 1.2217404930290556 | 1 | 0.020964314373548266 | 28.097601174197244 |
| B_op_noKdrive | 10 | 7006.2 | 45 | 1755.5804804112763 | 0.25059811667480625 | 2.807964780162765 | 0.773718394357181 | 0.1417034601086114 | 609.6896664282565 | 6426.2 | 44.8 | 1284.9119864612817 | 0.20005478458989825 | 2.776430086584442 | 0.7886820617488385 | 0.13278695207813354 | 502.2010581095463 |
| C_op_withKdrive | 10 | 6974.4 | 44.9 | 1666.5524816006423 | 0.23904416852225677 | 2.808464260590534 | 0.7701770992265906 | 0.14253838878768704 | 591.7784222594294 | 6402.2 | 45 | 1191.7169846247882 | 0.18617035937393633 | 2.7736476860320725 | 0.7861359413966633 | 0.13323932246409018 | 493.7719224711239 |

### top_p5_motifs_hazard.csv (top 5 rows for B/C combined)

| condition | motifId | countMean | epPerMoveMean |
| --- | --- | --- | --- |
| B_op_noKdrive | 1 | 1033 | 0 |
| C_op_withKdrive | 1 | 1004.1 | 0 |
| B_op_noKdrive | 4 | 666.6 | 0 |
| C_op_withKdrive | 4 | 657.8 | 0 |
| C_op_withKdrive | 7 | 607.3 | 0 |

Notes:
- Exploratory signal only; not a safe claim.
- The P5-action motif stream under selection shows nontrivial entropy and coarseEP (hazard), but epPerMove for dominant motifs remains near 0 in B; C shows nonzero total ep but similar motif distribution.
- B/C show larger action alphabets than A, which inflates entropy and unique-motif counts relative to legacy.
- Controls still needed: instrumentation invariance (logging on/off) and null-selection (P6 off) to verify symmetry metrics collapse.

Exploratory takeaways:
- B/C exhibit ~45 motifs vs A’s 8 in the hazard window.
- B/C symmetryGap ≈ 0.14 vs A ≈ 0.02; coarseEP also higher in B/C than A.
- B and C remain close on motif statistics (no strong K-drive separation).

STATUS: exploratory signal present in P5-action motif lens; not safe-claim ready; next controls = invariance + null-selection.

## 2025-12-30 — Moving hazard homeostasis v1 (exploratory)

Commands:

```bash
node scripts/run-moving-hazard-homeostasis.mjs
node scripts/test-instrumentation-invariance.mjs
node scripts/run-moving-hazard-null-control.mjs
```

### moving_hazard_summary.csv (A/B/C/D)

| variant | missFrac | uptimeTailMean | errTailMean | recoveryMedian | recoveryP95 | epTotalMean | epRepairMean | epOpKMean | epClockMean | epTotalRateMean | motifEntropyMean | motifSymmetryGapMean | motifCoarseEPMean | trackingDeltaHkMean | trackingDeltaR2Mean | trackingDeltaDmagMean |
| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- |
| A | 0.14358974358974358 | 0.3060745924330081 | 0.22876448576291225 | 6000 | 34000 | 1877.4608616315402 | 707.2393541702858 | 0 | 1170.2215074612543 | 0.04889220993832135 | 1.2289738320048174 | 0.1120058388900355 | 15.196680432106765 | 0 | 0 | 0 |
| B | 0.2076923076923077 | 0.34627850512796604 | 0.21897545698768028 | 4000 | 33000 | 1820.8707121983552 | 531.2492079001206 | 0 | 1289.6215042982346 | 0.047418508130165495 | 2.712323627871284 | 0.5425805663484208 | 104.97002918578447 | 0.004000534210690459 | -0.02725751707422988 | 0.013972185691268091 |
| C | 0.16923076923076924 | 0.306977135055758 | 0.23495455868944762 | 6000 | 32000 | 1489.1362148092774 | 242.62240167677115 | 0 | 1246.5138131325061 | 0.038779588927324936 | 2.8979123619659912 | 0.5396283582751856 | 160.25593642141834 | -0.00001435049638575701 | 0.000415371897251464 | 0.00029812515121115666 |
| D | 0.1794871794871795 | 0.3278539578632685 | 0.22453577321163326 | 5000 | 31000 | 1451.4651905058297 | 233.9350941491767 | -0.16525600082017122 | 1217.6953523574732 | 0.03779857266942265 | 2.8964310918471408 | 0.5407960180143228 | 160.08659588679626 | -0.0002404157495507358 | -0.00009213038468174112 | 0.000378853068315321 |

### moving_hazard_by_hazard.csv (first 10 rows)

| variant | hazardIndex | missFrac | recoveryMedian | uptimeTailMean | motifEntropyMean | motifSymmetryGapMean | motifCoarseEPMean | trackingDeltaHkMean | trackingDeltaR2Mean | trackingDeltaDmagMean |
| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- |
| A | 4 | 0.125 | 8000 | 0.27649510401878097 | 1.2060817782515536 | 0.10890918289503013 | 14.751732447411063 | 0 | 0 | 0 |
| A | 12 | 0.18571428571428572 | 5000 | 0.2604991757216742 | 1.2290440283327733 | 0.11539377448459957 | 14.85846903026296 | 0 | 0 | 0 |
| A | 20 | 0.15 | 6000 | 0.2644900687547746 | 1.2386495014734111 | 0.10841378611697057 | 14.263672326735811 | 0 | 0 | 0 |
| A | 28 | 0.05 | 3000 | 0.38363099744784807 | 1.2360253772115894 | 0.1242636137290239 | 16.71836386165729 | 0 | 0 | 0 |
| A | 36 | 0.275 | 10000 | 0.2511145082688466 | 1.2340534998363322 | 0.11584887647925728 | 15.189110381890853 | 0 | 0 | 0 |
| A | 44 | 0.1 | 4000 | 0.36631811102196155 | 1.2375248527528484 | 0.11050756429167463 | 15.631553333844394 | 0 | 0 | 0 |
| A | 52 | 0.075 | 9000 | 0.2715500082276398 | 1.2527776208270072 | 0.10750285458807245 | 15.689848269887955 | 0 | 0 | 0 |
| A | 60 | 0.175 | 2000 | 0.4382598169502662 | 1.2204734038603107 | 0.10576276283473618 | 15.169300341242428 | 0 | 0 | 0 |
| B | 4 | 0.225 | 3000 | 0.3694337030799995 | 2.6363225159494803 | 0.5176010633150945 | 97.16277970792605 | -0.09824108042123787 | -0.07147505680892904 | 0.03919044363035312 |
| B | 12 | 0.2714285714285714 | 4000 | 0.32898333771387256 | 2.709785174378555 | 0.541769422912502 | 104.60242939190144 | 0.012161362153935125 | -0.0005815046189640175 | 0.00984924123884366 |

### instrumentation_invariance.csv (summary line)

`INSTRUMENTATION_INVARIANCE: PASS`

### moving_hazard_null_control.csv

| condition | missFrac | motifEntropyMean | motifSymmetryGapMean | motifCoarseEPMean | epTotalRate | epRepairMean | epOpKMean | epClockMean |
| --- | --- | --- | --- | --- | --- | --- | --- | --- |
| driven | 0.3894736842105263 | 2.633369164249779 | 0.609189207564794 | 68.00558881751738 | 0.05322741979626384 | 118.73131494932856 | 0 | 679.6799819946289 |
| null | 0.4 | 2.777094883040026 | 0.6547495701294636 | 78.83442061047839 | 0 | 0 | 0 | 0 |

Exploratory observations (not SAFE claims):
- Motif entropy and symmetry gap are higher in op modes (B/C/D) than legacy under moving hazard.
- Hazard-index splits show nonuniform missFrac and coarseEP across locations, with tracking deltas varying by hazard index.
- Null control shows nonzero motif symmetry/coarseEP even without drive; interpret motif irreversibility with caution.

Exploratory; not treated as SAFE claims.

## 2025-12-30 — Moving hazard finishing kit v1 (report triage, exploratory)

Commands:

```bash
node scripts/run-moving-hazard-stationary-vs-moving.mjs
node scripts/run-moving-hazard-speed-sweep.mjs
node scripts/run-moving-hazard-null-control.mjs
```

### finishing_stationary_vs_moving.csv

| variant | scenario | missFrac | uptimeTailMean | errTailMean | recoveryP95 | epTotalRateMean | epRepairPerActionMean | motifEntropyMean | motifSymmetryGapMean | avgPairwiseJSD | mutualInfoHM |
| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- |
| A | stationary | 0.31025641025641026 | 0.31193181462412234 | 0.23392851628428554 | 14000 | 0.07331138221373931 | 0.23958953309744335 | 1.2225376869723712 | 0.16244376098879895 | 0 | 0 |
| C | stationary | 0.358974358974359 | 0.31549754376677447 | 0.2336301438945671 | 14000 | 0.05579842753366621 | 0.07934753818426825 | 2.8318270387093767 | 0.6836483652045188 | 0 | 0 |
| A | moving | 0.36666666666666664 | 0.2971397797359336 | 0.23377523651562113 | 13000 | 0.07564282829284667 | 0.23771565935207054 | 1.2079269396037038 | 0.16167104250074193 | 0.0008472426105986085 | 0.0014996992411072925 |
| C | moving | 0.40512820512820513 | 0.29730204624435397 | 0.23963946310100168 | 14000 | 0.059656648590569024 | 0.08285897289903178 | 2.7924055639966796 | 0.669875219438312 | 0.011278817261180393 | 0.020986926863386955 |

### finishing_speed_sweep.csv

| variant | hazardHoldEvents | missFrac | uptimeTailMean | recoveryP95 | epTotalRateMean | epRepairPerActionMean | avgPairwiseJSD | mutualInfoHM |
| --- | --- | --- | --- | --- | --- | --- | --- | --- |
| A | 1 | 0.3368421052631579 | 0.35438088227561915 | 12000 | 0.07087427138077583 | 0.20518421541046442 | 0.0030004860557128576 | 0.004632602205252064 |
| C | 1 | 0.5578947368421052 | 0.2074385263858948 | 11000 | 0.06971117200499108 | 0.07878476331499748 | 0.017946594269727917 | 0.03223347828855402 |
| A | 2 | 0.3368421052631579 | 0.29203726098462934 | 13000 | 0.07265633449654829 | 0.22602385629463298 | 0.0024387564498305017 | 0.0042044001668488 |
| C | 2 | 0.42105263157894735 | 0.25960789210789204 | 14000 | 0.061650835362174784 | 0.07366399943281124 | 0.021107866501205556 | 0.038199422221503666 |
| A | 4 | 0.3894736842105263 | 0.2657361644203749 | 14000 | 0.08243014468377098 | 0.23482779602346723 | 0.003435203273471208 | 0.0053208608629142645 |
| C | 4 | 0.3894736842105263 | 0.35825591075591073 | 13000 | 0.05322741979626384 | 0.06684809567591789 | 0.03111080891541846 | 0.04914689490844264 |
| A | 8 | 0.3157894736842105 | 0.40411848385532584 | 11000 | 0.06257633481778598 | 0.1807601628961216 | 0.0014922743745757585 | 0.0018553457077495018 |
| C | 8 | 0.5263157894736842 | 0.25758092784408576 | 12000 | 0.06618879314609132 | 0.07081906572028619 | 0.0335304684309491 | 0.04138083112968251 |
| A | 16 | 0.3368421052631579 | 0.3629739558686927 | 11000 | 0.06693827152118348 | 0.1930572003981302 | 0.00039116068019315087 | 0.00021030583901341093 |
| C | 16 | 0.4105263157894737 | 0.3413148839464629 | 12000 | 0.05514781447490707 | 0.06455372232540074 | 0.0220927752415767 | 0.01127324463914348 |

### finishing_null_control_ep_per_action.csv

| condition | epTotalRate | epRepairMean | epRepairPerActionMean | repairActionCountMean |
| --- | --- | --- | --- | --- |
| driven | 0.05322741979626384 | 118.73131494932856 | 0.06684809567591789 | 1251.5263157894738 |
| null | 0 | 0 | 0 | 1352.2736842105264 |

Exploratory takeaways (not SAFE claims):
- EP/action attribution sanity: driven vs null shows epTotalRate and epRepairPerAction collapse to 0 while repair-action counts stay comparable.
- Context sensitivity appears only when contexts exist: stationary hazards have avgPairwiseJSD=0, mutualInfoHM=0; moving hazards are nonzero.
- Op-coupled mode increases context dependence of repair motifs: moving hazard shows much larger avgPairwiseJSD/mutualInfoHM for C vs A.
- Timescale window suggests EP–reliability advantage: at hazardHoldEvents=4, C matches A’s missFrac with lower epTotalRate and higher uptimeTail, plus much lower epRepairPerAction.

Caveats:
- Not robust homeostasis: uptimeTail remains well below 1 across these regimes.
- motifSymmetryGap/coarseEP are descriptive action-graph measures, not thermodynamic EP.
- Evidence supports context-dependent repair policy, not direct “K tracks hazard location.”
</file>

<file path="crates/sim-core/src/lib.rs">
use js_sys::{
    Array, Float32Array, Float64Array, Int16Array, Int8Array, Object, Reflect, Uint16Array,
    Uint32Array, Uint8Array,
};
use wasm_bindgen::prelude::*;

const DEFAULT_GRID_SIZE: usize = 16;
const MAX_META_LAYERS: u16 = 16;
const MOVE_KIND_COUNT: usize = 11;
const MOVE_KIND_LABELS: [&str; MOVE_KIND_COUNT] = [
    "X",
    "P1Base",
    "P1Meta",
    "P2Base",
    "P2Meta",
    "P4Base",
    "P4Meta",
    "P5Base",
    "P5Meta",
    "OpK",
    "Clock",
];

const MOVE_X: usize = 0;
const MOVE_P1_BASE: usize = 1;
const MOVE_P1_META: usize = 2;
const MOVE_P2_BASE: usize = 3;
const MOVE_P2_META: usize = 4;
const MOVE_P4_BASE: usize = 5;
const MOVE_P4_META: usize = 6;
const MOVE_P5_BASE: usize = 7;
const MOVE_P5_META: usize = 8;
const MOVE_OPK: usize = 9;
const MOVE_CLOCK: usize = 10;

const OP_STENCIL_CROSS: [(i32, i32); 5] = [(0, 0), (1, 0), (-1, 0), (0, 1), (0, -1)];
const OP_STENCIL_FULL: [(i32, i32); 9] = [
    (0, 0),
    (1, 0),
    (-1, 0),
    (0, 1),
    (0, -1),
    (1, 1),
    (-1, 1),
    (1, -1),
    (-1, -1),
];

#[wasm_bindgen]
pub struct Sim {
    n: usize,
    positions: Vec<f32>, // [x0,y0,x1,y1,...] in [0,1)
    w: Vec<u8>, // upper-triangular edge weights (P1), length n*(n-1)/2
    n_counter: Vec<i16>,
    a_counter: Vec<u16>,
    s_field: Vec<u8>,
    meta_field: Vec<u8>,
    meta_n_field: Vec<i16>,
    meta_a_field: Vec<u16>,
    meta_w_edges: Vec<u8>,
    op_k: Vec<u8>,
    rng: u32,
    params: Params,
    diag: DiagTotals,
    phase: u8,
    p3_cycle_len: u8,
    p3_start_positions: Vec<f32>,
    p3_obs1: Vec<f32>,
    p3_obs2: Vec<f32>,
    p3_disp_x: f32,
    p3_disp_y: f32,
    p3_disp_mag: f32,
    p3_loop_area: f32,
    sum_w: i32,
    sum_s: i32,
    ep_naive_total: f64,
    ep_exact_total: f64,
    ep_q_stats: [EpQStats; MOVE_KIND_COUNT],
    ep_naive_by_move: [f64; MOVE_KIND_COUNT],
    ep_exact_by_move: [f64; MOVE_KIND_COUNT],
    accept_log_u32: Vec<u32>,
    accept_log_ep: Vec<f64>,
    accept_log_overflowed: bool,
    step_count: u32,
    clock_state: u8,
    clock_q: i64,
    clock_fwd: u64,
    clock_bwd: u64,
}

#[derive(Clone, Copy)]
struct Params {
    beta: f32,
    step_size: f32,
    p_write: f32, // probability of proposing a P1 write step (vs X move)
    p_n_write: f32, // probability of proposing a P4 counter write step
    p_a_write: f32, // probability of proposing a P2 apparatus write step
    p_s_write: f32, // probability of proposing a P5 field write step
    p3_on: bool,    // protocol-cycle scheduling (P3)
    p6_on: bool,    // resource transduction (P6)
    p6_s_factor: f32,
    mu_high: f32,
    mu_low: f32,
    // Deliverable A energy params (minimal subset for X + P1)
    kappa_rep: f32,
    r0: f32,
    kappa_bond: f32,
    r_star: f32,
    lambda_w: f32,
    l_w: u8,
    lambda_n: f32,
    l_n: i16,
    lambda_a: f32,
    l_a: u16,
    lambda_s: f32,
    l_s: u8,
    grid_size: u16,
    r_propose: f32, // neighbor radius for P1 proposals
    meta_layers: u16,
    eta: f32,
    eta_drive: f32,
    op_coupling_on: bool,
    op_stencil: u8,
    op_budget_k: u8,
    op_k_target_weight: f32,
    s_coupling_mode: u8,
    op_drive_on_k: bool,
    accept_log_on: bool,
    accept_log_mask: u32,
    accept_log_cap: u32,
    ep_debug: bool,
    init_random: bool,
    code_noise_rate: f32,
    code_noise_batch: u16,
    code_noise_layer: u16,
    clock_on: bool,
    clock_k: u8,
    clock_frac: f32,
    clock_uses_p6: bool,
    repair_clock_gated: bool,
    repair_gate_mode: u8,
    repair_gate_span: u8,
}

#[wasm_bindgen]
impl Sim {
    fn accept_log_push(
        &mut self,
        t: u32,
        q: u32,
        move_id: u8,
        aux_a: u8,
        aux_b: u8,
        aux_c: u8,
        ep_delta: f64,
    ) {
        if !self.params.accept_log_on {
            return;
        }
        if self.accept_log_overflowed {
            return;
        }
        let mask = 1u32 << (move_id as u32);
        if (self.params.accept_log_mask & mask) == 0 {
            return;
        }
        if (self.accept_log_ep.len() as u32) >= self.params.accept_log_cap {
            self.accept_log_overflowed = true;
            return;
        }
        self.accept_log_u32.push(t);
        self.accept_log_u32.push(q);
        let meta = (move_id as u32)
            | ((aux_a as u32) << 8)
            | ((aux_b as u32) << 16)
            | ((aux_c as u32) << 24);
        self.accept_log_u32.push(meta);
        self.accept_log_ep.push(ep_delta);
    }

    #[wasm_bindgen(constructor)]
    pub fn new(n: usize, seed: u32) -> Sim {
        let m = n.saturating_mul(n.saturating_sub(1)) / 2;
        let mut sim = Sim {
            n,
            positions: vec![0.0; 2 * n],
            w: vec![0u8; m],
            n_counter: vec![0i16; n],
            a_counter: vec![0u16; n],
            s_field: vec![0u8; DEFAULT_GRID_SIZE * DEFAULT_GRID_SIZE],
            meta_field: Vec::new(),
            meta_n_field: Vec::new(),
            meta_a_field: Vec::new(),
            meta_w_edges: Vec::new(),
            op_k: Vec::new(),
            rng: if seed == 0 { 1 } else { seed },
            params: Params {
                beta: 1.0,
                step_size: 0.01,
                p_write: 0.2,
                p_n_write: 0.05,
                p_a_write: 0.05,
                p_s_write: 0.05,
                p3_on: false,
                p6_on: false,
                p6_s_factor: 1.0,
                mu_high: 1.0,
                mu_low: -1.0,
                kappa_rep: 50.0,
                r0: 0.03,
                kappa_bond: 3.0,
                r_star: 0.18,
                lambda_w: 0.12,
                l_w: 5,
                lambda_n: 0.5,
                l_n: 6,
                lambda_a: 0.5,
                l_a: 6,
                lambda_s: 0.5,
                l_s: 6,
                grid_size: DEFAULT_GRID_SIZE as u16,
                r_propose: 0.22,
                meta_layers: 0,
                eta: 0.0,
                eta_drive: 0.0,
                op_coupling_on: false,
                op_stencil: 0,
                op_budget_k: 16,
                op_k_target_weight: 1.0,
                s_coupling_mode: 0,
                op_drive_on_k: true,
                accept_log_on: false,
                accept_log_mask: 0,
                accept_log_cap: 100000,
                ep_debug: false,
                init_random: false,
                code_noise_rate: 0.0,
                code_noise_batch: 1,
                code_noise_layer: 0,
                clock_on: false,
                clock_k: 8,
                clock_frac: 0.2,
                clock_uses_p6: true,
                repair_clock_gated: false,
                repair_gate_mode: 0,
                repair_gate_span: 1,
            },
            diag: DiagTotals::default(),
            phase: 0,
            p3_cycle_len: 0,
            p3_start_positions: Vec::new(),
            p3_obs1: Vec::new(),
            p3_obs2: Vec::new(),
            p3_disp_x: 0.0,
            p3_disp_y: 0.0,
            p3_disp_mag: 0.0,
            p3_loop_area: 0.0,
            sum_w: 0,
            sum_s: 0,
            ep_naive_total: 0.0,
            ep_exact_total: 0.0,
            ep_q_stats: [EpQStats::default(); MOVE_KIND_COUNT],
            ep_naive_by_move: [0.0; MOVE_KIND_COUNT],
            ep_exact_by_move: [0.0; MOVE_KIND_COUNT],
            accept_log_u32: Vec::new(),
            accept_log_ep: Vec::new(),
            accept_log_overflowed: false,
            step_count: 0,
            clock_state: 0,
            clock_q: 0,
            clock_fwd: 0,
            clock_bwd: 0,
        };
        for i in 0..n {
            let x = sim.rand01();
            let y = sim.rand01();
            sim.positions[2 * i] = x;
            sim.positions[2 * i + 1] = y;
        }
        sim
    }

    pub fn n(&self) -> usize {
        self.n
    }

    pub fn step(&mut self, steps: u32) {
        // Null-regime mixture of reversible kernels (Deliverables A/B):
        // - X: symmetric position proposals + Metropolis against E(Z)
        // - P1: symmetric +/-1 write proposals + Metropolis against E(Z)
        for _ in 0..steps {
            self.step_count = self.step_count.wrapping_add(1);
            let mut step_diag = StepDiag::default();
            if self.params.p3_on {
                self.protocol_step(&mut step_diag);
            } else {
                let r = self.rand01();
                if r < self.params.p_write {
                    let delta = if self.params.meta_layers == 0 {
                        self.p1_write_step()
                    } else {
                        let target = self.pick_write_target();
                        if target == 0 {
                            self.p1_write_step()
                        } else {
                            self.p1_write_step_meta((target - 1) as usize)
                        }
                    };
                    if delta > 0 {
                        step_diag.w_plus = 1;
                    } else if delta < 0 {
                        step_diag.w_minus = 1;
                    }
                } else if r < self.params.p_write + self.params.p_n_write {
                    let delta = if self.params.meta_layers == 0 {
                        self.p4_write_step()
                    } else {
                        let target = self.pick_write_target();
                        if target == 0 {
                            self.p4_write_step()
                        } else {
                            self.p4_write_step_meta((target - 1) as usize)
                        }
                    };
                    if delta > 0 {
                        step_diag.n_plus = 1;
                    } else if delta < 0 {
                        step_diag.n_minus = 1;
                    }
                } else if r < self.params.p_write + self.params.p_n_write + self.params.p_a_write {
                    let delta = if self.params.meta_layers == 0 {
                        self.p2_write_step()
                    } else {
                        let target = self.pick_write_target();
                        if target == 0 {
                            self.p2_write_step()
                        } else {
                            self.p2_write_step_meta((target - 1) as usize)
                        }
                    };
                    if delta > 0 {
                        step_diag.a_plus = 1;
                    } else if delta < 0 {
                        step_diag.a_minus = 1;
                    }
                } else if r
                    < self.params.p_write
                        + self.params.p_n_write
                        + self.params.p_a_write
                        + self.params.p_s_write
                {
                    let delta = if self.params.meta_layers == 0 {
                        self.p5_write_step()
                    } else if self.params.op_coupling_on {
                        let target = self.pick_p5_target_op();
                        let layers = self.params.meta_layers as usize;
                        if target == 0 {
                            self.p5_write_step()
                        } else if target <= layers {
                            self.p5_write_step_meta((target - 1) as usize)
                        } else {
                            self.p5_write_step_opk(target - (layers + 1))
                        }
                    } else {
                        let target = self.pick_write_target();
                        if target == 0 {
                            self.p5_write_step()
                        } else {
                            self.p5_write_step_meta((target - 1) as usize)
                        }
                    };
                    if delta > 0 {
                        step_diag.s_plus = 1;
                    } else if delta < 0 {
                        step_diag.s_minus = 1;
                    }
                } else {
                    self.x_move_step();
                }
            }
            self.diag.push(step_diag);
            self.maybe_code_noise();
        }
    }

    pub fn positions(&self) -> Float32Array {
        Float32Array::from(self.positions.as_slice())
    }

    pub fn counters(&self) -> Int16Array {
        Int16Array::from(self.n_counter.as_slice())
    }

    pub fn apparatus(&self) -> Uint16Array {
        Uint16Array::from(self.a_counter.as_slice())
    }

    #[wasm_bindgen]
    pub fn bonds(&self, threshold: u8) -> Uint32Array {
        let mut out: Vec<u32> = Vec::new();
        out.reserve(self.w.len()); // upper bound; pairs will be 2*edges
        for i in 0..self.n {
            for j in (i + 1)..self.n {
                let idx = edge_index(self.n, i, j);
                if self.w[idx] >= threshold {
                    out.push(i as u32);
                    out.push(j as u32);
                }
            }
        }
        Uint32Array::from(out.as_slice())
    }

    pub fn field(&self) -> Uint8Array {
        Uint8Array::from(self.s_field.as_slice())
    }

    pub fn base_s_field(&self) -> Uint8Array {
        Uint8Array::from(self.s_field.as_slice())
    }

    pub fn meta_layers(&self) -> u16 {
        self.params.meta_layers
    }

    pub fn meta_field(&self) -> Uint8Array {
        if self.params.meta_layers == 0 {
            Uint8Array::new_with_length(0)
        } else {
            Uint8Array::from(self.meta_field.as_slice())
        }
    }

    pub fn meta_n_field(&self) -> Int16Array {
        if self.params.meta_layers == 0 {
            Int16Array::new_with_length(0)
        } else {
            Int16Array::from(self.meta_n_field.as_slice())
        }
    }

    pub fn meta_a_field(&self) -> Uint16Array {
        if self.params.meta_layers == 0 {
            Uint16Array::new_with_length(0)
        } else {
            Uint16Array::from(self.meta_a_field.as_slice())
        }
    }

    pub fn meta_w_edges(&self) -> Uint8Array {
        if self.params.meta_layers == 0 {
            Uint8Array::new_with_length(0)
        } else {
            Uint8Array::from(self.meta_w_edges.as_slice())
        }
    }

    pub fn meta_edge_count(&self) -> u32 {
        meta_edge_count(self.params.grid_size as usize) as u32
    }

    pub fn op_r_count(&self) -> u32 {
        self.op_r_count_internal() as u32
    }

    pub fn op_offsets(&self) -> Int8Array {
        let offsets = self.op_offsets_internal();
        let mut out: Vec<i8> = Vec::with_capacity(offsets.len() * 2);
        for (dx, dy) in offsets {
            out.push(*dx as i8);
            out.push(*dy as i8);
        }
        Int8Array::from(out.as_slice())
    }

    pub fn op_k_tokens(&self) -> Uint8Array {
        if !self.params.op_coupling_on || self.params.meta_layers == 0 {
            Uint8Array::new_with_length(0)
        } else {
            Uint8Array::from(self.op_k.as_slice())
        }
    }

    pub fn op_budget_k(&self) -> u32 {
        self.params.op_budget_k as u32
    }

    pub fn op_interfaces(&self) -> u32 {
        self.params.meta_layers as u32
    }

    pub fn op_stencil_id(&self) -> u32 {
        self.params.op_stencil as u32
    }

    pub fn ep_total(&self) -> f64 {
        self.ep_naive_total
    }

    pub fn ep_naive_total(&self) -> f64 {
        self.ep_naive_total
    }

    pub fn ep_exact_total(&self) -> f64 {
        self.ep_exact_total
    }

    pub fn ep_naive_by_move(&self) -> Float64Array {
        Float64Array::from(self.ep_naive_by_move.as_slice())
    }

    pub fn ep_exact_by_move(&self) -> Float64Array {
        Float64Array::from(self.ep_exact_by_move.as_slice())
    }

    pub fn ep_move_labels(&self) -> Array {
        let labels = Array::new();
        for label in MOVE_KIND_LABELS {
            labels.push(&JsValue::from_str(label));
        }
        labels
    }

    pub fn accept_log_len(&self) -> u32 {
        self.accept_log_ep.len() as u32
    }

    pub fn accept_log_u32(&self) -> Uint32Array {
        Uint32Array::from(self.accept_log_u32.as_slice())
    }

    pub fn accept_log_ep(&self) -> Float64Array {
        Float64Array::from(self.accept_log_ep.as_slice())
    }

    pub fn accept_log_overflowed(&self) -> bool {
        self.accept_log_overflowed
    }

    pub fn accept_log_clear(&mut self) {
        self.accept_log_u32.clear();
        self.accept_log_ep.clear();
        self.accept_log_overflowed = false;
    }

    pub fn clock_state(&self) -> u32 {
        self.clock_state as u32
    }

    pub fn clock_q(&self) -> i64 {
        self.clock_q
    }

    pub fn clock_fwd(&self) -> u64 {
        self.clock_fwd
    }

    pub fn clock_bwd(&self) -> u64 {
        self.clock_bwd
    }

    pub fn ep_q_stats(&self) -> Object {
        let labels = Array::new();
        let mut means: Vec<f64> = Vec::with_capacity(MOVE_KIND_COUNT);
        let mut max_abs: Vec<f64> = Vec::with_capacity(MOVE_KIND_COUNT);
        let mut counts: Vec<u32> = Vec::with_capacity(MOVE_KIND_COUNT);
        for (idx, label) in MOVE_KIND_LABELS.iter().enumerate() {
            labels.push(&JsValue::from_str(label));
            let stats = self.ep_q_stats[idx];
            let mean = if stats.count > 0 {
                stats.sum / (stats.count as f64)
            } else {
                0.0
            };
            means.push(mean);
            max_abs.push(stats.max_abs);
            counts.push(stats.count.min(u64::from(u32::MAX)) as u32);
        }
        let o = Object::new();
        let _ = Reflect::set(&o, &JsValue::from_str("labels"), &labels);
        let _ = Reflect::set(&o, &JsValue::from_str("mean"), &Float64Array::from(means.as_slice()));
        let _ = Reflect::set(
            &o,
            &JsValue::from_str("maxAbs"),
            &Float64Array::from(max_abs.as_slice()),
        );
        let _ = Reflect::set(&o, &JsValue::from_str("count"), &Uint32Array::from(counts.as_slice()));
        o
    }

    #[wasm_bindgen]
    pub fn apply_perturbation(&mut self, params: JsValue) {
        if !params.is_object() {
            return;
        }
        let target = match get_string(&params, "target") {
            Some(v) => v,
            None => return,
        };
        let mode = get_string(&params, "mode").unwrap_or_else(|| "randomize".to_string());
        let region = get_string(&params, "region").unwrap_or_else(|| "all".to_string());
        let frac = get_f32(&params, "frac").unwrap_or(0.0).clamp(0.0, 1.0);
        if frac <= 0.0 {
            return;
        }
        let target_quadrant = if region == "quadrant" {
            Some(get_u8(&params, "quadrant").unwrap_or(0).min(3))
        } else {
            None
        };
        let target_stripe = if region == "stripe" {
            let bins = get_u8(&params, "bins").unwrap_or(self.params.clock_k).max(1);
            let span = get_u8(&params, "span").unwrap_or(1).max(1);
            let bin = get_u8(&params, "bin").unwrap_or(0);
            Some((bins, span, bin))
        } else {
            None
        };
        let seed = get_u32(&params, "seed").unwrap_or_else(|| self.rand_u32());
        let mut rng = seed;
        let mut next_u32 = || {
            // xorshift32
            let mut x = rng;
            x ^= x << 13;
            x ^= x >> 17;
            x ^= x << 5;
            rng = x;
            x
        };

        let g = self.params.grid_size as usize;
        let cells = g * g;
        let l_s = self.params.l_s;
        let mut touched_base = false;

        if target == "baseS" {
            for (idx, s) in self.s_field.iter_mut().enumerate() {
                if let Some(q) = target_quadrant {
                    let x = idx % g;
                    let y = idx / g;
                    let qx = if x < g / 2 { 0 } else { 1 };
                    let qy = if y < g / 2 { 0 } else { 1 };
                    let quad = (qy * 2 + qx) as u8;
                    if quad != q {
                        continue;
                    }
                }
                if let Some((bins, span, bin)) = target_stripe {
                    let x = idx % g;
                    let stripe = ((x as f32 / g as f32) * (bins as f32)).floor() as u8;
                    let span = span.min(bins);
                    let mut ok = false;
                    for i in 0..span {
                        if stripe == (bin + i) % bins {
                            ok = true;
                            break;
                        }
                    }
                    if !ok {
                        continue;
                    }
                }
                let u = next_u32() >> 8;
                let r = (u as f32) / ((1u32 << 24) as f32);
                if r < frac {
                    *s = match mode.as_str() {
                        "zero" => 0,
                        _ => (next_u32() % (l_s as u32 + 1)) as u8,
                    };
                    touched_base = true;
                }
            }
        } else if target == "metaS" {
            let layer = get_u16(&params, "layer").unwrap_or(0) as usize;
            if layer >= self.params.meta_layers as usize {
                return;
            }
            let base = layer * cells;
            let end = base + cells;
            for (offset, s) in self.meta_field[base..end].iter_mut().enumerate() {
                if let Some(q) = target_quadrant {
                    let x = offset % g;
                    let y = offset / g;
                    let qx = if x < g / 2 { 0 } else { 1 };
                    let qy = if y < g / 2 { 0 } else { 1 };
                    let quad = (qy * 2 + qx) as u8;
                    if quad != q {
                        continue;
                    }
                }
                if let Some((bins, span, bin)) = target_stripe {
                    let x = offset % g;
                    let stripe = ((x as f32 / g as f32) * (bins as f32)).floor() as u8;
                    let span = span.min(bins);
                    let mut ok = false;
                    for i in 0..span {
                        if stripe == (bin + i) % bins {
                            ok = true;
                            break;
                        }
                    }
                    if !ok {
                        continue;
                    }
                }
                let u = next_u32() >> 8;
                let r = (u as f32) / ((1u32 << 24) as f32);
                if r < frac {
                    *s = match mode.as_str() {
                        "zero" => 0,
                        _ => (next_u32() % (l_s as u32 + 1)) as u8,
                    };
                }
            }
        }

        if touched_base {
            self.recompute_sum_s();
        }
    }

    #[wasm_bindgen]
    pub fn energy_breakdown(&self) -> Object {
        let (u_rep, u_bond, e_w, e_n, e_a, e_s, total) = self.energy_breakdown_inner();
        let o = Object::new();
        let _ = Reflect::set(&o, &JsValue::from_str("uRep"), &JsValue::from_f64(u_rep as f64));
        let _ = Reflect::set(&o, &JsValue::from_str("uBond"), &JsValue::from_f64(u_bond as f64));
        let _ = Reflect::set(&o, &JsValue::from_str("eW"), &JsValue::from_f64(e_w as f64));
        let _ = Reflect::set(&o, &JsValue::from_str("eN"), &JsValue::from_f64(e_n as f64));
        let _ = Reflect::set(&o, &JsValue::from_str("eA"), &JsValue::from_f64(e_a as f64));
        let _ = Reflect::set(&o, &JsValue::from_str("eS"), &JsValue::from_f64(e_s as f64));
        let _ = Reflect::set(&o, &JsValue::from_str("total"), &JsValue::from_f64(total as f64));
        o
    }

    #[wasm_bindgen]
    pub fn diagnostics(&self) -> Object {
        let (w_plus, w_minus, n_plus, n_minus, a_plus, a_minus, s_plus, s_minus, window) =
            self.diag.counts();
        let (
            w_plus_h,
            w_minus_h,
            w_plus_l,
            w_minus_l,
            n_plus_h,
            n_minus_h,
            n_plus_l,
            n_minus_l,
            a_plus_h,
            a_minus_h,
            a_plus_l,
            a_minus_l,
            s_plus_h,
            s_minus_h,
            s_plus_l,
            s_minus_l,
        ) = self.diag.counts_hl();
        let o = Object::new();
        let _ = Reflect::set(&o, &JsValue::from_str("wPlus"), &JsValue::from_f64(w_plus as f64));
        let _ = Reflect::set(&o, &JsValue::from_str("wMinus"), &JsValue::from_f64(w_minus as f64));
        let _ = Reflect::set(&o, &JsValue::from_str("nPlus"), &JsValue::from_f64(n_plus as f64));
        let _ = Reflect::set(&o, &JsValue::from_str("nMinus"), &JsValue::from_f64(n_minus as f64));
        let _ = Reflect::set(&o, &JsValue::from_str("aPlus"), &JsValue::from_f64(a_plus as f64));
        let _ = Reflect::set(&o, &JsValue::from_str("aMinus"), &JsValue::from_f64(a_minus as f64));
        let _ = Reflect::set(&o, &JsValue::from_str("sPlus"), &JsValue::from_f64(s_plus as f64));
        let _ = Reflect::set(
            &o,
            &JsValue::from_str("sMinus"),
            &JsValue::from_f64(s_minus as f64),
        );
        let _ = Reflect::set(&o, &JsValue::from_str("wPlusH"), &JsValue::from_f64(w_plus_h as f64));
        let _ = Reflect::set(&o, &JsValue::from_str("wMinusH"), &JsValue::from_f64(w_minus_h as f64));
        let _ = Reflect::set(&o, &JsValue::from_str("wPlusL"), &JsValue::from_f64(w_plus_l as f64));
        let _ = Reflect::set(&o, &JsValue::from_str("wMinusL"), &JsValue::from_f64(w_minus_l as f64));
        let _ = Reflect::set(&o, &JsValue::from_str("nPlusH"), &JsValue::from_f64(n_plus_h as f64));
        let _ = Reflect::set(&o, &JsValue::from_str("nMinusH"), &JsValue::from_f64(n_minus_h as f64));
        let _ = Reflect::set(&o, &JsValue::from_str("nPlusL"), &JsValue::from_f64(n_plus_l as f64));
        let _ = Reflect::set(&o, &JsValue::from_str("nMinusL"), &JsValue::from_f64(n_minus_l as f64));
        let _ = Reflect::set(&o, &JsValue::from_str("aPlusH"), &JsValue::from_f64(a_plus_h as f64));
        let _ = Reflect::set(&o, &JsValue::from_str("aMinusH"), &JsValue::from_f64(a_minus_h as f64));
        let _ = Reflect::set(&o, &JsValue::from_str("aPlusL"), &JsValue::from_f64(a_plus_l as f64));
        let _ = Reflect::set(&o, &JsValue::from_str("aMinusL"), &JsValue::from_f64(a_minus_l as f64));
        let _ = Reflect::set(&o, &JsValue::from_str("sPlusH"), &JsValue::from_f64(s_plus_h as f64));
        let _ = Reflect::set(&o, &JsValue::from_str("sMinusH"), &JsValue::from_f64(s_minus_h as f64));
        let _ = Reflect::set(&o, &JsValue::from_str("sPlusL"), &JsValue::from_f64(s_plus_l as f64));
        let _ = Reflect::set(&o, &JsValue::from_str("sMinusL"), &JsValue::from_f64(s_minus_l as f64));
        let _ = Reflect::set(&o, &JsValue::from_str("window"), &JsValue::from_f64(window as f64));
        let (j_w, a_w, sigma_w) = diag_flux_affinity(w_plus, w_minus, window);
        let (j_n, a_n, sigma_n) = diag_flux_affinity(n_plus, n_minus, window);
        let (j_a, a_a, sigma_a) = diag_flux_affinity(a_plus, a_minus, window);
        let (j_s, a_s, sigma_s) = diag_flux_affinity(s_plus, s_minus, window);
        let a_m6_w = diag_m6_affinity(w_plus_h, w_minus_h, w_plus_l, w_minus_l);
        let a_m6_n = diag_m6_affinity(n_plus_h, n_minus_h, n_plus_l, n_minus_l);
        let a_m6_a = diag_m6_affinity(a_plus_h, a_minus_h, a_plus_l, a_minus_l);
        let a_m6_s = diag_m6_affinity(s_plus_h, s_minus_h, s_plus_l, s_minus_l);
        let _ = Reflect::set(&o, &JsValue::from_str("aM6W"), &JsValue::from_f64(a_m6_w as f64));
        let _ = Reflect::set(&o, &JsValue::from_str("aM6N"), &JsValue::from_f64(a_m6_n as f64));
        let _ = Reflect::set(&o, &JsValue::from_str("aM6A"), &JsValue::from_f64(a_m6_a as f64));
        let _ = Reflect::set(&o, &JsValue::from_str("aM6S"), &JsValue::from_f64(a_m6_s as f64));
        let _ = Reflect::set(&o, &JsValue::from_str("jW"), &JsValue::from_f64(j_w as f64));
        let _ = Reflect::set(&o, &JsValue::from_str("aW"), &JsValue::from_f64(a_w as f64));
        let _ = Reflect::set(&o, &JsValue::from_str("jN"), &JsValue::from_f64(j_n as f64));
        let _ = Reflect::set(&o, &JsValue::from_str("aN"), &JsValue::from_f64(a_n as f64));
        let _ = Reflect::set(&o, &JsValue::from_str("jA"), &JsValue::from_f64(j_a as f64));
        let _ = Reflect::set(&o, &JsValue::from_str("aA"), &JsValue::from_f64(a_a as f64));
        let _ = Reflect::set(&o, &JsValue::from_str("jS"), &JsValue::from_f64(j_s as f64));
        let _ = Reflect::set(&o, &JsValue::from_str("aS"), &JsValue::from_f64(a_s as f64));
        let _ = Reflect::set(
            &o,
            &JsValue::from_str("sigmaMem"),
            &JsValue::from_f64((sigma_w + sigma_n + sigma_a + sigma_s) as f64),
        );
        let _ = Reflect::set(&o, &JsValue::from_str("p3CycleLen"), &JsValue::from_f64(self.p3_cycle_len as f64));
        let _ = Reflect::set(&o, &JsValue::from_str("p3DispX"), &JsValue::from_f64(self.p3_disp_x as f64));
        let _ = Reflect::set(&o, &JsValue::from_str("p3DispY"), &JsValue::from_f64(self.p3_disp_y as f64));
        let _ = Reflect::set(&o, &JsValue::from_str("p3DispMag"), &JsValue::from_f64(self.p3_disp_mag as f64));
        let _ = Reflect::set(&o, &JsValue::from_str("p3LoopArea"), &JsValue::from_f64(self.p3_loop_area as f64));
        let hist = self.w_histogram();
        let _ = Reflect::set(&o, &JsValue::from_str("wHist"), &hist);
        let s_hist = self.s_histogram();
        let _ = Reflect::set(&o, &JsValue::from_str("sHist"), &s_hist);
        o
    }

    #[wasm_bindgen]
    pub fn set_params(&mut self, params: JsValue) {
        // Accept a plain JS object with numeric fields; ignore missing fields.
        if !params.is_object() {
            return;
        }

        let prev_p3 = self.params.p3_on;
        let prev_grid_size = self.params.grid_size;
        let prev_meta_layers = self.params.meta_layers;
        let prev_op_on = self.params.op_coupling_on;
        let prev_op_stencil = self.params.op_stencil;
        let prev_op_budget = self.params.op_budget_k;
        if let Some(v) = get_f32(&params, "beta") {
            if v.is_finite() && v > 0.0 {
                self.params.beta = v;
            }
        }
        if let Some(v) = get_f32(&params, "stepSize") {
            if v.is_finite() && v > 0.0 {
                self.params.step_size = v.min(0.25);
            }
        }
        if let Some(v) = get_f32(&params, "pWrite") {
            if v.is_finite() {
                self.params.p_write = v.clamp(0.0, 1.0);
            }
        }
        if let Some(v) = get_f32(&params, "pNWrite") {
            if v.is_finite() {
                self.params.p_n_write = v.clamp(0.0, 1.0);
            }
        }
        if let Some(v) = get_f32(&params, "pAWrite") {
            if v.is_finite() {
                self.params.p_a_write = v.clamp(0.0, 1.0);
            }
        }
        if let Some(v) = get_f32(&params, "pSWrite") {
            if v.is_finite() {
                self.params.p_s_write = v.clamp(0.0, 1.0);
            }
        }
        if let Some(v) = get_f32(&params, "p3On") {
            if v.is_finite() {
                self.params.p3_on = v >= 0.5;
            }
        }
        if let Some(v) = get_f32(&params, "p6On") {
            if v.is_finite() {
                self.params.p6_on = v >= 0.5;
            }
        }
        if let Some(v) = get_f32(&params, "p6SFactor") {
            if v.is_finite() {
                self.params.p6_s_factor = v.clamp(0.0, 1.0);
            }
        }
        if let Some(v) = get_f32(&params, "muHigh") {
            if v.is_finite() {
                self.params.mu_high = v;
            }
        }
        if let Some(v) = get_f32(&params, "muLow") {
            if v.is_finite() {
                self.params.mu_low = v;
            }
        }
        if self.params.p3_on != prev_p3 {
            self.phase = 0;
            self.p3_cycle_len = 0;
            self.p3_start_positions.clear();
            self.p3_obs1.clear();
            self.p3_obs2.clear();
            self.p3_disp_x = 0.0;
            self.p3_disp_y = 0.0;
            self.p3_disp_mag = 0.0;
            self.p3_loop_area = 0.0;
        }
        let sum = self.params.p_write + self.params.p_n_write + self.params.p_a_write + self.params.p_s_write;
        if sum > 1.0 {
            self.params.p_write /= sum;
            self.params.p_n_write /= sum;
            self.params.p_a_write /= sum;
            self.params.p_s_write /= sum;
        }
        if let Some(v) = get_f32(&params, "kappaRep") {
            if v.is_finite() && v >= 0.0 {
                self.params.kappa_rep = v;
            }
        }
        if let Some(v) = get_f32(&params, "r0") {
            if v.is_finite() && v >= 0.0 && v <= 0.5 {
                self.params.r0 = v;
            }
        }
        if let Some(v) = get_f32(&params, "kappaBond") {
            if v.is_finite() && v >= 0.0 {
                self.params.kappa_bond = v;
            }
        }
        if let Some(v) = get_f32(&params, "rStar") {
            if v.is_finite() && v >= 0.0 && v <= 0.5 {
                self.params.r_star = v;
            }
        }
        if let Some(v) = get_f32(&params, "lambdaW") {
            if v.is_finite() && v >= 0.0 {
                self.params.lambda_w = v;
            }
        }
        if let Some(v) = get_f32(&params, "lambdaN") {
            if v.is_finite() && v >= 0.0 {
                self.params.lambda_n = v;
            }
        }
        if let Some(v) = get_f32(&params, "lambdaA") {
            if v.is_finite() && v >= 0.0 {
                self.params.lambda_a = v;
            }
        }
        if let Some(v) = get_f32(&params, "lambdaS") {
            if v.is_finite() && v >= 0.0 {
                self.params.lambda_s = v;
            }
        }
        if let Some(v) = get_u8(&params, "lW") {
            let new_lw = v.max(1);
            self.params.l_w = new_lw;
            for w in &mut self.w {
                if *w > new_lw {
                    *w = new_lw;
                }
            }
            for w in &mut self.meta_w_edges {
                if *w > new_lw {
                    *w = new_lw;
                }
            }
            self.recompute_sum_w();
        }
        if let Some(v) = get_i16(&params, "lN") {
            let new_ln = v.max(1);
            self.params.l_n = new_ln;
            for n in &mut self.n_counter {
                if *n > new_ln {
                    *n = new_ln;
                } else if *n < -new_ln {
                    *n = -new_ln;
                }
            }
            for n in &mut self.meta_n_field {
                if *n > new_ln {
                    *n = new_ln;
                } else if *n < -new_ln {
                    *n = -new_ln;
                }
            }
        }
        if let Some(v) = get_u16(&params, "lA") {
            let new_la = v.max(1);
            self.params.l_a = new_la;
            for a in &mut self.a_counter {
                if *a > new_la {
                    *a = new_la;
                }
            }
            for a in &mut self.meta_a_field {
                if *a > new_la {
                    *a = new_la;
                }
            }
        }
        if let Some(v) = get_u8(&params, "lS") {
            let new_ls = v.max(1);
            self.params.l_s = new_ls;
            for s in &mut self.s_field {
                if *s > new_ls {
                    *s = new_ls;
                }
            }
            for s in &mut self.meta_field {
                if *s > new_ls {
                    *s = new_ls;
                }
            }
            self.recompute_sum_s();
        }
        if let Some(v) = get_u16(&params, "gridSize") {
            let new_g = v.max(2).min(256);
            if new_g != self.params.grid_size {
                self.params.grid_size = new_g;
                self.s_field = vec![0u8; (new_g as usize) * (new_g as usize)];
                self.recompute_sum_s();
            }
        }
        if let Some(v) = get_u16(&params, "metaLayers") {
            let new_layers = v.min(MAX_META_LAYERS);
            self.params.meta_layers = new_layers;
        }
        if let Some(v) = get_f32(&params, "eta") {
            if v.is_finite() {
                self.params.eta = v.clamp(0.0, 1.0);
            }
        }
        if let Some(v) = get_f32(&params, "etaDrive") {
            if v.is_finite() {
                self.params.eta_drive = v.clamp(0.0, 1.0);
            }
        }
        if let Some(v) = get_f32(&params, "opCouplingOn") {
            if v.is_finite() {
                self.params.op_coupling_on = v >= 0.5;
            }
        }
        if let Some(v) = get_u8(&params, "opStencil") {
            self.params.op_stencil = v.min(1);
        }
        if let Some(v) = get_u8(&params, "opBudgetK") {
            self.params.op_budget_k = v.max(1);
        }
        if let Some(v) = get_f32(&params, "opKTargetWeight") {
            if v.is_finite() {
                self.params.op_k_target_weight = v.clamp(0.0, 10.0);
            }
        }
        if let Some(v) = get_u8(&params, "sCouplingMode") {
            self.params.s_coupling_mode = v.min(1);
        }
        if let Some(v) = get_f32(&params, "opDriveOnK") {
            if v.is_finite() {
                self.params.op_drive_on_k = v >= 0.5;
            }
        }
        if let Some(v) = get_f32(&params, "acceptLogOn") {
            if v.is_finite() {
                self.params.accept_log_on = v >= 0.5;
            }
        }
        if let Some(v) = get_u32(&params, "acceptLogMask") {
            self.params.accept_log_mask = v;
        }
        if let Some(v) = get_u32(&params, "acceptLogCap") {
            self.params.accept_log_cap = v.clamp(1000, 2_000_000);
        }
        if let Some(v) = get_f32(&params, "epDebug") {
            if v.is_finite() {
                self.params.ep_debug = v >= 0.5;
            }
        }
        if let Some(v) = get_f32(&params, "initRandom") {
            if v.is_finite() {
                let on = v >= 0.5;
                self.params.init_random = on;
                if on {
                    self.randomize_state();
                    self.ep_naive_total = 0.0;
                    self.ep_exact_total = 0.0;
                    self.ep_naive_by_move = [0.0; MOVE_KIND_COUNT];
                    self.ep_exact_by_move = [0.0; MOVE_KIND_COUNT];
                }
            }
        }
        if let Some(v) = get_f32(&params, "codeNoiseRate") {
            if v.is_finite() {
                self.params.code_noise_rate = v.clamp(0.0, 1.0);
            }
        }
        if let Some(v) = get_u16(&params, "codeNoiseBatch") {
            let batch = v.max(1);
            self.params.code_noise_batch = batch;
        }
        if let Some(v) = get_u16(&params, "codeNoiseLayer") {
            self.params.code_noise_layer = v;
        }
        if let Some(v) = get_f32(&params, "clockOn") {
            if v.is_finite() {
                self.params.clock_on = v >= 0.5;
            }
        }
        if let Some(v) = get_u8(&params, "clockK") {
            let new_k = v.max(3);
            self.params.clock_k = new_k;
            if self.clock_state >= new_k {
                self.clock_state = 0;
            }
        }
        if let Some(v) = get_f32(&params, "clockFrac") {
            if v.is_finite() {
                self.params.clock_frac = v.clamp(0.0, 1.0);
            }
        }
        if let Some(v) = get_f32(&params, "clockUsesP6") {
            if v.is_finite() {
                self.params.clock_uses_p6 = v >= 0.5;
            }
        }
        if let Some(v) = get_f32(&params, "repairClockGated") {
            if v.is_finite() {
                self.params.repair_clock_gated = v >= 0.5;
            }
        }
        if let Some(v) = get_u8(&params, "repairGateMode") {
            self.params.repair_gate_mode = v.min(1);
        }
        if let Some(v) = get_u8(&params, "repairGateSpan") {
            self.params.repair_gate_span = v.max(1);
        }
        if self.params.s_coupling_mode > 0 && !self.params.op_coupling_on {
            self.params.s_coupling_mode = 0;
        }
        if self.params.grid_size != prev_grid_size || self.params.meta_layers != prev_meta_layers {
            self.resize_meta_arrays();
        }
        if !self.params.op_coupling_on || self.params.meta_layers == 0 {
            self.op_k.clear();
        } else if self.params.grid_size != prev_grid_size
            || self.params.meta_layers != prev_meta_layers
            || prev_op_on != self.params.op_coupling_on
            || prev_op_stencil != self.params.op_stencil
            || prev_op_budget != self.params.op_budget_k
        {
            self.init_op_k();
        }
        if let Some(v) = get_f32(&params, "rPropose") {
            if v.is_finite() && v >= 0.0 && v <= 0.5 {
                self.params.r_propose = v;
            }
        }
    }
}

impl Sim {
    fn recompute_sum_w(&mut self) {
        self.sum_w = self.w.iter().map(|w| *w as i32).sum();
    }

    fn recompute_sum_s(&mut self) {
        self.sum_s = self.s_field.iter().map(|s| *s as i32).sum();
    }

    fn resize_meta_arrays(&mut self) {
        let layers = self.params.meta_layers as usize;
        if layers == 0 {
            self.meta_field.clear();
            self.meta_n_field.clear();
            self.meta_a_field.clear();
            self.meta_w_edges.clear();
            return;
        }
        let g = self.params.grid_size as usize;
        let cells = layers * g * g;
        self.meta_field = vec![0u8; cells];
        self.meta_n_field = vec![0i16; cells];
        self.meta_a_field = vec![0u16; cells];
        self.meta_w_edges = vec![0u8; layers * meta_edge_count(g)];
    }

    fn op_offsets_internal(&self) -> &'static [(i32, i32)] {
        if self.params.op_stencil == 1 {
            &OP_STENCIL_FULL
        } else {
            &OP_STENCIL_CROSS
        }
    }

    fn op_r_count_internal(&self) -> usize {
        self.op_offsets_internal().len()
    }

    fn op_k_index(&self, interface: usize, q: usize, r_idx: usize) -> usize {
        let g = self.params.grid_size as usize;
        let cells = g * g;
        let r_count = self.op_r_count_internal();
        ((interface * cells + q) * r_count) + r_idx
    }

    fn mismatch_bin(upper: u8, lower: u8) -> u8 {
        let diff = (upper as i16) - (lower as i16);
        if diff < 0 {
            0
        } else if diff == 0 {
            1
        } else {
            2
        }
    }

    fn op_k_dir(&self, interface: usize, q: usize) -> u8 {
        if self.op_k.is_empty() {
            return 0;
        }
        if interface >= self.params.meta_layers as usize {
            return 0;
        }
        let r_count = self.op_r_count_internal();
        if r_count == 0 || r_count > u8::MAX as usize {
            // Stencil sizes should be small; clamp to 0 if out of range.
            return 0;
        }
        let mut best_idx = 0usize;
        let mut best_val = 0u8;
        for r_idx in 0..r_count {
            let idx = self.op_k_index(interface, q, r_idx);
            let val = self.op_k[idx];
            if val > best_val {
                best_val = val;
                best_idx = r_idx;
            }
        }
        best_idx as u8
    }

    fn offset_index(q: usize, dx: i32, dy: i32, g: usize) -> usize {
        let x = (q % g) as i32;
        let y = (q / g) as i32;
        let nx = (x + dx).rem_euclid(g as i32) as usize;
        let ny = (y + dy).rem_euclid(g as i32) as usize;
        ny * g + nx
    }

    fn op_lower_norm(&self, interface: usize, q: usize) -> f32 {
        let denom = self.params.l_s.max(1) as f32;
        let val = if interface == 0 {
            self.s_field[q]
        } else {
            let g = self.params.grid_size as usize;
            let cells = g * g;
            self.meta_field[(interface - 1) * cells + q]
        };
        (val as f32) / denom
    }

    fn op_upper_norm(&self, interface: usize, q: usize) -> f32 {
        let denom = self.params.l_s.max(1) as f32;
        let g = self.params.grid_size as usize;
        let cells = g * g;
        let val = self.meta_field[interface * cells + q];
        (val as f32) / denom
    }

    fn op_pred_norm(&self, interface: usize, q: usize) -> f32 {
        let g = self.params.grid_size as usize;
        let cells = g * g;
        if self.op_k.is_empty() || interface >= self.params.meta_layers as usize || q >= cells {
            return 0.0;
        }
        let budget = self.params.op_budget_k as f32;
        if budget <= 0.0 {
            return 0.0;
        }
        let mut acc = 0.0;
        for (r_idx, (dx, dy)) in self.op_offsets_internal().iter().enumerate() {
            let q_off = Self::offset_index(q, *dx, *dy, g);
            let k_idx = self.op_k_index(interface, q, r_idx);
            let weight = (self.op_k[k_idx] as f32) / budget;
            acc += weight * self.op_lower_norm(interface, q_off);
        }
        acc
    }

    fn delta_raw_s_op(&self, level: usize, idx: usize, s0: u8, s1: u8) -> f32 {
        if !self.params.op_coupling_on || self.params.s_coupling_mode == 0 {
            return 0.0;
        }
        let layers = self.params.meta_layers as usize;
        if layers == 0 {
            return 0.0;
        }
        let g = self.params.grid_size as usize;
        if g == 0 {
            return 0.0;
        }
        let denom = self.params.l_s.max(1) as f32;
        let s0n = (s0 as f32) / denom;
        let s1n = (s1 as f32) / denom;
        let mut delta = 0.0;
        if level > 0 {
            let interface = level - 1;
            if interface < layers {
                let pred = self.op_pred_norm(interface, idx);
                delta += 0.5 * ((s1n - pred).powi(2) - (s0n - pred).powi(2));
            }
        }
        if level < layers {
            let interface = level;
            for (r_idx, (dx, dy)) in self.op_offsets_internal().iter().enumerate() {
                let q_prime = Self::offset_index(idx, -*dx, -*dy, g);
                let pred_old = self.op_pred_norm(interface, q_prime);
                let upper = self.op_upper_norm(interface, q_prime);
                let k_idx = self.op_k_index(interface, q_prime, r_idx);
                let weight = (self.op_k[k_idx] as f32) / (self.params.op_budget_k as f32);
                let pred_new = pred_old + weight * (s1n - s0n);
                delta += 0.5 * ((upper - pred_new).powi(2) - (upper - pred_old).powi(2));
            }
        }
        delta
    }

    fn delta_raw_k_op(&self, interface: usize, q: usize, r_from: usize, r_to: usize) -> f32 {
        if !self.params.op_coupling_on || self.params.s_coupling_mode == 0 {
            return 0.0;
        }
        let layers = self.params.meta_layers as usize;
        if layers == 0 || interface >= layers {
            return 0.0;
        }
        let g = self.params.grid_size as usize;
        if g == 0 {
            return 0.0;
        }
        let budget = self.params.op_budget_k as f32;
        if budget <= 0.0 {
            return 0.0;
        }
        let pred_old = self.op_pred_norm(interface, q);
        let upper = self.op_upper_norm(interface, q);
        let offsets = self.op_offsets_internal();
        let (dx_from, dy_from) = offsets[r_from];
        let (dx_to, dy_to) = offsets[r_to];
        let lower_from = self.op_lower_norm(interface, Self::offset_index(q, dx_from, dy_from, g));
        let lower_to = self.op_lower_norm(interface, Self::offset_index(q, dx_to, dy_to, g));
        let pred_new = pred_old + (lower_to - lower_from) / budget;
        0.5 * ((upper - pred_new).powi(2) - (upper - pred_old).powi(2))
    }

    fn init_op_k(&mut self) {
        if !self.params.op_coupling_on || self.params.meta_layers == 0 {
            self.op_k.clear();
            return;
        }
        let layers = self.params.meta_layers as usize;
        let g = self.params.grid_size as usize;
        let cells = g * g;
        let r_count = self.op_r_count_internal();
        let budget = self.params.op_budget_k;
        if r_count == 0 || budget == 0 {
            self.op_k.clear();
            return;
        }
        let base = budget / (r_count as u8);
        let rem = (budget % (r_count as u8)) as usize;
        self.op_k = vec![0u8; layers * cells * r_count];
        for interface in 0..layers {
            for q in 0..cells {
                let start = (interface * cells + q) * r_count;
                for r in 0..r_count {
                    let mut val = base;
                    if r < rem {
                        val = val.saturating_add(1);
                    }
                    self.op_k[start + r] = val;
                }
            }
        }
    }

    fn randomize_state(&mut self) {
        let mut rng = self.rng;
        let mut next_u32 = || {
            let mut x = rng;
            x ^= x << 13;
            x ^= x >> 17;
            x ^= x << 5;
            rng = x;
            x
        };
        let mut rand01 = || {
            let u = next_u32() >> 8;
            (u as f64) / ((1u32 << 24) as f64)
        };

        let beta = self.params.beta as f64;
        let l_w = self.params.l_w as usize;
        let l_s = self.params.l_s as usize;
        let l_a = self.params.l_a as usize;
        let l_n = self.params.l_n.max(1) as i32;

        let weights_s: Vec<f64> = (0..=l_s)
            .map(|v| (-0.5 * beta * (self.params.lambda_s as f64) * (v as f64).powi(2)).exp())
            .collect();
        let weights_a: Vec<f64> = (0..=l_a)
            .map(|v| (-0.5 * beta * (self.params.lambda_a as f64) * (v as f64).powi(2)).exp())
            .collect();
        let weights_n: Vec<f64> = (-l_n..=l_n)
            .map(|v| (-0.5 * beta * (self.params.lambda_n as f64) * (v as f64).powi(2)).exp())
            .collect();
        let weights_w_base: Vec<f64> = (0..=l_w)
            .map(|v| (-0.5 * beta * (self.params.lambda_w as f64) * (v as f64).powi(2)).exp())
            .collect();

        let sample_index = |weights: &[f64], r: f64| -> usize {
            let total: f64 = weights.iter().sum();
            if total <= 0.0 {
                return 0;
            }
            let mut acc = 0.0;
            let target = r * total;
            for (idx, w) in weights.iter().enumerate() {
                acc += *w;
                if acc >= target {
                    return idx;
                }
            }
            weights.len().saturating_sub(1)
        };

        if self.n > 1 {
            let mut idx = 0usize;
            for i in 0..self.n {
                for j in (i + 1)..self.n {
                    let r = torus_dist(
                        self.positions[2 * i],
                        self.positions[2 * i + 1],
                        self.positions[2 * j],
                        self.positions[2 * j + 1],
                    );
                    let bond_shape =
                        0.5 * (self.params.kappa_bond as f64) * (r - self.params.r_star).powi(2) as f64;
                    let weights = if bond_shape > 0.0 {
                        let mut local = Vec::with_capacity(l_w + 1);
                        for v in 0..=l_w {
                            let vf = v as f64;
                            let e = 0.5 * beta * (self.params.lambda_w as f64) * vf * vf
                                + beta * bond_shape * vf;
                            local.push((-e).exp());
                        }
                        local
                    } else {
                        weights_w_base.clone()
                    };
                    self.w[idx] = sample_index(&weights, rand01()) as u8;
                    idx += 1;
                }
            }
        }
        for n in &mut self.n_counter {
            let idx = sample_index(&weights_n, rand01());
            *n = (idx as i16) - (l_n as i16);
        }
        for a in &mut self.a_counter {
            *a = sample_index(&weights_a, rand01()) as u16;
        }
        for s in &mut self.s_field {
            *s = sample_index(&weights_s, rand01()) as u8;
        }
        if self.params.meta_layers > 0 {
            for s in &mut self.meta_field {
                *s = sample_index(&weights_s, rand01()) as u8;
            }
            for n in &mut self.meta_n_field {
                let idx = sample_index(&weights_n, rand01());
                *n = (idx as i16) - (l_n as i16);
            }
            for a in &mut self.meta_a_field {
                *a = sample_index(&weights_a, rand01()) as u16;
            }
            for w in &mut self.meta_w_edges {
                *w = sample_index(&weights_w_base, rand01()) as u8;
            }
        }
        if self.params.op_coupling_on && self.params.meta_layers > 0 {
            self.init_op_k();
        } else {
            self.op_k.clear();
        }
        self.rng = rng;
        self.recompute_sum_w();
        self.recompute_sum_s();
    }

    fn pick_write_target(&mut self) -> usize {
        let total = (self.params.meta_layers as u32) + 1;
        (self.rand_u32() % total) as usize
    }

    fn pick_p5_target_op(&mut self) -> usize {
        let layers = self.params.meta_layers as u32;
        let weight = self.params.op_k_target_weight;
        if (weight - 1.0).abs() < 1e-6 {
            let total = 1 + (2 * layers);
            (self.rand_u32() % total) as usize
        } else {
            let layers_usize = layers as usize;
            if layers_usize == 0 {
                return 0;
            }
            let s_weight = (layers_usize + 1) as f32;
            let op_weight = (layers_usize as f32) * weight.max(0.0);
            let total = s_weight + op_weight;
            if total <= 0.0 {
                return 0;
            }
            let r = self.rand01() * total;
            if r < s_weight {
                (self.rand_u32() % (layers + 1)) as usize
            } else {
                let idx = (self.rand_u32() % layers) as usize;
                (layers_usize + 1) + idx
            }
        }
    }

    fn rand_u32(&mut self) -> u32 {
        // xorshift32
        let mut x = self.rng;
        x ^= x << 13;
        x ^= x >> 17;
        x ^= x << 5;
        self.rng = x;
        x
    }

    fn rand01(&mut self) -> f32 {
        // Map to [0,1) using 24 high bits for stable float conversion.
        let u = self.rand_u32() >> 8;
        (u as f32) / ((1u32 << 24) as f32)
    }

    fn clock_step(&mut self) -> bool {
        let k = self.params.clock_k.max(3);
        let up = self.rand01() < 0.5;
        let c0 = self.clock_state;
        let c1 = if up {
            if c0 + 1 >= k {
                0
            } else {
                c0 + 1
            }
        } else if c0 == 0 {
            k - 1
        } else {
            c0 - 1
        };
        let work = if self.params.p6_on && self.params.clock_uses_p6 {
            let mu = self.params.mu_high;
            if up { mu } else { -mu }
        } else {
            0.0
        };
        if self.accept_move(0.0, work, 0.0, MOVE_CLOCK) {
            self.clock_state = c1;
            if up {
                self.clock_q += 1;
                self.clock_fwd = self.clock_fwd.saturating_add(1);
            } else {
                self.clock_q -= 1;
                self.clock_bwd = self.clock_bwd.saturating_add(1);
            }
            return true;
        }
        false
    }

    fn clock_gate_allows(&self, x: f32, y: f32) -> bool {
        if !self.params.repair_clock_gated {
            return true;
        }
        let active = if self.params.clock_on {
            self.clock_state
        } else {
            0
        };
        match self.params.repair_gate_mode {
            1 => {
                let bins = self.params.clock_k.max(1) as usize;
                let idx = ((x * (bins as f32)).floor() as usize).min(bins - 1);
                let active_bin = (active as usize) % bins;
                let span = self.params.repair_gate_span.max(1) as usize;
                let span = span.min(bins);
                for i in 0..span {
                    if (active_bin + i) % bins == idx {
                        return true;
                    }
                }
                false
            }
            _ => {
                let qx = if x < 0.5 { 0 } else { 1 };
                let qy = if y < 0.5 { 0 } else { 1 };
                let quadrant = (qy * 2 + qx) as u8;
                quadrant == (active % 4)
            }
        }
    }

    fn x_move_step(&mut self) {
        if self.n <= 1 {
            return;
        }
        let i = (self.rand_u32() as usize) % self.n;
        let x0 = self.positions[2 * i];
        let y0 = self.positions[2 * i + 1];

        // Symmetric proposal on the torus (small box step).
        let dx = (self.rand01() - 0.5) * 2.0 * self.params.step_size;
        let dy = (self.rand01() - 0.5) * 2.0 * self.params.step_size;
        let x1 = wrap01(x0 + dx);
        let y1 = wrap01(y0 + dy);

        let d_e = self.delta_e_move_particle(i, x0, y0, x1, y1);
        if self.accept_move(d_e, 0.0, 0.0, MOVE_X) {
            self.positions[2 * i] = x1;
            self.positions[2 * i + 1] = y1;
        }
    }

    fn p1_write_step(&mut self) -> i8 {
        let m = self.w.len();
        if m == 0 {
            return 0;
        }

        // Choose a neighbor pair uniformly among those within r_propose.
        let mut chosen: Option<(usize, usize)> = None;
        let mut count = 0u32;
        for i in 0..self.n {
            for j in (i + 1)..self.n {
                let r = torus_dist(
                    self.positions[2 * i],
                    self.positions[2 * i + 1],
                    self.positions[2 * j],
                    self.positions[2 * j + 1],
                );
                if r <= self.params.r_propose {
                    count += 1;
                    if self.rand01() < 1.0 / (count as f32) {
                        chosen = Some((i, j));
                    }
                }
            }
        }

        let (i, j) = match chosen {
            Some(pair) => pair,
            None => return 0,
        };
        let idx = edge_index(self.n, i, j);
        let w0 = self.w[idx];

        let up = self.rand01() < 0.5;
        let w1 = if up {
            if w0 >= self.params.l_w {
                return 0;
            }
            w0 + 1
        } else {
            if w0 == 0 {
                return 0;
            }
            w0 - 1
        };

        let r = torus_dist(
            self.positions[2 * i],
            self.positions[2 * i + 1],
            self.positions[2 * j],
            self.positions[2 * j + 1],
        );
        let d_e = self.delta_e_write(w0, w1, r);
        let (work, high_ctx) = if self.params.p6_on {
            let (mx, my) = torus_midpoint(
                self.positions[2 * i],
                self.positions[2 * i + 1],
                self.positions[2 * j],
                self.positions[2 * j + 1],
            );
            let mu = self.mu_at(mx, my);
            (if up { mu } else { -mu }, mx < 0.5)
        } else {
            (0.0, false)
        };
        if self.accept_move(d_e, work, 0.0, MOVE_P1_BASE) {
            self.w[idx] = w1;
            self.sum_w += if up { 1 } else { -1 };
            if self.params.p6_on {
                if high_ctx {
                    if up {
                        self.diag.w_plus_h = self.diag.w_plus_h.saturating_add(1);
                    } else {
                        self.diag.w_minus_h = self.diag.w_minus_h.saturating_add(1);
                    }
                } else if up {
                    self.diag.w_plus_l = self.diag.w_plus_l.saturating_add(1);
                } else {
                    self.diag.w_minus_l = self.diag.w_minus_l.saturating_add(1);
                }
            }
            return if up { 1 } else { -1 };
        }
        0
    }

    fn p4_write_step(&mut self) -> i8 {
        if self.params.clock_on && self.params.clock_frac > 0.0 {
            if self.rand01() < self.params.clock_frac {
                self.clock_step();
                return 0;
            }
        }
        if self.n == 0 {
            return 0;
        }
        let k = (self.rand_u32() as usize) % self.n;
        let n0 = self.n_counter[k];
        let up = self.rand01() < 0.5;
        let n1 = if up {
            if n0 >= self.params.l_n {
                return 0;
            }
            n0 + 1
        } else {
            if n0 <= -self.params.l_n {
                return 0;
            }
            n0 - 1
        };
        let d_e = self.delta_e_counter(n0, n1);
        let (work, high_ctx) = if self.params.p6_on {
            let x = self.positions[2 * k];
            let y = self.positions[2 * k + 1];
            let mu = self.mu_at(x, y);
            (if up { mu } else { -mu }, x < 0.5)
        } else {
            (0.0, false)
        };
        if self.accept_move(d_e, work, 0.0, MOVE_P4_BASE) {
            self.n_counter[k] = n1;
            if self.params.p6_on {
                if high_ctx {
                    if up {
                        self.diag.n_plus_h = self.diag.n_plus_h.saturating_add(1);
                    } else {
                        self.diag.n_minus_h = self.diag.n_minus_h.saturating_add(1);
                    }
                } else if up {
                    self.diag.n_plus_l = self.diag.n_plus_l.saturating_add(1);
                } else {
                    self.diag.n_minus_l = self.diag.n_minus_l.saturating_add(1);
                }
            }
            return if up { 1 } else { -1 };
        }
        0
    }

    fn p2_write_step(&mut self) -> i8 {
        if self.n == 0 {
            return 0;
        }
        let k = (self.rand_u32() as usize) % self.n;
        let a0 = self.a_counter[k];
        let up = self.rand01() < 0.5;
        let a1 = if up {
            if a0 >= self.params.l_a {
                return 0;
            }
            a0 + 1
        } else {
            if a0 == 0 {
                return 0;
            }
            a0 - 1
        };
        let d_e = self.delta_e_apparatus(a0, a1);
        let (work, high_ctx) = if self.params.p6_on {
            let x = self.positions[2 * k];
            let y = self.positions[2 * k + 1];
            let mu = self.mu_at(x, y);
            (if up { mu } else { -mu }, x < 0.5)
        } else {
            (0.0, false)
        };
        if self.accept_move(d_e, work, 0.0, MOVE_P2_BASE) {
            self.a_counter[k] = a1;
            if self.params.p6_on {
                if high_ctx {
                    if up {
                        self.diag.a_plus_h = self.diag.a_plus_h.saturating_add(1);
                    } else {
                        self.diag.a_minus_h = self.diag.a_minus_h.saturating_add(1);
                    }
                } else if up {
                    self.diag.a_plus_l = self.diag.a_plus_l.saturating_add(1);
                } else {
                    self.diag.a_minus_l = self.diag.a_minus_l.saturating_add(1);
                }
            }
            return if up { 1 } else { -1 };
        }
        0
    }

    fn p5_write_step(&mut self) -> i8 {
        let g = self.params.grid_size as usize;
        if g == 0 {
            return 0;
        }
        let idx = (self.rand_u32() as usize) % (g * g);
        let s0 = self.s_field[idx];
        let mismatch_bin = if self.params.meta_layers > 0 {
            let lower = self.meta_field[idx];
            Self::mismatch_bin(s0, lower)
        } else {
            1
        };
        let k_dir = self.op_k_dir(0, idx);
        let g_f = self.params.grid_size as f32;
        let x = ((idx % g as usize) as f32 + 0.5) / g_f;
        let y = ((idx / g as usize) as f32 + 0.5) / g_f;
        let up = self.rand01() < 0.5;
        let s1 = if up {
            if s0 >= self.params.l_s {
                return 0;
            }
            s0 + 1
        } else {
            if s0 == 0 {
                return 0;
            }
            s0 - 1
        };
        let mut d_e = self.delta_e_field(s0, s1);
        if self.params.s_coupling_mode == 0 {
            d_e += self.delta_e_s_couple_level(0, idx, s0, s1);
        } else {
            d_e += self.params.eta * self.delta_raw_s_op(0, idx, s0, s1);
        }
        let (work, high_ctx) = if self.params.p6_on {
            let mu = self.mu_at(x, y);
            let scaled = mu * self.params.p6_s_factor;
            (if up { scaled } else { -scaled }, x < 0.5)
        } else {
            (0.0, false)
        };
        let align_work = self.drive_align_work(0, idx, s0, s1, x, y);
        let ep_before = self.ep_exact_total;
        if self.accept_move(d_e, work + align_work, 0.0, MOVE_P5_BASE) {
            self.s_field[idx] = s1;
            self.sum_s += if up { 1 } else { -1 };
            let ep_delta = self.ep_exact_total - ep_before;
            self.accept_log_push(
                self.step_count,
                idx as u32,
                MOVE_P5_BASE as u8,
                255,
                mismatch_bin,
                k_dir,
                ep_delta,
            );
            if self.params.p6_on {
                if high_ctx {
                    if up {
                        self.diag.s_plus_h = self.diag.s_plus_h.saturating_add(1);
                    } else {
                        self.diag.s_minus_h = self.diag.s_minus_h.saturating_add(1);
                    }
                } else if up {
                    self.diag.s_plus_l = self.diag.s_plus_l.saturating_add(1);
                } else {
                    self.diag.s_minus_l = self.diag.s_minus_l.saturating_add(1);
                }
            }
            return if up { 1 } else { -1 };
        }
        0
    }

    fn p5_write_step_meta(&mut self, layer: usize) -> i8 {
        let g = self.params.grid_size as usize;
        if g == 0 || layer >= self.params.meta_layers as usize {
            return 0;
        }
        let cells = g * g;
        let base = layer * cells;
        let idx_local = (self.rand_u32() as usize) % cells;
        let idx = base + idx_local;
        let s0 = self.meta_field[idx];
        let lower = if layer == 0 {
            self.s_field[idx_local]
        } else {
            self.meta_field[(layer - 1) * cells + idx_local]
        };
        let mismatch_bin = Self::mismatch_bin(s0, lower);
        let k_dir = self.op_k_dir(layer, idx_local);
        let (x, y) = grid_cell_center(idx_local, g);
        // When gated, only allow P5 updates in the active quadrant.
        if self.params.repair_clock_gated && !self.clock_gate_allows(x, y) {
            return 0;
        }
        let up = self.rand01() < 0.5;
        let s1 = if up {
            if s0 >= self.params.l_s {
                return 0;
            }
            s0 + 1
        } else {
            if s0 == 0 {
                return 0;
            }
            s0 - 1
        };
        let mut d_e = self.delta_e_field(s0, s1);
        if self.params.s_coupling_mode == 0 {
            d_e += self.delta_e_s_couple_level(layer + 1, idx_local, s0, s1);
        } else {
            d_e += self.params.eta * self.delta_raw_s_op(layer + 1, idx_local, s0, s1);
        }
        let (work, high_ctx) = if self.params.p6_on {
            let mu = self.mu_at(x, y);
            let scaled = mu * self.params.p6_s_factor;
            (if up { scaled } else { -scaled }, x < 0.5)
        } else {
            (0.0, false)
        };
        let align_work = self.drive_align_work(layer + 1, idx_local, s0, s1, x, y);
        let ep_before = self.ep_exact_total;
        if self.accept_move(d_e, work + align_work, 0.0, MOVE_P5_META) {
            self.meta_field[idx] = s1;
            let ep_delta = self.ep_exact_total - ep_before;
            self.accept_log_push(
                self.step_count,
                idx_local as u32,
                MOVE_P5_META as u8,
                layer as u8,
                mismatch_bin,
                k_dir,
                ep_delta,
            );
            if self.params.p6_on {
                if high_ctx {
                    if up {
                        self.diag.s_plus_h = self.diag.s_plus_h.saturating_add(1);
                    } else {
                        self.diag.s_minus_h = self.diag.s_minus_h.saturating_add(1);
                    }
                } else if up {
                    self.diag.s_plus_l = self.diag.s_plus_l.saturating_add(1);
                } else {
                    self.diag.s_minus_l = self.diag.s_minus_l.saturating_add(1);
                }
            }
            return if up { 1 } else { -1 };
        }
        0
    }

    fn p5_write_step_opk(&mut self, interface: usize) -> i8 {
        if !self.params.op_coupling_on {
            return 0;
        }
        let layers = self.params.meta_layers as usize;
        if layers == 0 || interface >= layers {
            return 0;
        }
        let g = self.params.grid_size as usize;
        if g == 0 {
            return 0;
        }
        let r_count = self.op_r_count_internal();
        if r_count < 2 || self.op_k.is_empty() {
            return 0;
        }
        let cells = g * g;
        let q = (self.rand_u32() as usize) % cells;
        let r_from = (self.rand_u32() as usize) % r_count;
        let mut r_to = (self.rand_u32() as usize) % (r_count - 1);
        if r_to >= r_from {
            r_to += 1;
        }
        let idx_from = self.op_k_index(interface, q, r_from);
        if self.op_k[idx_from] == 0 {
            return 0;
        }
        let idx_to = self.op_k_index(interface, q, r_to);
        let delta_raw = self.delta_raw_k_op(interface, q, r_from, r_to);
        let mut d_e = 0.0;
        if self.params.s_coupling_mode == 1 {
            d_e = self.params.eta * delta_raw;
        }
        let mut work = 0.0;
        if self.params.p6_on && self.params.op_drive_on_k && self.params.s_coupling_mode == 1 {
            let (x, y) = grid_cell_center(q, g);
            let mu_scale = self.mu_at(x, y).abs();
            let scale = self.params.l_s.max(1) as f32;
            work = -self.params.eta_drive * delta_raw * scale * scale * mu_scale;
        }
        let ep_before = self.ep_exact_total;
        if self.accept_move(d_e, work, 0.0, MOVE_OPK) {
            self.op_k[idx_from] = self.op_k[idx_from].saturating_sub(1);
            self.op_k[idx_to] = self.op_k[idx_to].saturating_add(1);
            let ep_delta = self.ep_exact_total - ep_before;
            self.accept_log_push(
                self.step_count,
                q as u32,
                MOVE_OPK as u8,
                0,
                r_from as u8,
                r_to as u8,
                ep_delta,
            );
        }
        0
    }

    fn p4_write_step_meta(&mut self, layer: usize) -> i8 {
        let g = self.params.grid_size as usize;
        if self.params.clock_on && self.params.clock_frac > 0.0 {
            if self.rand01() < self.params.clock_frac {
                self.clock_step();
                return 0;
            }
        }
        if g == 0 || layer >= self.params.meta_layers as usize {
            return 0;
        }
        let cells = g * g;
        let base = layer * cells;
        let idx_local = (self.rand_u32() as usize) % cells;
        let idx = base + idx_local;
        let n0 = self.meta_n_field[idx];
        let up = self.rand01() < 0.5;
        let n1 = if up {
            if n0 >= self.params.l_n {
                return 0;
            }
            n0 + 1
        } else {
            if n0 <= -self.params.l_n {
                return 0;
            }
            n0 - 1
        };
        let mut d_e = self.delta_e_counter(n0, n1);
        d_e += self.delta_e_meta_n_couple(layer, idx_local, n0, n1);
        let (work, high_ctx) = if self.params.p6_on {
            let (x, y) = grid_cell_center(idx_local, g);
            let mu = self.mu_at(x, y);
            (if up { mu } else { -mu }, x < 0.5)
        } else {
            (0.0, false)
        };
        if self.accept_move(d_e, work, 0.0, MOVE_P4_META) {
            self.meta_n_field[idx] = n1;
            if self.params.p6_on {
                if high_ctx {
                    if up {
                        self.diag.n_plus_h = self.diag.n_plus_h.saturating_add(1);
                    } else {
                        self.diag.n_minus_h = self.diag.n_minus_h.saturating_add(1);
                    }
                } else if up {
                    self.diag.n_plus_l = self.diag.n_plus_l.saturating_add(1);
                } else {
                    self.diag.n_minus_l = self.diag.n_minus_l.saturating_add(1);
                }
            }
            return if up { 1 } else { -1 };
        }
        0
    }

    fn p2_write_step_meta(&mut self, layer: usize) -> i8 {
        let g = self.params.grid_size as usize;
        if g == 0 || layer >= self.params.meta_layers as usize {
            return 0;
        }
        let cells = g * g;
        let base = layer * cells;
        let idx_local = (self.rand_u32() as usize) % cells;
        let idx = base + idx_local;
        let a0 = self.meta_a_field[idx];
        let up = self.rand01() < 0.5;
        let a1 = if up {
            if a0 >= self.params.l_a {
                return 0;
            }
            a0 + 1
        } else {
            if a0 == 0 {
                return 0;
            }
            a0 - 1
        };
        let mut d_e = self.delta_e_apparatus(a0, a1);
        d_e += self.delta_e_meta_a_couple(layer, idx_local, a0, a1);
        let (work, high_ctx) = if self.params.p6_on {
            let (x, y) = grid_cell_center(idx_local, g);
            let mu = self.mu_at(x, y);
            (if up { mu } else { -mu }, x < 0.5)
        } else {
            (0.0, false)
        };
        if self.accept_move(d_e, work, 0.0, MOVE_P2_META) {
            self.meta_a_field[idx] = a1;
            if self.params.p6_on {
                if high_ctx {
                    if up {
                        self.diag.a_plus_h = self.diag.a_plus_h.saturating_add(1);
                    } else {
                        self.diag.a_minus_h = self.diag.a_minus_h.saturating_add(1);
                    }
                } else if up {
                    self.diag.a_plus_l = self.diag.a_plus_l.saturating_add(1);
                } else {
                    self.diag.a_minus_l = self.diag.a_minus_l.saturating_add(1);
                }
            }
            return if up { 1 } else { -1 };
        }
        0
    }

    fn p1_write_step_meta(&mut self, layer: usize) -> i8 {
        let g = self.params.grid_size as usize;
        if g == 0 || layer >= self.params.meta_layers as usize {
            return 0;
        }
        let edges_per_layer = meta_edge_count(g);
        if edges_per_layer == 0 {
            return 0;
        }
        let base = layer * edges_per_layer;
        let edge = (self.rand_u32() as usize) % edges_per_layer;
        let idx = base + edge;
        let w0 = self.meta_w_edges[idx];
        let up = self.rand01() < 0.5;
        let w1 = if up {
            if w0 >= self.params.l_w {
                return 0;
            }
            w0 + 1
        } else {
            if w0 == 0 {
                return 0;
            }
            w0 - 1
        };
        let w0f = w0 as f32;
        let w1f = w1 as f32;
        let mut d_e = 0.5 * self.params.lambda_w * (w1f * w1f - w0f * w0f);
        d_e += self.delta_e_meta_w_couple(layer, edge, w0, w1);
        let (work, high_ctx) = if self.params.p6_on {
            let (mx, my) = meta_edge_midpoint(edge, g);
            let mu = self.mu_at(mx, my);
            (if up { mu } else { -mu }, mx < 0.5)
        } else {
            (0.0, false)
        };
        if self.accept_move(d_e, work, 0.0, MOVE_P1_META) {
            self.meta_w_edges[idx] = w1;
            if self.params.p6_on {
                if high_ctx {
                    if up {
                        self.diag.w_plus_h = self.diag.w_plus_h.saturating_add(1);
                    } else {
                        self.diag.w_minus_h = self.diag.w_minus_h.saturating_add(1);
                    }
                } else if up {
                    self.diag.w_plus_l = self.diag.w_plus_l.saturating_add(1);
                } else {
                    self.diag.w_minus_l = self.diag.w_minus_l.saturating_add(1);
                }
            }
            return if up { 1 } else { -1 };
        }
        0
    }

    fn protocol_step(&mut self, step_diag: &mut StepDiag) {
        let mut kernels = [0u8; 5];
        let mut len = 0usize;
        kernels[len] = 0; // X
        len += 1;
        if self.params.p_write > 0.0 {
            kernels[len] = 1;
            len += 1;
        }
        if self.params.p_a_write > 0.0 {
            kernels[len] = 2;
            len += 1;
        }
        if self.params.p_n_write > 0.0 {
            kernels[len] = 3;
            len += 1;
        }
        if self.params.p_s_write > 0.0 {
            kernels[len] = 4;
            len += 1;
        }
        if len == 0 {
            return;
        }
        if self.p3_cycle_len != len as u8 {
            self.p3_cycle_len = len as u8;
            self.phase = 0;
            self.p3_obs1 = vec![0.0; len];
            self.p3_obs2 = vec![0.0; len];
            self.p3_start_positions = self.positions.clone();
        }
        if self.phase == 0 && self.p3_start_positions.len() != self.positions.len() {
            self.p3_start_positions = self.positions.clone();
        }
        let idx = (self.phase as usize) % len;
        match kernels[idx] {
            0 => self.x_move_step(),
            1 => {
                let delta = if self.params.meta_layers == 0 {
                    self.p1_write_step()
                } else {
                    let target = self.pick_write_target();
                    if target == 0 {
                        self.p1_write_step()
                    } else {
                        self.p1_write_step_meta((target - 1) as usize)
                    }
                };
                if delta > 0 {
                    step_diag.w_plus = 1;
                } else if delta < 0 {
                    step_diag.w_minus = 1;
                }
            }
            2 => {
                let delta = if self.params.meta_layers == 0 {
                    self.p2_write_step()
                } else {
                    let target = self.pick_write_target();
                    if target == 0 {
                        self.p2_write_step()
                    } else {
                        self.p2_write_step_meta((target - 1) as usize)
                    }
                };
                if delta > 0 {
                    step_diag.a_plus = 1;
                } else if delta < 0 {
                    step_diag.a_minus = 1;
                }
            }
            3 => {
                let delta = if self.params.meta_layers == 0 {
                    self.p4_write_step()
                } else {
                    let target = self.pick_write_target();
                    if target == 0 {
                        self.p4_write_step()
                    } else {
                        self.p4_write_step_meta((target - 1) as usize)
                    }
                };
                if delta > 0 {
                    step_diag.n_plus = 1;
                } else if delta < 0 {
                    step_diag.n_minus = 1;
                }
            }
            4 => {
                let delta = if self.params.meta_layers == 0 {
                    self.p5_write_step()
                } else if self.params.op_coupling_on {
                    let target = self.pick_p5_target_op();
                    let layers = self.params.meta_layers as usize;
                    if target == 0 {
                        self.p5_write_step()
                    } else if target <= layers {
                        self.p5_write_step_meta((target - 1) as usize)
                    } else {
                        self.p5_write_step_opk(target - (layers + 1))
                    }
                } else {
                    let target = self.pick_write_target();
                    if target == 0 {
                        self.p5_write_step()
                    } else {
                        self.p5_write_step_meta((target - 1) as usize)
                    }
                };
                if delta > 0 {
                    step_diag.s_plus = 1;
                } else if delta < 0 {
                    step_diag.s_minus = 1;
                }
            }
            _ => {}
        }
        if idx < self.p3_obs1.len() {
            self.p3_obs1[idx] = self.sum_w as f32;
            self.p3_obs2[idx] = self.sum_s as f32;
        }
        self.phase = self.phase.wrapping_add(1);
        if self.phase as usize >= len {
            self.phase = 0;
            self.update_p3_cycle_diagnostics();
            self.p3_start_positions = self.positions.clone();
        }
    }

    fn update_p3_cycle_diagnostics(&mut self) {
        if self.p3_start_positions.len() != self.positions.len() || self.n == 0 {
            return;
        }
        let mut dx = 0.0f32;
        let mut dy = 0.0f32;
        for i in 0..self.n {
            let x0 = self.p3_start_positions[2 * i];
            let y0 = self.p3_start_positions[2 * i + 1];
            let x1 = self.positions[2 * i];
            let y1 = self.positions[2 * i + 1];
            let (ddx, ddy) = torus_delta(x0, y0, x1, y1);
            dx += ddx;
            dy += ddy;
        }
        let n_inv = 1.0 / (self.n as f32);
        self.p3_disp_x = dx * n_inv;
        self.p3_disp_y = dy * n_inv;
        self.p3_disp_mag = (self.p3_disp_x * self.p3_disp_x + self.p3_disp_y * self.p3_disp_y).sqrt();

        let len = self.p3_obs1.len();
        if len >= 2 {
            let mut area = 0.0f32;
            for i in 0..len {
                let j = (i + 1) % len;
                area += self.p3_obs1[i] * self.p3_obs2[j] - self.p3_obs1[j] * self.p3_obs2[i];
            }
            self.p3_loop_area = 0.5 * area;
        }
    }

    fn accept_move(&mut self, delta_e: f32, work: f32, log_q_ratio: f32, move_kind: usize) -> bool {
        let effective = delta_e - work;
        let log_a_ratio = -self.params.beta * effective;
        let accepted = if effective <= 0.0 {
            true
        } else {
            let a = log_a_ratio.exp();
            self.rand01() < a.min(1.0)
        };
        if accepted {
            self.ep_naive_total += log_a_ratio as f64;
            self.ep_exact_total += (log_a_ratio + log_q_ratio) as f64;
            if move_kind < MOVE_KIND_COUNT {
                self.ep_naive_by_move[move_kind] += log_a_ratio as f64;
                self.ep_exact_by_move[move_kind] += (log_a_ratio + log_q_ratio) as f64;
            }
            if self.params.ep_debug {
                self.ep_q_stats[move_kind].record(log_q_ratio);
            }
        }
        accepted
    }

    fn maybe_code_noise(&mut self) {
        if self.params.code_noise_rate <= 0.0 {
            return;
        }
        let layers = self.params.meta_layers as usize;
        if layers == 0 {
            return;
        }
        if self.rand01() >= self.params.code_noise_rate {
            return;
        }
        let g = self.params.grid_size as usize;
        if g == 0 {
            return;
        }
        let cells = g * g;
        let layer = self.params.code_noise_layer as usize;
        if layer >= layers {
            return;
        }
        let base = layer * cells;
        let max_val = self.params.l_s as u32 + 1;
        let batch = self.params.code_noise_batch.max(1) as usize;
        for _ in 0..batch {
            let idx = base + ((self.rand_u32() as usize) % cells);
            let val = if max_val == 0 {
                0
            } else {
                (self.rand_u32() % max_val) as u8
            };
            self.meta_field[idx] = val;
        }
    }

    fn delta_e_write(&self, w0: u8, w1: u8, r: f32) -> f32 {
        let w0f = w0 as f32;
        let w1f = w1 as f32;
        let e_w0 = 0.5 * self.params.lambda_w * w0f * w0f;
        let e_w1 = 0.5 * self.params.lambda_w * w1f * w1f;
        let bond_shape = 0.5 * self.params.kappa_bond * (r - self.params.r_star).powi(2);
        (e_w1 - e_w0) + bond_shape * (w1f - w0f)
    }

    fn delta_e_counter(&self, n0: i16, n1: i16) -> f32 {
        let n0f = n0 as f32;
        let n1f = n1 as f32;
        let e0 = 0.5 * self.params.lambda_n * n0f * n0f;
        let e1 = 0.5 * self.params.lambda_n * n1f * n1f;
        e1 - e0
    }

    fn delta_e_apparatus(&self, a0: u16, a1: u16) -> f32 {
        let a0f = a0 as f32;
        let a1f = a1 as f32;
        let e0 = 0.5 * self.params.lambda_a * a0f * a0f;
        let e1 = 0.5 * self.params.lambda_a * a1f * a1f;
        e1 - e0
    }

    fn delta_e_field(&self, s0: u8, s1: u8) -> f32 {
        let s0f = s0 as f32;
        let s1f = s1 as f32;
        let e0 = 0.5 * self.params.lambda_s * s0f * s0f;
        let e1 = 0.5 * self.params.lambda_s * s1f * s1f;
        e1 - e0
    }

    fn delta_e_s_couple_level(&self, level: usize, idx: usize, s0: u8, s1: u8) -> f32 {
        let layers = self.params.meta_layers as usize;
        if self.params.eta == 0.0 || layers == 0 {
            return 0.0;
        }
        let denom = self.params.l_s.max(1) as f32;
        let s0n = (s0 as f32) / denom;
        let s1n = (s1 as f32) / denom;
        let g = self.params.grid_size as usize;
        let cells = g * g;
        let mut delta = 0.0;
        if level > 0 {
            let neighbor = if level == 1 {
                self.s_field[idx]
            } else {
                self.meta_field[(level - 2) * cells + idx]
            };
            let nn = (neighbor as f32) / denom;
            delta += 0.5 * self.params.eta * ((s1n - nn).powi(2) - (s0n - nn).powi(2));
        }
        if level < layers {
            let neighbor = self.meta_field[level * cells + idx];
            let nn = (neighbor as f32) / denom;
            delta += 0.5 * self.params.eta * ((s1n - nn).powi(2) - (s0n - nn).powi(2));
        }
        delta
    }

    fn delta_s_mismatch_level(&self, level: usize, idx: usize, s0: u8, s1: u8) -> f32 {
        let layers = self.params.meta_layers as usize;
        if layers == 0 {
            return 0.0;
        }
        let denom = self.params.l_s.max(1) as f32;
        let s0n = (s0 as f32) / denom;
        let s1n = (s1 as f32) / denom;
        let g = self.params.grid_size as usize;
        let cells = g * g;
        let mut delta = 0.0;
        if level > 0 {
            let neighbor = if level == 1 {
                self.s_field[idx]
            } else {
                self.meta_field[(level - 2) * cells + idx]
            };
            let nn = (neighbor as f32) / denom;
            delta += 0.5 * ((s1n - nn).powi(2) - (s0n - nn).powi(2));
        }
        if level < layers {
            let neighbor = self.meta_field[level * cells + idx];
            let nn = (neighbor as f32) / denom;
            delta += 0.5 * ((s1n - nn).powi(2) - (s0n - nn).powi(2));
        }
        delta
    }

    fn drive_align_work(&self, level: usize, idx: usize, s0: u8, s1: u8, x: f32, y: f32) -> f32 {
        if !self.params.p6_on || self.params.eta_drive == 0.0 {
            return 0.0;
        }
        if self.params.repair_clock_gated && level == 0 {
            return 0.0;
        }
        let gated = self.params.repair_clock_gated && level > 0;
        if gated && !self.clock_gate_allows(x, y) {
            return 0.0;
        }
        let delta = if self.params.s_coupling_mode == 1 {
            self.delta_raw_s_op(level, idx, s0, s1)
        } else {
            self.delta_s_mismatch_level(level, idx, s0, s1)
        };
        let scale = self.params.l_s.max(1) as f32;
        // Boost work under quadrant gating to offset reduced coverage.
        let gate_scale = if gated { 16.0 } else { 1.0 };
        let mu_scale = self.mu_at(x, y).abs();
        -self.params.eta_drive * delta * scale * scale * gate_scale * mu_scale
    }

    fn delta_e_meta_a_couple(&self, layer: usize, idx: usize, a0: u16, a1: u16) -> f32 {
        let layers = self.params.meta_layers as usize;
        if self.params.eta == 0.0 || layers < 2 {
            return 0.0;
        }
        let denom = self.params.l_a.max(1) as f32;
        let a0n = (a0 as f32) / denom;
        let a1n = (a1 as f32) / denom;
        let g = self.params.grid_size as usize;
        let cells = g * g;
        let mut delta = 0.0;
        if layer > 0 {
            let neighbor = self.meta_a_field[(layer - 1) * cells + idx];
            let nn = (neighbor as f32) / denom;
            delta += 0.5 * self.params.eta * ((a1n - nn).powi(2) - (a0n - nn).powi(2));
        }
        if layer + 1 < layers {
            let neighbor = self.meta_a_field[(layer + 1) * cells + idx];
            let nn = (neighbor as f32) / denom;
            delta += 0.5 * self.params.eta * ((a1n - nn).powi(2) - (a0n - nn).powi(2));
        }
        delta
    }

    fn delta_e_meta_n_couple(&self, layer: usize, idx: usize, n0: i16, n1: i16) -> f32 {
        let layers = self.params.meta_layers as usize;
        if self.params.eta == 0.0 || layers < 2 {
            return 0.0;
        }
        let denom = self.params.l_n.max(1) as f32;
        let n0n = (n0 as f32) / denom;
        let n1n = (n1 as f32) / denom;
        let g = self.params.grid_size as usize;
        let cells = g * g;
        let mut delta = 0.0;
        if layer > 0 {
            let neighbor = self.meta_n_field[(layer - 1) * cells + idx];
            let nn = (neighbor as f32) / denom;
            delta += 0.5 * self.params.eta * ((n1n - nn).powi(2) - (n0n - nn).powi(2));
        }
        if layer + 1 < layers {
            let neighbor = self.meta_n_field[(layer + 1) * cells + idx];
            let nn = (neighbor as f32) / denom;
            delta += 0.5 * self.params.eta * ((n1n - nn).powi(2) - (n0n - nn).powi(2));
        }
        delta
    }

    fn delta_e_meta_w_couple(&self, layer: usize, edge: usize, w0: u8, w1: u8) -> f32 {
        let layers = self.params.meta_layers as usize;
        if self.params.eta == 0.0 || layers < 2 {
            return 0.0;
        }
        let denom = self.params.l_w.max(1) as f32;
        let w0n = (w0 as f32) / denom;
        let w1n = (w1 as f32) / denom;
        let g = self.params.grid_size as usize;
        let edges = meta_edge_count(g);
        let mut delta = 0.0;
        if layer > 0 {
            let neighbor = self.meta_w_edges[(layer - 1) * edges + edge];
            let nn = (neighbor as f32) / denom;
            delta += 0.5 * self.params.eta * ((w1n - nn).powi(2) - (w0n - nn).powi(2));
        }
        if layer + 1 < layers {
            let neighbor = self.meta_w_edges[(layer + 1) * edges + edge];
            let nn = (neighbor as f32) / denom;
            delta += 0.5 * self.params.eta * ((w1n - nn).powi(2) - (w0n - nn).powi(2));
        }
        delta
    }

    fn mu_at(&self, x: f32, _y: f32) -> f32 {
        if x < 0.5 {
            self.params.mu_high
        } else {
            self.params.mu_low
        }
    }

    fn delta_e_move_particle(&self, i: usize, x0: f32, y0: f32, x1: f32, y1: f32) -> f32 {
        let mut d_rep = 0.0f32;
        let mut d_bond = 0.0f32;
        for j in 0..self.n {
            if j == i {
                continue;
            }
            let xj = self.positions[2 * j];
            let yj = self.positions[2 * j + 1];
            let r0 = torus_dist(x0, y0, xj, yj);
            let r1 = torus_dist(x1, y1, xj, yj);
            d_rep += repulsion_energy(self.params.kappa_rep, self.params.r0, r1)
                - repulsion_energy(self.params.kappa_rep, self.params.r0, r0);

            let (a, b) = if i < j { (i, j) } else { (j, i) };
            let w = self.w[edge_index(self.n, a, b)];
            if w > 0 {
                let wf = w as f32;
                let bond0 = 0.5 * self.params.kappa_bond * wf * (r0 - self.params.r_star).powi(2);
                let bond1 = 0.5 * self.params.kappa_bond * wf * (r1 - self.params.r_star).powi(2);
                d_bond += bond1 - bond0;
            }
        }
        d_rep + d_bond
    }

    fn energy_breakdown_inner(&self) -> (f32, f32, f32, f32, f32, f32, f32) {
        let mut u_rep = 0.0f32;
        let mut u_bond = 0.0f32;
        let mut e_w = 0.0f32;
        let mut e_n = 0.0f32;
        let mut e_a = 0.0f32;
        let mut e_s = 0.0f32;

        for i in 0..self.n {
            for j in (i + 1)..self.n {
                let r = torus_dist(
                    self.positions[2 * i],
                    self.positions[2 * i + 1],
                    self.positions[2 * j],
                    self.positions[2 * j + 1],
                );
                u_rep += repulsion_energy(self.params.kappa_rep, self.params.r0, r);
                let w = self.w[edge_index(self.n, i, j)] as f32;
                if w > 0.0 {
                    u_bond += 0.5 * self.params.kappa_bond * w * (r - self.params.r_star).powi(2);
                }
            }
        }

        for &w in &self.w {
            let wf = w as f32;
            e_w += 0.5 * self.params.lambda_w * wf * wf;
        }

        for &n in &self.n_counter {
            let nf = n as f32;
            e_n += 0.5 * self.params.lambda_n * nf * nf;
        }

        for &a in &self.a_counter {
            let af = a as f32;
            e_a += 0.5 * self.params.lambda_a * af * af;
        }

        for &s in &self.s_field {
            let sf = s as f32;
            e_s += 0.5 * self.params.lambda_s * sf * sf;
        }

        for &w in &self.meta_w_edges {
            let wf = w as f32;
            e_w += 0.5 * self.params.lambda_w * wf * wf;
        }

        for &n in &self.meta_n_field {
            let nf = n as f32;
            e_n += 0.5 * self.params.lambda_n * nf * nf;
        }

        for &a in &self.meta_a_field {
            let af = a as f32;
            e_a += 0.5 * self.params.lambda_a * af * af;
        }

        for &s in &self.meta_field {
            let sf = s as f32;
            e_s += 0.5 * self.params.lambda_s * sf * sf;
        }

        let total = u_rep + u_bond + e_w + e_n + e_a + e_s;
        (u_rep, u_bond, e_w, e_n, e_a, e_s, total)
    }

    fn w_histogram(&self) -> Uint32Array {
        let bins = (self.params.l_w as usize) + 1;
        let mut hist = vec![0u32; bins];
        for &w in &self.w {
            let idx = (w as usize).min(bins - 1);
            hist[idx] += 1;
        }
        Uint32Array::from(hist.as_slice())
    }

    fn s_histogram(&self) -> Uint32Array {
        let bins = (self.params.l_s as usize) + 1;
        let mut hist = vec![0u32; bins];
        for &s in &self.s_field {
            let idx = (s as usize).min(bins - 1);
            hist[idx] += 1;
        }
        Uint32Array::from(hist.as_slice())
    }
}

#[derive(Clone, Copy, Default)]
struct EpQStats {
    count: u64,
    sum: f64,
    max_abs: f64,
}

impl EpQStats {
    fn record(&mut self, value: f32) {
        let v = value as f64;
        self.count = self.count.saturating_add(1);
        self.sum += v;
        let abs = v.abs();
        if abs > self.max_abs {
            self.max_abs = abs;
        }
    }
}

#[derive(Clone, Copy, Default)]
struct StepDiag {
    w_plus: u32,
    w_minus: u32,
    n_plus: u32,
    n_minus: u32,
    a_plus: u32,
    a_minus: u32,
    s_plus: u32,
    s_minus: u32,
    w_plus_h: u32,
    w_minus_h: u32,
    w_plus_l: u32,
    w_minus_l: u32,
    n_plus_h: u32,
    n_minus_h: u32,
    n_plus_l: u32,
    n_minus_l: u32,
    a_plus_h: u32,
    a_minus_h: u32,
    a_plus_l: u32,
    a_minus_l: u32,
    s_plus_h: u32,
    s_minus_h: u32,
    s_plus_l: u32,
    s_minus_l: u32,
}

#[derive(Clone, Copy, Default)]
struct DiagTotals {
    steps: u32,
    w_plus: u32,
    w_minus: u32,
    n_plus: u32,
    n_minus: u32,
    a_plus: u32,
    a_minus: u32,
    s_plus: u32,
    s_minus: u32,
    w_plus_h: u32,
    w_minus_h: u32,
    w_plus_l: u32,
    w_minus_l: u32,
    n_plus_h: u32,
    n_minus_h: u32,
    n_plus_l: u32,
    n_minus_l: u32,
    a_plus_h: u32,
    a_minus_h: u32,
    a_plus_l: u32,
    a_minus_l: u32,
    s_plus_h: u32,
    s_minus_h: u32,
    s_plus_l: u32,
    s_minus_l: u32,
}

impl DiagTotals {
    fn push(&mut self, step: StepDiag) {
        self.steps = self.steps.saturating_add(1);
        self.w_plus = self.w_plus.saturating_add(step.w_plus);
        self.w_minus = self.w_minus.saturating_add(step.w_minus);
        self.n_plus = self.n_plus.saturating_add(step.n_plus);
        self.n_minus = self.n_minus.saturating_add(step.n_minus);
        self.a_plus = self.a_plus.saturating_add(step.a_plus);
        self.a_minus = self.a_minus.saturating_add(step.a_minus);
        self.s_plus = self.s_plus.saturating_add(step.s_plus);
        self.s_minus = self.s_minus.saturating_add(step.s_minus);
        self.w_plus_h = self.w_plus_h.saturating_add(step.w_plus_h);
        self.w_minus_h = self.w_minus_h.saturating_add(step.w_minus_h);
        self.w_plus_l = self.w_plus_l.saturating_add(step.w_plus_l);
        self.w_minus_l = self.w_minus_l.saturating_add(step.w_minus_l);
        self.n_plus_h = self.n_plus_h.saturating_add(step.n_plus_h);
        self.n_minus_h = self.n_minus_h.saturating_add(step.n_minus_h);
        self.n_plus_l = self.n_plus_l.saturating_add(step.n_plus_l);
        self.n_minus_l = self.n_minus_l.saturating_add(step.n_minus_l);
        self.a_plus_h = self.a_plus_h.saturating_add(step.a_plus_h);
        self.a_minus_h = self.a_minus_h.saturating_add(step.a_minus_h);
        self.a_plus_l = self.a_plus_l.saturating_add(step.a_plus_l);
        self.a_minus_l = self.a_minus_l.saturating_add(step.a_minus_l);
        self.s_plus_h = self.s_plus_h.saturating_add(step.s_plus_h);
        self.s_minus_h = self.s_minus_h.saturating_add(step.s_minus_h);
        self.s_plus_l = self.s_plus_l.saturating_add(step.s_plus_l);
        self.s_minus_l = self.s_minus_l.saturating_add(step.s_minus_l);
    }

    fn counts(&self) -> (u32, u32, u32, u32, u32, u32, u32, u32, u32) {
        (
            self.w_plus,
            self.w_minus,
            self.n_plus,
            self.n_minus,
            self.a_plus,
            self.a_minus,
            self.s_plus,
            self.s_minus,
            self.steps,
        )
    }

    fn counts_hl(&self) -> (u32, u32, u32, u32, u32, u32, u32, u32, u32, u32, u32, u32, u32, u32, u32, u32) {
        (
            self.w_plus_h,
            self.w_minus_h,
            self.w_plus_l,
            self.w_minus_l,
            self.n_plus_h,
            self.n_minus_h,
            self.n_plus_l,
            self.n_minus_l,
            self.a_plus_h,
            self.a_minus_h,
            self.a_plus_l,
            self.a_minus_l,
            self.s_plus_h,
            self.s_minus_h,
            self.s_plus_l,
            self.s_minus_l,
        )
    }
}

fn diag_flux_affinity(n_plus: u32, n_minus: u32, window: u32) -> (f32, f32, f32) {
    if window == 0 {
        return (0.0, 0.0, 0.0);
    }
    let j = (n_plus as f32 - n_minus as f32) / (window as f32);
    let a = ((n_plus + 1) as f32 / (n_minus + 1) as f32).ln();
    let sigma = j * a;
    (j, a, sigma)
}

fn diag_m6_affinity(nh_plus: u32, nh_minus: u32, nl_plus: u32, nl_minus: u32) -> f32 {
    let a = (nh_plus + 1) as f32;
    let b = (nl_minus + 1) as f32;
    let c = (nh_minus + 1) as f32;
    let d = (nl_plus + 1) as f32;
    (a * b / (c * d)).ln()
}

fn wrap01(mut x: f32) -> f32 {
    if x >= 1.0 {
        x -= 1.0;
    } else if x < 0.0 {
        x += 1.0;
    }
    x
}

fn get_f32(obj: &JsValue, key: &str) -> Option<f32> {
    let v = Reflect::get(obj, &JsValue::from_str(key)).ok()?;
    if v.is_undefined() || v.is_null() {
        return None;
    }
    v.as_f64().map(|n| n as f32)
}

fn get_string(obj: &JsValue, key: &str) -> Option<String> {
    let v = Reflect::get(obj, &JsValue::from_str(key)).ok()?;
    if v.is_undefined() || v.is_null() {
        return None;
    }
    v.as_string()
}

fn get_u8(obj: &JsValue, key: &str) -> Option<u8> {
    let v = Reflect::get(obj, &JsValue::from_str(key)).ok()?;
    if v.is_undefined() || v.is_null() {
        return None;
    }
    let n = v.as_f64()?;
    if !n.is_finite() {
        return None;
    }
    Some(n.round().clamp(0.0, 255.0) as u8)
}

fn get_i16(obj: &JsValue, key: &str) -> Option<i16> {
    let v = Reflect::get(obj, &JsValue::from_str(key)).ok()?;
    if v.is_undefined() || v.is_null() {
        return None;
    }
    let n = v.as_f64()?;
    if !n.is_finite() {
        return None;
    }
    Some(n.round().clamp(-32768.0, 32767.0) as i16)
}

fn get_u16(obj: &JsValue, key: &str) -> Option<u16> {
    let v = Reflect::get(obj, &JsValue::from_str(key)).ok()?;
    if v.is_undefined() || v.is_null() {
        return None;
    }
    let n = v.as_f64()?;
    if !n.is_finite() {
        return None;
    }
    Some(n.round().clamp(0.0, 65535.0) as u16)
}

fn get_u32(obj: &JsValue, key: &str) -> Option<u32> {
    let v = Reflect::get(obj, &JsValue::from_str(key)).ok()?;
    if v.is_undefined() || v.is_null() {
        return None;
    }
    let n = v.as_f64()?;
    if !n.is_finite() {
        return None;
    }
    Some(n.round().clamp(0.0, 4294967295.0) as u32)
}

fn edge_index(n: usize, i: usize, j: usize) -> usize {
    debug_assert!(i < j);
    // Row-major upper-triangle (excluding diagonal):
    // row i has entries (i,i+1)...(i,n-1), length n-i-1.
    let before = i * (2 * n - i - 1) / 2;
    before + (j - i - 1)
}

fn torus_dist(x0: f32, y0: f32, x1: f32, y1: f32) -> f32 {
    let mut dx = x0 - x1;
    let mut dy = y0 - y1;
    if dx > 0.5 {
        dx -= 1.0;
    } else if dx < -0.5 {
        dx += 1.0;
    }
    if dy > 0.5 {
        dy -= 1.0;
    } else if dy < -0.5 {
        dy += 1.0;
    }
    (dx * dx + dy * dy).sqrt()
}

fn torus_delta(x0: f32, y0: f32, x1: f32, y1: f32) -> (f32, f32) {
    let mut dx = x1 - x0;
    let mut dy = y1 - y0;
    if dx > 0.5 {
        dx -= 1.0;
    } else if dx < -0.5 {
        dx += 1.0;
    }
    if dy > 0.5 {
        dy -= 1.0;
    } else if dy < -0.5 {
        dy += 1.0;
    }
    (dx, dy)
}

fn torus_midpoint(x0: f32, y0: f32, x1: f32, y1: f32) -> (f32, f32) {
    let (dx, dy) = torus_delta(x0, y0, x1, y1);
    (wrap01(x0 + 0.5 * dx), wrap01(y0 + 0.5 * dy))
}

fn meta_edge_count(grid: usize) -> usize {
    grid.saturating_mul(grid).saturating_mul(2)
}

fn grid_cell_center(idx: usize, grid: usize) -> (f32, f32) {
    let g = grid as f32;
    let x = (idx % grid) as f32;
    let y = (idx / grid) as f32;
    ((x + 0.5) / g, (y + 0.5) / g)
}

// Edge indexing: 0..g*g are horizontal (cell -> right neighbor), g*g..2*g*g vertical (cell -> down).
fn meta_edge_midpoint(edge: usize, grid: usize) -> (f32, f32) {
    let cells = grid * grid;
    if edge < cells {
        let x = edge % grid;
        let y = edge / grid;
        let x2 = (x + 1) % grid;
        let idx1 = y * grid + x2;
        let (x0, y0) = grid_cell_center(edge, grid);
        let (x1, y1) = grid_cell_center(idx1, grid);
        torus_midpoint(x0, y0, x1, y1)
    } else {
        let local = edge - cells;
        let x = local % grid;
        let y = local / grid;
        let y2 = (y + 1) % grid;
        let idx1 = y2 * grid + x;
        let (x0, y0) = grid_cell_center(local, grid);
        let (x1, y1) = grid_cell_center(idx1, grid);
        torus_midpoint(x0, y0, x1, y1)
    }
}

fn repulsion_energy(kappa_rep: f32, r0: f32, r: f32) -> f32 {
    let d = (r0 - r).max(0.0);
    0.5 * kappa_rep * d * d
}

#[cfg(test)]
mod tests {
    use super::*;

    struct Lcg {
        state: u32,
    }

    impl Lcg {
        fn new(seed: u32) -> Self {
            Self { state: seed }
        }

        fn next_u32(&mut self) -> u32 {
            self.state = self.state.wrapping_mul(1664525).wrapping_add(1013904223);
            self.state
        }

        fn next_usize(&mut self, max: usize) -> usize {
            if max == 0 {
                0
            } else {
                (self.next_u32() as usize) % max
            }
        }

        fn next_u8_range(&mut self, max_inclusive: u8) -> u8 {
            if max_inclusive == 0 {
                0
            } else {
                (self.next_u32() % (max_inclusive as u32 + 1)) as u8
            }
        }

        fn next_u16_range(&mut self, max_inclusive: u16) -> u16 {
            if max_inclusive == 0 {
                0
            } else {
                (self.next_u32() % (max_inclusive as u32 + 1)) as u16
            }
        }

        fn next_i16_range(&mut self, min_inclusive: i16, max_inclusive: i16) -> i16 {
            if min_inclusive >= max_inclusive {
                return min_inclusive;
            }
            let span = (max_inclusive as i32 - min_inclusive as i32 + 1) as u32;
            min_inclusive + (self.next_u32() % span) as i16
        }
    }

    fn coupling_energy_s(
        params: &Params,
        grid: usize,
        meta_layers: usize,
        base_s: &[u8],
        meta_s: &[u8],
    ) -> f64 {
        if params.eta == 0.0 || meta_layers == 0 {
            return 0.0;
        }
        let denom = params.l_s.max(1) as f64;
        let eta = params.eta as f64;
        let cells = grid * grid;
        let mut energy = 0.0;
        for level in 1..=meta_layers {
            let lower = if level == 1 {
                &base_s[..cells]
            } else {
                &meta_s[(level - 2) * cells..(level - 1) * cells]
            };
            let upper = &meta_s[(level - 1) * cells..level * cells];
            for i in 0..cells {
                let a = (lower[i] as f64) / denom;
                let b = (upper[i] as f64) / denom;
                let diff = b - a;
                energy += 0.5 * eta * diff * diff;
            }
        }
        energy
    }

    fn coupling_energy_meta_a(
        params: &Params,
        grid: usize,
        meta_layers: usize,
        meta_a: &[u16],
    ) -> f64 {
        if params.eta == 0.0 || meta_layers < 2 {
            return 0.0;
        }
        let denom = params.l_a.max(1) as f64;
        let eta = params.eta as f64;
        let cells = grid * grid;
        let mut energy = 0.0;
        for layer in 1..meta_layers {
            let lower = &meta_a[(layer - 1) * cells..layer * cells];
            let upper = &meta_a[layer * cells..(layer + 1) * cells];
            for i in 0..cells {
                let a = (lower[i] as f64) / denom;
                let b = (upper[i] as f64) / denom;
                let diff = b - a;
                energy += 0.5 * eta * diff * diff;
            }
        }
        energy
    }

    fn coupling_energy_meta_n(
        params: &Params,
        grid: usize,
        meta_layers: usize,
        meta_n: &[i16],
    ) -> f64 {
        if params.eta == 0.0 || meta_layers < 2 {
            return 0.0;
        }
        let denom = params.l_n.max(1) as f64;
        let eta = params.eta as f64;
        let cells = grid * grid;
        let mut energy = 0.0;
        for layer in 1..meta_layers {
            let lower = &meta_n[(layer - 1) * cells..layer * cells];
            let upper = &meta_n[layer * cells..(layer + 1) * cells];
            for i in 0..cells {
                let a = (lower[i] as f64) / denom;
                let b = (upper[i] as f64) / denom;
                let diff = b - a;
                energy += 0.5 * eta * diff * diff;
            }
        }
        energy
    }

    fn coupling_energy_meta_w(
        params: &Params,
        grid: usize,
        meta_layers: usize,
        meta_w: &[u8],
    ) -> f64 {
        if params.eta == 0.0 || meta_layers < 2 {
            return 0.0;
        }
        let denom = params.l_w.max(1) as f64;
        let eta = params.eta as f64;
        let edges = meta_edge_count(grid);
        let mut energy = 0.0;
        for layer in 1..meta_layers {
            let lower = &meta_w[(layer - 1) * edges..layer * edges];
            let upper = &meta_w[layer * edges..(layer + 1) * edges];
            for i in 0..edges {
                let a = (lower[i] as f64) / denom;
                let b = (upper[i] as f64) / denom;
                let diff = b - a;
                energy += 0.5 * eta * diff * diff;
            }
        }
        energy
    }

    fn fill_random_fields(sim: &mut Sim, rng: &mut Lcg) {
        let l_s = sim.params.l_s;
        let l_w = sim.params.l_w;
        let l_a = sim.params.l_a;
        let l_n = sim.params.l_n;
        for s in &mut sim.s_field {
            *s = rng.next_u8_range(l_s);
        }
        for s in &mut sim.meta_field {
            *s = rng.next_u8_range(l_s);
        }
        for w in &mut sim.meta_w_edges {
            *w = rng.next_u8_range(l_w);
        }
        for a in &mut sim.meta_a_field {
            *a = rng.next_u16_range(l_a);
        }
        for n in &mut sim.meta_n_field {
            *n = rng.next_i16_range(-l_n, l_n);
        }
    }

    fn propose_u8(rng: &mut Lcg, max: u8, current: u8) -> u8 {
        if max == 0 {
            return 0;
        }
        if current == 0 {
            1
        } else if current >= max {
            current - 1
        } else if rng.next_u32() % 2 == 0 {
            current + 1
        } else {
            current - 1
        }
    }

    fn propose_u16(rng: &mut Lcg, max: u16, current: u16) -> u16 {
        if max == 0 {
            return 0;
        }
        if current == 0 {
            1
        } else if current >= max {
            current - 1
        } else if rng.next_u32() % 2 == 0 {
            current + 1
        } else {
            current - 1
        }
    }

    fn propose_i16(rng: &mut Lcg, max: i16, current: i16) -> i16 {
        if max <= 0 {
            return 0;
        }
        if current <= -max {
            current + 1
        } else if current >= max {
            current - 1
        } else if rng.next_u32() % 2 == 0 {
            current + 1
        } else {
            current - 1
        }
    }

    #[test]
    fn test_delta_e_s_couple_matches_energy_diff() {
        let mut sim = Sim::new(1, 1);
        let g = 5usize;
        let layers = 3usize;
        sim.params.grid_size = g as u16;
        sim.params.meta_layers = layers as u16;
        sim.params.l_s = 7;
        sim.params.eta = 0.7;
        sim.resize_meta_arrays();

        let mut rng = Lcg::new(1234);
        fill_random_fields(&mut sim, &mut rng);

        let cells = g * g;
        let level = rng.next_usize(layers + 1);
        let idx = rng.next_usize(cells);
        let s0 = if level == 0 {
            sim.s_field[idx]
        } else {
            sim.meta_field[(level - 1) * cells + idx]
        };
        let s1 = propose_u8(&mut rng, sim.params.l_s, s0);
        let delta = sim.delta_e_s_couple_level(level, idx, s0, s1);

        let base_before = sim.s_field.clone();
        let meta_before = sim.meta_field.clone();
        let mut base_after = base_before.clone();
        let mut meta_after = meta_before.clone();
        if level == 0 {
            base_after[idx] = s1;
        } else {
            meta_after[(level - 1) * cells + idx] = s1;
        }

        let e_before = coupling_energy_s(&sim.params, g, layers, &base_before, &meta_before);
        let e_after = coupling_energy_s(&sim.params, g, layers, &base_after, &meta_after);
        let diff = (e_after - e_before - delta as f64).abs();
        assert!(diff < 1e-5);
    }

    #[test]
    fn test_delta_e_meta_a_couple_matches_energy_diff() {
        let mut sim = Sim::new(1, 1);
        let g = 5usize;
        let layers = 3usize;
        sim.params.grid_size = g as u16;
        sim.params.meta_layers = layers as u16;
        sim.params.l_a = 9;
        sim.params.eta = 0.7;
        sim.resize_meta_arrays();

        let mut rng = Lcg::new(5678);
        fill_random_fields(&mut sim, &mut rng);

        let cells = g * g;
        let layer = rng.next_usize(layers);
        let idx = rng.next_usize(cells);
        let a0 = sim.meta_a_field[layer * cells + idx];
        let a1 = propose_u16(&mut rng, sim.params.l_a, a0);
        let delta = sim.delta_e_meta_a_couple(layer, idx, a0, a1);

        let meta_before = sim.meta_a_field.clone();
        let mut meta_after = meta_before.clone();
        meta_after[layer * cells + idx] = a1;

        let e_before = coupling_energy_meta_a(&sim.params, g, layers, &meta_before);
        let e_after = coupling_energy_meta_a(&sim.params, g, layers, &meta_after);
        let diff = (e_after - e_before - delta as f64).abs();
        assert!(diff < 1e-5);
    }

    #[test]
    fn test_delta_e_meta_n_couple_matches_energy_diff() {
        let mut sim = Sim::new(1, 1);
        let g = 5usize;
        let layers = 3usize;
        sim.params.grid_size = g as u16;
        sim.params.meta_layers = layers as u16;
        sim.params.l_n = 7;
        sim.params.eta = 0.7;
        sim.resize_meta_arrays();

        let mut rng = Lcg::new(9012);
        fill_random_fields(&mut sim, &mut rng);

        let cells = g * g;
        let layer = rng.next_usize(layers);
        let idx = rng.next_usize(cells);
        let n0 = sim.meta_n_field[layer * cells + idx];
        let n1 = propose_i16(&mut rng, sim.params.l_n, n0);
        let delta = sim.delta_e_meta_n_couple(layer, idx, n0, n1);

        let meta_before = sim.meta_n_field.clone();
        let mut meta_after = meta_before.clone();
        meta_after[layer * cells + idx] = n1;

        let e_before = coupling_energy_meta_n(&sim.params, g, layers, &meta_before);
        let e_after = coupling_energy_meta_n(&sim.params, g, layers, &meta_after);
        let diff = (e_after - e_before - delta as f64).abs();
        assert!(diff < 1e-5);
    }

    #[test]
    fn test_delta_e_meta_w_couple_matches_energy_diff() {
        let mut sim = Sim::new(1, 1);
        let g = 5usize;
        let layers = 3usize;
        sim.params.grid_size = g as u16;
        sim.params.meta_layers = layers as u16;
        sim.params.l_w = 8;
        sim.params.eta = 0.7;
        sim.resize_meta_arrays();

        let mut rng = Lcg::new(3456);
        fill_random_fields(&mut sim, &mut rng);

        let edges = meta_edge_count(g);
        let layer = rng.next_usize(layers);
        let edge = rng.next_usize(edges);
        let w0 = sim.meta_w_edges[layer * edges + edge];
        let w1 = propose_u8(&mut rng, sim.params.l_w, w0);
        let delta = sim.delta_e_meta_w_couple(layer, edge, w0, w1);

        let meta_before = sim.meta_w_edges.clone();
        let mut meta_after = meta_before.clone();
        meta_after[layer * edges + edge] = w1;

        let e_before = coupling_energy_meta_w(&sim.params, g, layers, &meta_before);
        let e_after = coupling_energy_meta_w(&sim.params, g, layers, &meta_after);
        let diff = (e_after - e_before - delta as f64).abs();
        assert!(diff < 1e-5);
    }
}
</file>

</files>
